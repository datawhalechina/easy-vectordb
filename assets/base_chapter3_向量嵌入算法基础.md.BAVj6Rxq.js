import{_ as i,o as a,c as n,ag as p}from"./chunks/framework.CzE6cJJL.js";const l="/easy-vecdb/images/vector_embeddings.png",e="/easy-vecdb/images/Word2Vec.png",h="/easy-vecdb/images/GloVe.png",t="/easy-vecdb/images/semantic_confusion.png",y=JSON.parse('{"title":"向量嵌入算法基础","description":"","frontmatter":{},"headers":[],"relativePath":"base/chapter3/向量嵌入算法基础.md","filePath":"base/chapter3/向量嵌入算法基础.md","lastUpdated":1769517530000}'),k={name:"base/chapter3/向量嵌入算法基础.md"};function r(E,s,d,g,o,c){return a(),n("div",null,[...s[0]||(s[0]=[p('<h1 id="向量嵌入算法基础" tabindex="-1">向量嵌入算法基础 <a class="header-anchor" href="#向量嵌入算法基础" aria-label="Permalink to &quot;向量嵌入算法基础&quot;">​</a></h1><p>向量嵌入（Vector Embeddings）是向量数据库的核心基础，其核心功能是将非结构化数据转换为高维向量，从而使其能在向量空间中实现存储、检索与分析。借助这一嵌入技术，文本、图像、音频等非结构化数据得以映射为具有语义关联性的向量表示，这为相似性搜索与数据挖掘提供了关键技术支撑。随着人工智能技术的持续发展，非结构化数据的应用范围正不断扩大。</p><p>本章开篇先系统介绍静态与动态向量嵌入的核心原理及实现路径，深入剖析二者在语义关系捕捉层面的独特优势与固有局限。通过聚焦数据降维技术在嵌入优化过程中的核心价值，具体阐释如何在有效降低数据复杂度的同时，最大限度保留关键语义信息。本章整体旨在为高效嵌入的生成与优化工作提供坚实的理论支撑与可落地的实践指引，进而为向量数据库系统的构建奠定核心技术基础。</p><p><img src="'+l+'" alt="vector_embeddings"><em>将非结构化数据（文本、图像）映射为高维向量</em></p><h2 id="_1-静态向量嵌入" tabindex="-1">1.静态向量嵌入 <a class="header-anchor" href="#_1-静态向量嵌入" aria-label="Permalink to &quot;1.静态向量嵌入&quot;">​</a></h2><p>静态向量嵌入（Static Vector Embedding）是向量数据库中常用的非结构化数据处理方式，其核心逻辑是通过固定不变的映射规则，将文本、图像、音频等非结构化数据，转换为维度固定、数值稳定的连续型向量。一旦向量生成，其数值便不再随数据所处的上下文环境、应用场景变化；这些静态向量可存储于向量数据库中，依托余弦相似度、欧氏距离等相似性计算方法，高效实现数据的检索、聚类、分类等任务。</p><p>在静态向量嵌入的应用中，文本领域的传统词向量模型是典型代表 —— 这类模型专注于将离散的词汇（或子词）映射为固定的高维向量，从而捕捉词汇间的基础语义关联（如 &quot;医生&quot; 与 &quot;医院&quot; 的场景关联性、&quot;篮球&quot; 与 &quot;足球&quot; 的类别相似性），以提升文本处理效率。本节以介绍 Word2Vec 和 GloVe 这两个最常用的静态向量嵌入模型为例，介绍其核心原理、实现步骤及应用场景。</p><h3 id="_1-1-word2vec" tabindex="-1">1.1 Word2Vec <a class="header-anchor" href="#_1-1-word2vec" aria-label="Permalink to &quot;1.1 Word2Vec&quot;">​</a></h3><p>Word2Vec 基于浅层神经网络架构，通过两种核心任务学习词汇的静态向量：</p><ul><li><p><strong>CBOW（连续词袋模型）</strong>：以 &quot;上下文词汇&quot; 为输入，训练模型预测 &quot;目标中心词&quot;，通过优化预测误差，让语义相近的词汇在向量空间中距离更近（如 &quot;春天&quot;&quot;花开&quot;&quot;温暖&quot; 的向量会聚集在相近区域）；</p></li><li><p><strong>Skip-gram（跳字模型）</strong>：与 CBOW 反向，以 &quot;目标中心词&quot; 为输入，预测 &quot;其周围的上下文词汇&quot;，更适用于处理低频词汇，能更精准学习罕见词的语义向量。</p></li></ul><p>该模型的训练依赖局部上下文窗口（通常设置为 5-10 个词），仅捕捉窗口内的词汇关联，生成的每个词汇对应唯一向量（如 &quot;手机&quot; 在 &quot;买手机&quot;&quot;用手机拍照&quot; 中向量完全一致），且向量数值终身固定，属于典型的静态向量。</p><h3 id="_2-2-glove-global-vectors-for-word-representation" tabindex="-1">2.2 GloVe（Global Vectors for Word Representation） <a class="header-anchor" href="#_2-2-glove-global-vectors-for-word-representation" aria-label="Permalink to &quot;2.2 GloVe（Global Vectors for Word Representation）&quot;">​</a></h3><p>GloVe 的核心优势在于结合全局语料信息，其训练逻辑不依赖神经网络，而是通过 &quot;统计 - 优化&quot; 两步实现：</p><ul><li><p><strong>统计全局共现频率</strong>：遍历整个语料库，统计 &quot;目标词 - 上下文词&quot; 共同出现的次数，构建 &quot;词 - 词共现矩阵&quot;（矩阵中每个元素代表两个词共同出现的频率，如 &quot;国王&quot; 与 &quot;王后&quot; 的共现频率高于 &quot;国王&quot; 与 &quot;苹果&quot;）；</p></li><li><p><strong>优化向量误差</strong>：通过数学模型预测 &quot;目标词与上下文词的共现概率比&quot;，并最小化 &quot;预测值&quot; 与 &quot;共现矩阵中真实值&quot; 的误差，最终生成每个词汇的固定向量。</p></li></ul><p>相比 Word2Vec，GloVe 能更精准捕捉词汇间的线性语义关系（如 &quot;国王 - 男人 + 女人 ≈ 王后&quot; 的向量运算成立），但本质仍是静态向量 —— 词汇向量不随上下文变化，仅由全局语料的共现规律决定。</p><p>上面的文字表述有点抽象，这样我们换一种方式来表述：</p><p>我们可以把 &quot;语言里的所有单词&quot; 想象成 &quot;一个巨大图书馆里的每一本书&quot;，而 &quot;单词之间的语义关系&quot;，就像 &quot;书和书之间的关联&quot;（比如同主题、常被一起翻阅等）。Word2Vec 和 GloVe 这两个模型，就像是两种不同风格的 &quot;图书整理员&quot;，用不同方法帮我们找到书与书的关联，再把这种关联变成 &quot;向量&quot;。</p><p><strong>Word2Vec 更像一位 &quot;喜欢观察邻居的管理员&quot;</strong>—— 它整理图书时，不看全图书馆的所有书，只盯着 &quot;每本书旁边的几本书&quot;（对应模型里的 &quot;上下文窗口&quot;，比如看某本书前后 5 本相邻的书）。</p><p>它的工作逻辑很简单，就靠两个小任务训练：</p><ul><li>比如看到 &quot;苹果&quot; 这本书旁边常放着 &quot;水果&quot;&quot;甜的&quot;&quot;咬一口&quot; 这些书，就试着 &quot;根据旁边的书（如&#39;水果&#39;&#39;甜的&#39;）猜出中间的书是&#39;苹果&#39;&quot;；</li><li>或者反过来，看到 &quot;苹果&quot; 这本书，就试着 &quot;猜出它旁边可能会放&#39;水果&#39;&#39;甜的&#39;这些书&quot;。</li></ul><p>练得多了，Word2Vec 就会发现：&quot;苹果&quot; 和 &quot;水果&quot; 总是紧挨着，&quot;猫&quot; 和 &quot;狗&quot;&quot;鱼&quot;&quot;毛茸茸&quot; 常相邻，于是就把这些语义相近的 &quot;书&quot;（单词），在向量里安排在离得近的位置。比如你查 &quot;苹果&quot; 的向量，会发现它和 &quot;香蕉&quot;&quot;橘子&quot; 的向量很像，和 &quot;汽车&quot;&quot;电脑&quot; 的向量差得很远 —— 这就是它通过 &quot;看邻居&quot; 找到的语义关联。</p><p><img src="'+e+'" alt="Word2Vec"></p><p><strong>GloVe 则像一位 &quot;爱做全面统计的管理员&quot;</strong>—— 它不满足于只看 &quot;某本书旁边的几本书&quot;，而是会先做一件大事：给整个图书馆画一张 &quot;全局统计表&quot;。</p><p>这张表会记录：</p><ul><li>每本书（比如 &quot;苹果&quot;）和其他所有书（比如 &quot;水果&quot;&quot;公司&quot;&quot;甜的&quot;&quot;电脑&quot;）&quot;一起出现过多少次&quot;；</li><li>再算出 &quot;它们一起出现的概率&quot;（比如 &quot;苹果&quot; 和 &quot;水果&quot; 一起出现的概率，是 &quot;苹果&quot; 和 &quot;电脑&quot; 一起出现概率的多少倍）。</li></ul><p>通过分析这张全局表，GloVe 能同时抓住 &quot;局部小关联&quot; 和 &quot;全局大规律&quot;：</p><ul><li>它也能像 Word2Vec 一样，知道 &quot;苹果&quot; 和 &quot;水果&quot; 关系近；</li><li>还能发现更细致的全局关联，比如 &quot;国王&quot; 和 &quot;王后&quot; 的关系，很像 &quot;男人&quot; 和 &quot;女人&quot; 的关系（这就是常说的 &quot;国王 - 男人 + 女人 ≈ 王后&quot; 的向量计算，靠全局统计才能更准确）。</li></ul><p><img src="'+h+'" alt="GloVe"></p><p>不过要注意一个关键点：GloVe 虽然看的是全局数据，但它给 &quot;苹果&quot; 这本书只分配一个固定的向量—— 所以它没法区分 &quot;苹果（水果）&quot; 和 &quot;苹果（公司）&quot; 的不同含义，因为这两个意思共用同一个向量。它能发现 &quot;苹果&quot; 既常和 &quot;水果&quot; 一起出现，也常和 &quot;公司&quot; 一起出现，却没法把这两个意思分开。</p><p>静态向量嵌入虽能捕捉词汇的基础语义关联，但由于其 &quot;一词一固定向量&quot; 的核心特性，在处理复杂语言场景时存在显著局限性，具体可归纳为以下三点：</p><ul><li><p><strong>多义词语义混淆</strong>：无法区分词汇的歧义</p><blockquote><p>例如：&quot;苹果&quot; 在 &quot;今天吃了一个苹果&quot;（水果语义）和 &quot;新买的苹果手机很好用&quot;（品牌语义）中，含义完全不同，但静态向量相同；</p></blockquote><p><img src="'+t+`" alt="semantic_confusion"><em>到底是哪个苹果呢？</em></p></li><li><p><strong>上下文场景信息丢失</strong>：忽略语义的场景依赖性</p><blockquote><p>&quot;跑步&quot; 在 &quot;清晨在公园跑步&quot;（户外、有氧运动）和 &quot;雨天在跑步机上跑步&quot;（室内、器械运动）中，场景差异明显，但静态向量完全一致；</p></blockquote></li><li><p><strong>动态语义无法适配</strong>：向量固定且不可更新</p><blockquote><p>&quot;手机&quot; 在十年前的语料中常与 &quot;通话、短信&quot; 关联，而现在常与 &quot;5G、拍照、智能办公&quot; 关联，但静态向量无法更新这种语义变化；</p></blockquote></li></ul><h2 id="_2-python-代码实战-直观验证静态向量的语境不变性" tabindex="-1">2.Python 代码实战：直观验证静态向量的语境不变性 <a class="header-anchor" href="#_2-python-代码实战-直观验证静态向量的语境不变性" aria-label="Permalink to &quot;2.Python 代码实战：直观验证静态向量的语境不变性&quot;">​</a></h2><p>为了更直观地展现静态向量的不足，我们以Word2Vec（静态向量代表模型） 为例，通过代码提取同一多义词在不同语境下的向量，验证其向量是否完全相同。</p><h3 id="_2-1-环境准备" tabindex="-1">2.1 环境准备 <a class="header-anchor" href="#_2-1-环境准备" aria-label="Permalink to &quot;2.1 环境准备&quot;">​</a></h3><p>首先安装依赖库（gensim提供 Word2Vec 的实现及预训练模型）：</p><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install gensim # 用于加载Word2Vec预训练模型</span></span>
<span class="line"><span>pip install numpy   # 用于向量计算</span></span>
<span class="line"><span>pip install modelscope # 魔塔社区模型下载</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h3 id="_2-2-代码实现-提取多义词在不同语境下的向量" tabindex="-1">2.2 代码实现：提取多义词在不同语境下的向量 <a class="header-anchor" href="#_2-2-代码实现-提取多义词在不同语境下的向量" aria-label="Permalink to &quot;2.2 代码实现：提取多义词在不同语境下的向量&quot;">​</a></h3><p>我们选择两个典型多义词 ——“apple”（对应中文 “苹果”，含 “水果”“品牌” 歧义）和 “bank”（对应中文 “银行”，含 “金融机构”“河岸” 歧义），通过以下步骤验证：</p><ol><li>加载预训练 Word2Vec 模型（采用 Google 新闻语料训练的 300 维模型，覆盖常见词汇）；</li><li>构造不同语境的句子，提取目标词的向量；</li><li>计算同一词在不同语境下的余弦相似度（向量相同则相似度 = 1.0）和欧氏距离（向量相同则距离 = 0）；</li><li>输出结果并分析。</li></ol><p>模型文件可通过魔塔社区下载：<a href="https://www.modelscope.cn/models/lili666/text2vec-word2vec-tencent-chinese/summary" target="_blank" rel="noreferrer">https://www.modelscope.cn/models/lili666/text2vec-word2vec-tencent-chinese/summary</a></p><p>本次演示，使用腾讯词训练的 200 维模型，覆盖常见词汇。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> gensim.models </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> KeyedVectors</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> test_static_embedding</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(model, target_word, related_words1, related_words2, desc1, desc2):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    测试静态向量对多义词的处理能力（基于gensim）</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param model: gensim加载的KeyedVectors模型</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param target_word: 多义词</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param related_words1: 与语义1相关的词汇列表</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param related_words2: 与语义2相关的词汇列表</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param desc1: 语义1描述</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param desc2: 语义2描述</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 检查目标词是否在模型词表中</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> target_word </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">not</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;⚠️ 词汇&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target_word</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;不在模型词表中&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 1. 验证同一词在不同语境下的向量是否相同</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    vec1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model[target_word]  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 语义1对应的向量（静态模型中唯一）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    vec2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model[target_word]  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 语义2对应的向量（静态模型中唯一）</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 计算余弦相似度（静态向量应恒为1.0）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    cos_sim </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.dot(vec1, vec2) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (np.linalg.norm(vec1) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.linalg.norm(vec2))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">=== 多义词：&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target_word</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39; ===&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;语义1：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">desc1</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">；语义2：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">desc2</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;两种语义下的向量余弦相似度：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cos_sim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:.4f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">（=1.0表示向量完全相同）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 2. 计算与两种语义相关词汇的相似度</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">【与</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">desc1</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">相关词汇的相似度】&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> word </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> related_words1:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> word </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # gensim的similarity方法直接计算词向量相似度</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            sim </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.similarity(target_word, word)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target_word</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;与&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">word</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:.4f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">【与</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">desc2</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">相关词汇的相似度】&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> word </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> related_words2:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> word </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            sim </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.similarity(target_word, word)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target_word</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;与&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">word</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:.4f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">结论：静态向量无法区分多义词的不同语义，向量对两种相关词汇均有一定关联&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __name__</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> ==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;__main__&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 加载腾讯轻量版中文词向量（需替换为本地文件路径）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model_path </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;./model/lili666/text2vec-word2vec-tencent-chinese/light_Tencent_AILab_ChineseEmbedding.bin&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 替换为你的模型文件路径</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    try</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 腾讯模型为二进制格式，binary=True</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> KeyedVectors.load_word2vec_format(model_path, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">binary</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;模型加载成功！&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    except</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> FileNotFoundError</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modelscope </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> snapshot_download</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        model_dir </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> snapshot_download(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;lili666/text2vec-word2vec-tencent-chinese&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">cache_dir</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./model&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    except</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Exception</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> e:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;模型加载失败：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">e</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        exit</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 测试：&quot;苹果&quot;（水果 vs 品牌）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    test_static_embedding(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">        model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">        target_word</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;打&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">        related_words_list</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;拳头&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;打架&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;击打&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;殴打&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 语义1：击打</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;游戏&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;篮球&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;排球&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;比赛&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 语义2：进行活动</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;毛衣&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;家具&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;铁具&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;编织&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 语义3：制作/编织</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;电话&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;视频&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;通讯&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;号码&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]   </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 语义4：通讯</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        ],</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">        desc_list</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;击打（如：用拳头打）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;进行活动（如：打游戏）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;制作/编织（如：打毛衣）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;通讯（如：打电话）&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br></div></div><div class="language-text vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>=== 多义词：&#39;打&#39; ===</span></span>
<span class="line"><span>语义列表：[&#39;击打（如：用拳头打）&#39;, &#39;进行活动（如：打游戏）&#39;, &#39;制作/编织（如：打毛衣）&#39;, &#39;通讯（如：打电话）&#39;]</span></span>
<span class="line"><span>任意两种语义下的向量余弦相似度：[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]（均为1.0，证明向量完全相同）</span></span>
<span class="line"><span></span></span>
<span class="line"><span>【与击打（如：用拳头打）相关词汇的相似度】</span></span>
<span class="line"><span>&#39;打&#39;与&#39;拳头&#39;：0.4525</span></span>
<span class="line"><span>&#39;打&#39;与&#39;打架&#39;：0.5573</span></span>
<span class="line"><span>&#39;打&#39;与&#39;击打&#39;：0.4717</span></span>
<span class="line"><span>&#39;打&#39;与&#39;殴打&#39;：0.3791</span></span>
<span class="line"><span></span></span>
<span class="line"><span>【与进行活动（如：打游戏）相关词汇的相似度】</span></span>
<span class="line"><span>&#39;打&#39;与&#39;游戏&#39;：0.4393</span></span>
<span class="line"><span>&#39;打&#39;与&#39;篮球&#39;：0.3739</span></span>
<span class="line"><span>&#39;打&#39;与&#39;排球&#39;：0.4302</span></span>
<span class="line"><span>&#39;打&#39;与&#39;比赛&#39;：0.5015</span></span>
<span class="line"><span></span></span>
<span class="line"><span>【与制作/编织（如：打毛衣）相关词汇的相似度】</span></span>
<span class="line"><span>&#39;打&#39;与&#39;毛衣&#39;：0.3089</span></span>
<span class="line"><span>&#39;打&#39;与&#39;家具&#39;：0.2496</span></span>
<span class="line"><span>&#39;打&#39;与&#39;编织&#39;：0.3060</span></span>
<span class="line"><span></span></span>
<span class="line"><span>【与通讯（如：打电话）相关词汇的相似度】</span></span>
<span class="line"><span>&#39;打&#39;与&#39;电话&#39;：0.5287</span></span>
<span class="line"><span>&#39;打&#39;与&#39;视频&#39;：0.3960</span></span>
<span class="line"><span>&#39;打&#39;与&#39;通讯&#39;：0.2415</span></span>
<span class="line"><span>&#39;打&#39;与&#39;号码&#39;：0.4213</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><p>结论：静态向量对差异极大的多义词语义完全无法区分，向量同时关联所有语义的相关词，导致语义混乱</p><h2 id="_3-动态向量嵌入" tabindex="-1">3.动态向量嵌入 <a class="header-anchor" href="#_3-动态向量嵌入" aria-label="Permalink to &quot;3.动态向量嵌入&quot;">​</a></h2><p>动态向量嵌入是自然语言处理（NLP）领域的一次重要技术升级，可生成与上下文紧密关联的语义表示。与静态向量嵌入不同，动态嵌入不再依赖固定的词汇向量，而是结合具体句子结构与上下文语境，为每个输入文本（或 token）动态调整其向量表示。以 BERT、GPT 为代表的预训练语言模型，正是借助深度神经网络架构与核心的自注意力机制，实现了对上下文信息的精准捕捉，进而让多义词的语义区分、复杂句子的深层理解成为可能。</p><p>动态词向量嵌入模型（以 BERT、GPT 为代表）的核心突破在于：<strong>摆脱静态嵌入 “一个词对应一个固定向量” 的局限，能根据词语所在的上下文（如句子中前后词的含义、搭配关系）动态生成向量</strong>。</p><p>其技术实现依赖两大关键：</p><ol><li><p><strong>自注意力机制</strong>：精准计算词语与上下文其他词的关联权重，捕捉语义关联强度；</p></li><li><p><strong>预训练任务</strong>：让模型在大规模文本数据中提前学习语义规律，为后续动态向量生成奠定基础。</p></li></ol><p>最终生成的向量能精准反映词语在具体语境中的真实含义（例如 “银行” 在 “去银行取钱” 与 “河边有银行” 中，向量会因上下文差异完全不同）。</p><h4 id="_3-1-bert-双向上下文理解的核心模型" tabindex="-1">3.1 BERT：双向上下文理解的核心模型 <a class="header-anchor" href="#_3-1-bert-双向上下文理解的核心模型" aria-label="Permalink to &quot;3.1 BERT：双向上下文理解的核心模型&quot;">​</a></h4><h5 id="_1-全称与定位" tabindex="-1">（1）全称与定位 <a class="header-anchor" href="#_1-全称与定位" aria-label="Permalink to &quot;（1）全称与定位&quot;">​</a></h5><p>BERT（Bidirectional Encoder Representations from Transformers，即 “基于 Transformer 的双向编码器表示”）是专为<strong>文本理解场景</strong>设计的模型，核心优势是能同时从词语的 “上文” 和 “下文” 捕捉语义信息，适合需要深度解析文本含义的任务。</p><h5 id="_2-核心技术-双向编码器-自注意力机制" tabindex="-1">（2）核心技术：双向编码器 + 自注意力机制 <a class="header-anchor" href="#_2-核心技术-双向编码器-自注意力机制" aria-label="Permalink to &quot;（2）核心技术：双向编码器 + 自注意力机制&quot;">​</a></h5><ul><li><p>基础架构：基于 Transformer 框架，但仅使用其中的 “编码器（Encoder）” 部分；</p></li><li><p>双向设计：分析任意词语时，模型会同时关注该词元上文和下文，以捕获词语在句子中的完整语义信息。</p><p>示例：分析句子 “小明在公园散步” 中的 “公园” 时，模型会同步关联上文 “小明在” 与下文 “散步”，通过自注意力机制计算 “公园” 与这两个片段的关联权重（如 “公园” 与 “散步” 的关联权重高于 “小明”），从而精准定位 “公园” 在该句中的语义角色（即 “散步的场所”）。</p></li></ul><h5 id="_3-预训练任务-遮掩语言模型-masked-language-model-mlm" tabindex="-1">（3）预训练任务：遮掩语言模型（Masked Language Model, MLM） <a class="header-anchor" href="#_3-预训练任务-遮掩语言模型-masked-language-model-mlm" aria-label="Permalink to &quot;（3）预训练任务：遮掩语言模型（Masked Language Model, MLM）&quot;">​</a></h5><p>MLM 任务的核心是让模型学习 “上下文语义关联”，具体流程如下：</p><ol><li><p>从输入句子中随机挑选 15% 的词语，用特殊符号 “[MASK]” 替换（例如将 “猫在沙发上睡觉” 改为 “猫在 [MASK] 上睡觉”）；</p></li><li><p>模型需基于 “[MASK]” 前后的双向上下文信息，预测被遮掩的原词；</p></li><li><p>经过大规模文本训练后，模型能掌握词语在不同语境下的搭配规律（如 “沙发” 常与 “上”“睡觉” 搭配），最终可生成 “词级”（单个词的向量）和 “句子级”（整个句子的向量）两类动态嵌入。</p></li></ol><h5 id="_4-输出特点与应用场景" tabindex="-1">（4）输出特点与应用场景 <a class="header-anchor" href="#_4-输出特点与应用场景" aria-label="Permalink to &quot;（4）输出特点与应用场景&quot;">​</a></h5><ul><li><p>向量特点：对 “上下文语义差异” 敏感度极高，能精准区分多义词在不同场景的含义；</p></li><li><p>适用任务：文本分类、情感分析、问答系统（如判断 “苹果” 是 “水果” 还是 “科技品牌”）、文本相似度计算等需 “深度理解文本含义” 的场景。</p></li></ul><h4 id="_3-2-gpt-单向文本生成的标杆模型" tabindex="-1">3.2 GPT：单向文本生成的标杆模型 <a class="header-anchor" href="#_3-2-gpt-单向文本生成的标杆模型" aria-label="Permalink to &quot;3.2 GPT：单向文本生成的标杆模型&quot;">​</a></h4><h5 id="_1-全称与定位-1" tabindex="-1">（1）全称与定位 <a class="header-anchor" href="#_1-全称与定位-1" aria-label="Permalink to &quot;（1）全称与定位&quot;">​</a></h5><p>GPT（Generative Pre-trained Transformer，即 “生成式预训练 Transformer”）是专为<strong>文本生成场景</strong>设计的模型，核心优势是能基于 “前文内容” 预测下文，生成具有连贯性的上下文相关向量，在逐词产出文本的任务中表现突出。</p><h5 id="_2-核心技术-单向解码器-自注意力机制" tabindex="-1">（2）核心技术：单向解码器 + 自注意力机制 <a class="header-anchor" href="#_2-核心技术-单向解码器-自注意力机制" aria-label="Permalink to &quot;（2）核心技术：单向解码器 + 自注意力机制&quot;">​</a></h5><ul><li><p>基础架构：基于 Transformer 框架，但仅使用其中的 “解码器（Decoder）” 部分（注：早期 GPT-1、GPT-2 为 “仅解码器架构”，聚焦单向序列建模）；</p></li><li><p>单向设计：分析任意词语时，模型仅能关注其 “上文”，无法提前获取下文信息。</p><p>示例：分析句子 “小明早上起来后” 中的 “后” 时，模型仅能关联上文 “小明早上起来”，无法预知下文可能出现的 “去吃早餐”；自注意力机制在此处的作用是计算前文每个词对 “后” 的影响权重（如 “早上” 对 “后” 的关联权重高于 “小明”），确保模型按 “时间顺序” 理解文本逻辑。</p></li></ul><h5 id="_3-预训练任务-自回归语言模型-autoregressive-language-model" tabindex="-1">（3）预训练任务：自回归语言模型（Autoregressive Language Model） <a class="header-anchor" href="#_3-预训练任务-自回归语言模型-autoregressive-language-model" aria-label="Permalink to &quot;（3）预训练任务：自回归语言模型（Autoregressive Language Model）&quot;">​</a></h5><p>自回归任务的核心是让模型学习 “文本序列连贯性”，具体流程如下：</p><ol><li><p>给模型输入一段文本的前 N 个词（例如 “今天天气很”），要求模型预测第 N+1 个词（可能输出 “好”“热”“冷” 等合理结果）；</p></li><li><p>将 “前 N+1 个词” 作为新输入，继续让模型预测第 N+2 个词，以此类推（类似 “文本接龙”）；</p></li><li><p>经过大规模训练后，模型能掌握文本的序列规律（如 “天气很” 后更可能接 “好” 而非 “苹果”），生成的词向量天然带有 “预测下文” 的属性，为后续生成任务提供支撑。</p></li></ol><h5 id="_4-输出特点与应用场景-1" tabindex="-1">（4）输出特点与应用场景 <a class="header-anchor" href="#_4-输出特点与应用场景-1" aria-label="Permalink to &quot;（4）输出特点与应用场景&quot;">​</a></h5><ul><li><p>向量特点：更侧重 “文本序列连贯性”，能保证生成内容的逻辑流畅性；</p></li><li><p>适用任务：文本生成（如文章写作、诗歌创作）、对话机器人（连续聊天）、机器翻译（逐句生成目标语言）、文本摘要等需 “逐词产出文本” 的场景。</p></li></ul><h2 id="_4-python-代码实战-直观验证动态向量" tabindex="-1">4.Python 代码实战：直观验证动态向量 <a class="header-anchor" href="#_4-python-代码实战-直观验证动态向量" aria-label="Permalink to &quot;4.Python 代码实战：直观验证动态向量&quot;">​</a></h2><h3 id="_4-1-环境准备" tabindex="-1">4.1 环境准备 <a class="header-anchor" href="#_4-1-环境准备" aria-label="Permalink to &quot;4.1 环境准备&quot;">​</a></h3><p>首先安装依赖库（gensim提供 Word2Vec 的实现及预训练模型）：</p><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install transformers  #加载模型</span></span>
<span class="line"><span>pip install torch   # 深度学习框架</span></span>
<span class="line"><span>pip install modelscope  #下载模型</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h3 id="_4-2-直观比较动态向量的" tabindex="-1">4.2 直观比较动态向量的 <a class="header-anchor" href="#_4-2-直观比较动态向量的" aria-label="Permalink to &quot;4.2 直观比较动态向量的&quot;">​</a></h3><p>模型文件可通过魔塔社区下载： <a href="https://www.modelscope.cn/models/iic/nlp_bert_entity-embedding_chinese-base/summary" target="_blank" rel="noreferrer">https://www.modelscope.cn/models/iic/nlp_bert_entity-embedding_chinese-base/summary</a> 本次演示项目使用阿里巴巴开源的Bert实体向量-中文-通用领域-base模型</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> os</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> transformers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> BertTokenizer, BertModel</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> test_dynamic_embedding</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tokenizer, bert_model, target_word, context_sentences, desc_list):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    测试动态向量（BERT）核心特性：多义词随上下文生成不同向量</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param tokenizer: BERT分词器（用于将句子转为模型可识别的token）</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param bert_model: BERT预训练模型（用于生成动态向量）</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param target_word: 待测试的多义词（如“打”）</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param context_sentences: 包含多义词的不同上下文句子（每种句子对应一种语义）</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    :param desc_list: 每个上下文句子对应的语义描述（与context_sentences一一对应）</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;=&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">60</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;【动态向量（BERT）测试：多义词&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target_word</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;】&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    dynamic_vecs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储不同上下文下多义词的动态向量（含语义描述和句子）</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 1. 为每个上下文生成多义词的动态向量</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, (sentence, desc) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">zip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(context_sentences, desc_list), </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 步骤1：BERT分词（将自然句子转为token ID、注意力掩码）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tokenizer(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            sentence,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">            return_tensors</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;pt&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 返回PyTorch张量（模型默认输入格式）</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">            padding</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,         </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 自动补全长度（确保输入维度统一）</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">            truncation</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">       # 自动截断过长句子（避免模型输入超界）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 步骤2：通过BERT模型生成隐藏层输出（动态向量来自最后一层）</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.no_grad():  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 关闭梯度计算（仅推理，不训练，加快速度）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            outputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> bert_model(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">inputs)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # last_hidden_state：BERT最后一层隐藏层输出，形状为 [1, seq_len, hidden_size]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # 1=批次大小（单句子），seq_len=句子token数量，hidden_size=向量维度（BERT-base为768）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            last_hidden_state </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs.last_hidden_state</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 步骤3：定位多义词在token序列中的位置（确保提取正确词的向量）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        token_ids </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input_ids&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].tolist()  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 取出当前句子的所有token ID</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        target_token_id </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tokenizer.convert_tokens_to_ids(target_word)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 多义词对应的token ID</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        try</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            target_pos </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> token_ids.index(target_token_id)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 找到多义词在序列中的索引</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        except</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> ValueError</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;⚠️ 句子&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sentence</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;中未找到&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target_word</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;，跳过该语境测试&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">            continue</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 步骤4：提取多义词的动态向量（对应位置的最后一层隐藏状态）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        target_vec </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> last_hidden_state[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, target_pos, :].numpy()  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 转为numpy数组（方便后续计算）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        dynamic_vecs.append((target_vec, desc, sentence))  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 存储向量、语义描述、原句</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 打印当前语境的基础信息</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">--- 语境</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">idx</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">desc</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ---&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;上下文句子：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sentence</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;动态向量维度：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target_vec.shape[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">（BERT-base默认768维）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 2. 计算不同语境下多义词的向量相似度（核心对比，体现动态特性）</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> +</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;-&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;【关键对比：不同语境下动态向量的相似度】&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(dynamic_vecs) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;⚠️ 有效语境不足2个，无法计算相似度对比&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 遍历所有语境组合，计算余弦相似度</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(dynamic_vecs)):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        vec_i, desc_i, sent_i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dynamic_vecs[i]</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> j </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(dynamic_vecs)):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            vec_j, desc_j, sent_j </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> dynamic_vecs[j]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # 余弦相似度公式：cosθ = (A·B) / (||A|| × ||B||)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            cos_sim </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.dot(vec_i, vec_j) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (np.linalg.norm(vec_i) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.linalg.norm(vec_j))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;语境</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">i</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">（</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">desc_i</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">）与语境</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">j</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">（</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">desc_j</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">）：相似度 = </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cos_sim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:.4f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 3. 总结动态向量核心特性</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> +</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;-&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;【动态向量核心结论】&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;1. 多义词&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target_word</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;在不同语境下的向量相似度显著低于1.0（静态向量恒为1.0）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;2. 语义差异越大，向量相似度越低（例如“击打”与“编织”的相似度远低于“游戏”与“比赛”）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;3. 动态向量能精准捕捉上下文语义，解决静态向量“一词一向量”的局限性&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;=&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">60</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __name__</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> ==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;__main__&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # --------------------------</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 1. 加载BERT模型与分词器（仅依赖ModelScope）</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # --------------------------</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 配置：ModelScope模型仓库ID + 本地保存路径（固定为ModelScope下载后的路径格式）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model_repo_id </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;iic/nlp_bert_entity-embedding_chinese-base&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 用户指定的ModelScope模型</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # ModelScope下载后，模型会保存在 cache_dir/仓库ID/版本 路径下，此处预设标准路径</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    local_model_dir </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;./model/</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model_repo_id</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    bert_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    try</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 第一步：检查本地是否已有ModelScope下载的模型</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> os.path.exists(local_model_dir):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;🔍 检测到本地ModelScope模型：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">local_model_dir</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">，正在加载...&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> BertTokenizer.from_pretrained(local_model_dir)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            bert_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> BertModel.from_pretrained(local_model_dir)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;✅ 本地ModelScope模型加载成功！&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 第二步：本地无模型，触发ModelScope下载</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        else</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">            from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modelscope </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> snapshot_download</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;🔍 未检测到本地模型，开始通过ModelScope下载：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model_repo_id</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # 调用ModelScope下载模型，指定缓存目录为./model</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            downloaded_dir </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> snapshot_download(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">                repo_id</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model_repo_id,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">                cache_dir</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;./model&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 模型下载后自动保存在 ./model/仓库ID/master 下</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">                revision</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;master&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">     # 模型版本（固定为master，匹配预设的local_model_dir）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            )</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;✅ 模型下载完成，保存路径：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">downloaded_dir</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # 从下载路径加载模型和分词器</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> BertTokenizer.from_pretrained(downloaded_dir)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            bert_model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> BertModel.from_pretrained(downloaded_dir)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;✅ 从ModelScope下载路径加载模型成功！&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 捕获所有模型加载相关错误（下载失败、路径错误、权限问题等）</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    except</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Exception</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> e:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;❌ 模型加载失败：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{str</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(e)</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;💡 建议：1. 检查网络连接（确保能访问ModelScope）；2. 确认./model目录有写入权限；3. 无需手动修改路径，ModelScope会自动管理&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        exit</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # --------------------------</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 2. 定义测试数据（多义词“打”的4种典型语境）</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # --------------------------</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    target_word </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;打&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 待测试的多义词</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 包含“打”的不同上下文句子（每种句子对应一种独特语义）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    context_sentences </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;他用拳头打坏人&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,          </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 语境1：击打动作</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;他在电脑上打游戏&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,        </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 语境2：进行活动</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;妈妈在灯下打毛衣&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,        </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 语境3：制作/编织</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;他给家人打电话&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">          # 语境4：通讯行为</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 每个语境对应的语义描述（与上下文句子一一对应）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    desc_list </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;击打动作（例：用拳头打坏人）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;进行活动（例：打游戏、打篮球）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;制作/编织（例：打毛衣、打家具）&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;通讯行为（例：打电话、打视频）&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # --------------------------</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 3. 执行动态向量测试</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # --------------------------</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    test_dynamic_embedding(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">        tokenizer</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tokenizer,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">        bert_model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">bert_model,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">        target_word</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">target_word,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">        context_sentences</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_sentences,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">        desc_list</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">desc_list</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br><span class="line-number">110</span><br><span class="line-number">111</span><br><span class="line-number">112</span><br><span class="line-number">113</span><br><span class="line-number">114</span><br><span class="line-number">115</span><br><span class="line-number">116</span><br><span class="line-number">117</span><br><span class="line-number">118</span><br><span class="line-number">119</span><br><span class="line-number">120</span><br><span class="line-number">121</span><br><span class="line-number">122</span><br><span class="line-number">123</span><br><span class="line-number">124</span><br><span class="line-number">125</span><br><span class="line-number">126</span><br><span class="line-number">127</span><br><span class="line-number">128</span><br><span class="line-number">129</span><br><span class="line-number">130</span><br><span class="line-number">131</span><br><span class="line-number">132</span><br><span class="line-number">133</span><br><span class="line-number">134</span><br><span class="line-number">135</span><br><span class="line-number">136</span><br><span class="line-number">137</span><br><span class="line-number">138</span><br><span class="line-number">139</span><br><span class="line-number">140</span><br><span class="line-number">141</span><br><span class="line-number">142</span><br><span class="line-number">143</span><br><span class="line-number">144</span><br><span class="line-number">145</span><br><span class="line-number">146</span><br><span class="line-number">147</span><br><span class="line-number">148</span><br><span class="line-number">149</span><br><span class="line-number">150</span><br><span class="line-number">151</span><br></div></div><div class="language-text vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>--- 语境1：击打动作（例：用拳头打坏人） ---</span></span>
<span class="line"><span>上下文句子：他用拳头打坏人</span></span>
<span class="line"><span>动态向量维度：768（BERT-base默认768维）</span></span>
<span class="line"><span></span></span>
<span class="line"><span>--- 语境2：进行活动（例：打游戏、打篮球） ---</span></span>
<span class="line"><span>上下文句子：他在电脑上打游戏</span></span>
<span class="line"><span>动态向量维度：768（BERT-base默认768维）</span></span>
<span class="line"><span></span></span>
<span class="line"><span>--- 语境3：制作/编织（例：打毛衣、打家具） ---</span></span>
<span class="line"><span>上下文句子：妈妈在灯下打毛衣</span></span>
<span class="line"><span>动态向量维度：768（BERT-base默认768维）</span></span>
<span class="line"><span></span></span>
<span class="line"><span>--- 语境4：通讯行为（例：打电话、打视频） ---</span></span>
<span class="line"><span>上下文句子：他给家人打电话</span></span>
<span class="line"><span>动态向量维度：768（BERT-base默认768维）</span></span>
<span class="line"><span></span></span>
<span class="line"><span>--------------------------------------------------</span></span>
<span class="line"><span>【关键对比：不同语境下动态向量的相似度】</span></span>
<span class="line"><span>语境1（击打动作（例：用拳头打坏人））与语境2（进行活动（例：打游戏、打篮球））：相似度 = 0.8171</span></span>
<span class="line"><span>语境1（击打动作（例：用拳头打坏人））与语境3（制作/编织（例：打毛衣、打家具））：相似度 = 0.8417</span></span>
<span class="line"><span>语境1（击打动作（例：用拳头打坏人））与语境4（通讯行为（例：打电话、打视频））：相似度 = 0.7743</span></span>
<span class="line"><span>语境2（进行活动（例：打游戏、打篮球））与语境3（制作/编织（例：打毛衣、打家具））：相似度 = 0.8723</span></span>
<span class="line"><span>语境2（进行活动（例：打游戏、打篮球））与语境4（通讯行为（例：打电话、打视频））：相似度 = 0.8177</span></span>
<span class="line"><span>语境3（制作/编织（例：打毛衣、打家具））与语境4（通讯行为（例：打电话、打视频））：相似度 = 0.8096</span></span>
<span class="line"><span></span></span>
<span class="line"><span>--------------------------------------------------</span></span>
<span class="line"><span>【动态向量核心结论】</span></span>
<span class="line"><span>1. 多义词&#39;打&#39;在不同语境下的向量相似度显著低于1.0（静态向量恒为1.0）</span></span>
<span class="line"><span>2. 语义差异越大，向量相似度越低（例如“击打”与“编织”的相似度远低于“游戏”与“比赛”）</span></span>
<span class="line"><span>3. 动态向量能精准捕捉上下文语义，解决静态向量“一词一向量”的局限性</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div>`,78)])])}const u=i(k,[["render",r]]);export{y as __pageData,u as default};
