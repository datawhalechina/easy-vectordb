#!/usr/bin/env python3
"""
æ–°çš„åç«¯API
ä½¿ç”¨é¢„è¿æ¥æ¶æ„ï¼Œé¿å…è¿æ¥é˜»å¡é—®é¢˜
"""

from fastapi import FastAPI, UploadFile, File, Form, Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import os
import yaml
import logging
from typing import List, Dict, Any, Optional
import time
from datetime import datetime
from pydantic import BaseModel

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Cre_milvus API",
    description="å‘é‡æ•°æ®åº“ç®¡ç†APIï¼Œæ”¯æŒé¢„è¿æ¥æ¶æ„",
    version="2.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ¨¡å‹
class MilvusConfig(BaseModel):
    host: str
    port: str
    collection_name: str
    vector_name: str = "default"
    index_name: str = "IVF_FLAT"
    replica_num: int = 1
    index_device: str = "cpu"

class SystemConfig(BaseModel):
    url_split: bool = False
    insert_mode: str = "è¦†ç›–ï¼ˆåˆ é™¤åŸæœ‰æ•°æ®ï¼‰"

class SearchConfig(BaseModel):
    top_k: int = 20
    col_choice: str = "hdbscan"
    reorder_strategy: str = "distance"

class DataConfig(BaseModel):
    data_location: str

class ChunkingConfig(BaseModel):
    strategy: str = "traditional"
    chunk_length: int = 512
    ppl_threshold: float = 0.3
    language: str = "zh"

class ConfigRequest(BaseModel):
    milvus: MilvusConfig
    system: SystemConfig
    search: SearchConfig
    data: DataConfig
    chunking: ChunkingConfig

class CollectionStateManager:
    """é›†åˆçŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self):
        self._collection_states = {}
        self._state_lock = {}
    
    def ensure_collection_loaded(self, collection_name: str) -> bool:
        """ç¡®ä¿é›†åˆå·²åŠ è½½"""
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self._collection_exists(collection_name):
                logger.info(f"é›†åˆ {collection_name} ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º")
                return self._create_collection_if_needed(collection_name)
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å·²åŠ è½½
            if not self._is_collection_loaded(collection_name):
                logger.info(f"é›†åˆ {collection_name} æœªåŠ è½½ï¼Œå¼€å§‹åŠ è½½")
                return self.load_collection_with_retry(collection_name)
            
            logger.info(f"é›†åˆ {collection_name} å·²åŠ è½½")
            return True
            
        except Exception as e:
            logger.error(f"ç¡®ä¿é›†åˆåŠ è½½å¤±è´¥: {e}")
            return False
    
    def get_collection_status(self, collection_name: str) -> Dict[str, Any]:
        """è·å–é›†åˆçŠ¶æ€"""
        try:
            exists = self._collection_exists(collection_name)
            loaded = self._is_collection_loaded(collection_name) if exists else False
            
            status = {
                "name": collection_name,
                "exists": exists,
                "loaded": loaded,
                "last_checked": datetime.now().isoformat(),
                "status": "ready" if (exists and loaded) else "not_ready"
            }
            
            if collection_name in self._collection_states:
                status.update(self._collection_states[collection_name])
            
            return status
            
        except Exception as e:
            logger.error(f"è·å–é›†åˆçŠ¶æ€å¤±è´¥: {e}")
            return {
                "name": collection_name,
                "exists": False,
                "loaded": False,
                "error": str(e),
                "status": "error"
            }
    
    def create_collection_if_not_exists(self, collection_name: str, schema: Dict = None) -> bool:
        """å¦‚æœé›†åˆä¸å­˜åœ¨åˆ™åˆ›å»º"""
        try:
            if self._collection_exists(collection_name):
                return True
            
            logger.info(f"åˆ›å»ºé›†åˆ: {collection_name}")
            return self._create_collection_if_needed(collection_name, schema)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            return False
    
    def load_collection_with_retry(self, collection_name: str, max_retries: int = 3) -> bool:
        """å¸¦é‡è¯•çš„é›†åˆåŠ è½½"""
        for attempt in range(max_retries):
            try:
                logger.info(f"å°è¯•åŠ è½½é›†åˆ {collection_name} (ç¬¬ {attempt + 1} æ¬¡)")
                
                # æ›´æ–°çŠ¶æ€
                self._collection_states[collection_name] = {
                    "load_status": "loading",
                    "load_attempt": attempt + 1,
                    "last_load_time": datetime.now().isoformat()
                }
                
                # æ‰§è¡ŒåŠ è½½
                success = self._load_collection(collection_name)
                
                if success:
                    self._collection_states[collection_name].update({
                        "load_status": "loaded",
                        "loaded_at": datetime.now().isoformat()
                    })
                    logger.info(f"é›†åˆ {collection_name} åŠ è½½æˆåŠŸ")
                    return True
                else:
                    logger.warning(f"é›†åˆ {collection_name} åŠ è½½å¤±è´¥ï¼Œå°è¯• {attempt + 1}/{max_retries}")
                    
            except Exception as e:
                logger.error(f"åŠ è½½é›†åˆæ—¶å‡ºé”™ (å°è¯• {attempt + 1}): {e}")
                
            # ç­‰å¾…åé‡è¯•
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        self._collection_states[collection_name] = {
            "load_status": "failed",
            "error": "åŠ è½½é‡è¯•æ¬¡æ•°å·²ç”¨å®Œ",
            "failed_at": datetime.now().isoformat()
        }
        return False
    
    def _collection_exists(self, collection_name: str) -> bool:
        """æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨"""
        try:
            from milvusBuilder.fast_insert import check_collection_exists
            return check_collection_exists(collection_name)
        except ImportError:
            # å¦‚æœæ²¡æœ‰fast_insertæ¨¡å—ï¼Œå°è¯•å…¶ä»–æ–¹å¼
            try:
                from milvusBuilder.persistent_connection import get_persistent_connection
                from pymilvus import utility
                conn = get_persistent_connection()
                connection_alias = conn.get_connection_alias()
                if connection_alias:
                    return utility.has_collection(collection_name, using=connection_alias)
                else:
                    logger.error("æ²¡æœ‰å¯ç”¨çš„Milvusè¿æ¥")
                    return False
            except Exception as e:
                logger.error(f"æ£€æŸ¥é›†åˆå­˜åœ¨æ€§å¤±è´¥: {e}")
                return False
    
    def _is_collection_loaded(self, collection_name: str) -> bool:
        """æ£€æŸ¥é›†åˆæ˜¯å¦å·²åŠ è½½"""
        try:
            from milvusBuilder.fast_insert import is_collection_loaded
            return is_collection_loaded(collection_name)
        except ImportError:
            # å¦‚æœæ²¡æœ‰fast_insertæ¨¡å—ï¼Œå°è¯•å…¶ä»–æ–¹å¼
            try:
                from milvusBuilder.persistent_connection import get_persistent_connection
                from pymilvus import Collection
                conn = get_persistent_connection()
                connection_alias = conn.get_connection_alias()
                if connection_alias:
                    # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
                    if not self._collection_exists(collection_name):
                        return False
                    # æ£€æŸ¥é›†åˆæ˜¯å¦å·²åŠ è½½
                    collection = Collection(collection_name, using=connection_alias)
                    return collection.is_loaded
                else:
                    logger.error("æ²¡æœ‰å¯ç”¨çš„Milvusè¿æ¥")
                    return False
            except Exception as e:
                logger.error(f"æ£€æŸ¥é›†åˆåŠ è½½çŠ¶æ€å¤±è´¥: {e}")
                return False
    
    def _load_collection(self, collection_name: str) -> bool:
        """åŠ è½½é›†åˆ"""
        try:
            from milvusBuilder.fast_insert import load_collection
            return load_collection(collection_name)
        except ImportError:
            # å¦‚æœæ²¡æœ‰fast_insertæ¨¡å—ï¼Œå°è¯•å…¶ä»–æ–¹å¼
            try:
                from milvusBuilder.persistent_connection import get_persistent_connection
                from pymilvus import Collection
                conn = get_persistent_connection()
                connection_alias = conn.get_connection_alias()
                if connection_alias:
                    collection = Collection(collection_name, using=connection_alias)
                    collection.load()
                    logger.info(f"é›†åˆ {collection_name} åŠ è½½æˆåŠŸ")
                    return True
                else:
                    logger.error("æ²¡æœ‰å¯ç”¨çš„Milvusè¿æ¥")
                    return False
            except Exception as e:
                logger.error(f"åŠ è½½é›†åˆå¤±è´¥: {e}")
                return False
    
    def _create_collection_if_needed(self, collection_name: str, schema: Dict = None) -> bool:
        """åˆ›å»ºé›†åˆï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        try:
            from milvusBuilder.fast_insert import create_collection_with_schema
            return create_collection_with_schema(collection_name, schema)
        except ImportError:
            # å¦‚æœæ²¡æœ‰fast_insertæ¨¡å—ï¼Œå°è¯•å…¶ä»–æ–¹å¼
            try:
                from milvusBuilder.persistent_connection import get_persistent_connection
                from pymilvus import Collection, FieldSchema, CollectionSchema, DataType
                conn = get_persistent_connection()
                connection_alias = conn.get_connection_alias()
                if connection_alias:
                    # å¦‚æœæ²¡æœ‰æä¾›schemaï¼Œä½¿ç”¨é»˜è®¤schema
                    if schema is None:
                        # åˆ›å»ºé»˜è®¤schema
                        fields = [
                            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=768),
                            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535)
                        ]
                        schema = CollectionSchema(fields, "Default collection schema", enable_dynamic_field=True)
                    
                    collection = Collection(name=collection_name, schema=schema, using=connection_alias)
                    logger.info(f"é›†åˆ {collection_name} åˆ›å»ºæˆåŠŸ")
                    return True
                else:
                    logger.error("æ²¡æœ‰å¯ç”¨çš„Milvusè¿æ¥")
                    return False
            except Exception as e:
                logger.error(f"åˆ›å»ºé›†åˆå¤±è´¥: {e}")
                return False

class InsertProgressTracker:
    """æ’å…¥è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self._progress_data = {}
        self._tracking_counter = 0
    
    def start_tracking(self, total_items: int) -> str:
        """å¼€å§‹è·Ÿè¸ªæ’å…¥è¿›åº¦"""
        self._tracking_counter += 1
        tracking_id = f"insert_{self._tracking_counter}_{int(time.time())}"
        
        self._progress_data[tracking_id] = {
            "tracking_id": tracking_id,
            "total_items": total_items,
            "processed_items": 0,
            "failed_items": 0,
            "start_time": datetime.now(),
            "estimated_completion": None,
            "current_status": "preparing",
            "error_details": [],
            "last_update": datetime.now()
        }
        
        logger.info(f"å¼€å§‹è·Ÿè¸ªæ’å…¥è¿›åº¦: {tracking_id}, æ€»é¡¹ç›®æ•°: {total_items}")
        return tracking_id
    
    def update_progress(self, tracking_id: str, processed: int, failed: int = 0, status: str = "inserting") -> None:
        """æ›´æ–°æ’å…¥è¿›åº¦"""
        if tracking_id not in self._progress_data:
            logger.warning(f"è·Ÿè¸ªID {tracking_id} ä¸å­˜åœ¨")
            return
        
        progress = self._progress_data[tracking_id]
        progress["processed_items"] = processed
        progress["failed_items"] = failed
        progress["current_status"] = status
        progress["last_update"] = datetime.now()
        
        # è®¡ç®—é¢„ä¼°å®Œæˆæ—¶é—´
        if processed > 0:
            elapsed_time = (datetime.now() - progress["start_time"]).total_seconds()
            items_per_second = processed / elapsed_time
            remaining_items = progress["total_items"] - processed
            
            if items_per_second > 0:
                from datetime import timedelta
                estimated_seconds = remaining_items / items_per_second
                progress["estimated_completion"] = datetime.now() + timedelta(seconds=estimated_seconds)
        
        logger.debug(f"æ›´æ–°è¿›åº¦ {tracking_id}: {processed}/{progress['total_items']} é¡¹å·²å¤„ç†")
    
    def get_progress_status(self, tracking_id: str) -> Dict[str, Any]:
        """è·å–æ’å…¥è¿›åº¦çŠ¶æ€"""
        if tracking_id not in self._progress_data:
            return {
                "error": "è·Ÿè¸ªIDä¸å­˜åœ¨",
                "status": "not_found"
            }
        
        progress = self._progress_data[tracking_id]
        
        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
        progress_percentage = 0
        if progress["total_items"] > 0:
            progress_percentage = (progress["processed_items"] / progress["total_items"]) * 100
        
        # è®¡ç®—å¤„ç†é€Ÿåº¦
        elapsed_time = (datetime.now() - progress["start_time"]).total_seconds()
        items_per_second = progress["processed_items"] / elapsed_time if elapsed_time > 0 else 0
        
        return {
            "tracking_id": tracking_id,
            "total_items": progress["total_items"],
            "processed_items": progress["processed_items"],
            "failed_items": progress["failed_items"],
            "progress_percentage": round(progress_percentage, 2),
            "current_status": progress["current_status"],
            "start_time": progress["start_time"].isoformat(),
            "last_update": progress["last_update"].isoformat(),
            "estimated_completion": progress["estimated_completion"].isoformat() if progress["estimated_completion"] else None,
            "items_per_second": round(items_per_second, 2),
            "error_details": progress["error_details"],
            "status": "active"
        }
    
    def finish_tracking(self, tracking_id: str, success: bool, final_message: str = "") -> Dict[str, Any]:
        """å®Œæˆæ’å…¥è¿›åº¦è·Ÿè¸ª"""
        if tracking_id not in self._progress_data:
            return {
                "error": "è·Ÿè¸ªIDä¸å­˜åœ¨",
                "status": "not_found"
            }
        
        progress = self._progress_data[tracking_id]
        progress["current_status"] = "completed" if success else "failed"
        progress["last_update"] = datetime.now()
        
        if final_message:
            progress["final_message"] = final_message
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        elapsed_time = (datetime.now() - progress["start_time"]).total_seconds()
        from datetime import timedelta
        final_stats = {
            "tracking_id": tracking_id,
            "success": success,
            "total_items": progress["total_items"],
            "processed_items": progress["processed_items"],
            "failed_items": progress["failed_items"],
            "success_rate": (progress["processed_items"] / progress["total_items"]) * 100 if progress["total_items"] > 0 else 0,
            "total_time_seconds": round(elapsed_time, 2),
            "average_items_per_second": round(progress["processed_items"] / elapsed_time, 2) if elapsed_time > 0 else 0,
            "final_message": final_message,
            "completed_at": datetime.now().isoformat()
        }
        
        logger.info(f"æ’å…¥è·Ÿè¸ªå®Œæˆ {tracking_id}: æˆåŠŸ={success}, å¤„ç†={progress['processed_items']}/{progress['total_items']}")
        
        return final_stats
    
    def add_error(self, tracking_id: str, error_message: str) -> None:
        """æ·»åŠ é”™è¯¯ä¿¡æ¯"""
        if tracking_id in self._progress_data:
            self._progress_data[tracking_id]["error_details"].append({
                "timestamp": datetime.now().isoformat(),
                "message": error_message
            })
    
    def cleanup_old_tracking(self, max_age_hours: int = 24) -> None:
        """æ¸…ç†æ—§çš„è·Ÿè¸ªæ•°æ®"""
        from datetime import timedelta
        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        
        to_remove = []
        for tracking_id, progress in self._progress_data.items():
            if progress["last_update"] < cutoff_time:
                to_remove.append(tracking_id)
        
        for tracking_id in to_remove:
            del self._progress_data[tracking_id]
            logger.info(f"æ¸…ç†æ—§çš„è·Ÿè¸ªæ•°æ®: {tracking_id}")

class ErrorRecoveryManager:
    """é”™è¯¯æ¢å¤ç®¡ç†å™¨"""
    
    def __init__(self):
        self._error_history = []
        self._recovery_strategies = {
            "glm_config_error": self._handle_glm_config_error,
            "chunking_error": self._handle_chunking_error,
            "collection_error": self._handle_collection_error,
            "connection_error": self._handle_connection_error
        }
    
    def handle_error(self, error_type: str, error: Exception, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç»Ÿä¸€é”™è¯¯å¤„ç†å…¥å£"""
        if context is None:
            context = {}
        
        # è®°å½•é”™è¯¯
        error_record = {
            "timestamp": datetime.now().isoformat(),
            "error_type": error_type,
            "error_message": str(error),
            "context": context,
            "recovery_attempted": False
        }
        
        self._error_history.append(error_record)
        logger.error(f"å¤„ç†é”™è¯¯ [{error_type}]: {str(error)}")
        
        # å°è¯•æ¢å¤
        recovery_action = self._attempt_recovery(error_type, error, context)
        error_record["recovery_attempted"] = True
        error_record["recovery_action"] = recovery_action
        
        return recovery_action
    
    def _attempt_recovery(self, error_type: str, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """å°è¯•é”™è¯¯æ¢å¤"""
        handler = self._recovery_strategies.get(error_type)
        
        if handler:
            try:
                return handler(error, context)
            except Exception as recovery_error:
                logger.error(f"æ¢å¤ç­–ç•¥æ‰§è¡Œå¤±è´¥: {recovery_error}")
                return {
                    "action": "manual_intervention_required",
                    "success": False,
                    "message": f"è‡ªåŠ¨æ¢å¤å¤±è´¥: {str(recovery_error)}",
                    "suggestions": ["è¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®", "è”ç³»æŠ€æœ¯æ”¯æŒ"]
                }
        else:
            return {
                "action": "no_recovery_strategy",
                "success": False,
                "message": f"æœªæ‰¾åˆ° {error_type} çš„æ¢å¤ç­–ç•¥",
                "suggestions": ["è¯·æ‰‹åŠ¨å¤„ç†æ­¤é”™è¯¯"]
            }
    
    def _handle_glm_config_error(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†GLMé…ç½®é”™è¯¯"""
        error_msg = str(error).lower()
        
        if "api" in error_msg and "key" in error_msg:
            return {
                "action": "invalid_api_key",
                "success": False,
                "message": "APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ",
                "suggestions": [
                    "æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤APIå¯†é’¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è°ƒç”¨é¢åº¦",
                    "å°è¯•é‡æ–°ç”ŸæˆAPIå¯†é’¥"
                ],
                "recovery_steps": [
                    "è®¿é—®æ™ºè°±AIå¼€æ”¾å¹³å°",
                    "æ£€æŸ¥APIå¯†é’¥çŠ¶æ€",
                    "æ›´æ–°é…ç½®ä¸­çš„APIå¯†é’¥"
                ]
            }
        elif "network" in error_msg or "connection" in error_msg:
            return {
                "action": "network_issue",
                "success": False,
                "message": "ç½‘ç»œè¿æ¥é—®é¢˜",
                "suggestions": [
                    "æ£€æŸ¥ç½‘ç»œè¿æ¥",
                    "ç¡®è®¤é˜²ç«å¢™è®¾ç½®",
                    "ç¨åé‡è¯•"
                ]
            }
        else:
            return {
                "action": "general_glm_error",
                "success": False,
                "message": "GLMé…ç½®é”™è¯¯",
                "suggestions": [
                    "æ£€æŸ¥GLMé…ç½®æ˜¯å¦å®Œæ•´",
                    "éªŒè¯æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®",
                    "é‡æ–°é…ç½®GLMè®¾ç½®"
                ]
            }
    
    def _handle_chunking_error(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†åˆ†å—é”™è¯¯"""
        error_msg = str(error).lower()
        
        if "dependency" in error_msg or "import" in error_msg:
            return {
                "action": "missing_dependencies",
                "success": False,
                "message": "åˆ†å—ä¾èµ–ç¼ºå¤±",
                "suggestions": [
                    "å®‰è£…ç¼ºå¤±çš„PythonåŒ…",
                    "æ£€æŸ¥torchæ˜¯å¦æ­£ç¡®å®‰è£…",
                    "éªŒè¯perplexity_chunkingæ¨¡å—"
                ],
                "recovery_steps": [
                    "pip install torch",
                    "pip install nltk jieba",
                    "ç¡®ä¿perplexity_chunking.pyæ–‡ä»¶å­˜åœ¨"
                ]
            }
        elif "glm" in error_msg or "api" in error_msg:
            return {
                "action": "fallback_to_traditional",
                "success": True,
                "message": "GLMä¸å¯ç”¨ï¼Œå·²é™çº§åˆ°ä¼ ç»Ÿåˆ†å—",
                "suggestions": [
                    "é…ç½®GLMä»¥å¯ç”¨é«˜çº§åˆ†å—åŠŸèƒ½",
                    "å½“å‰ä½¿ç”¨ä¼ ç»Ÿåˆ†å—æ–¹æ³•"
                ]
            }
        else:
            return {
                "action": "chunking_fallback",
                "success": True,
                "message": "åˆ†å—ç­–ç•¥å·²é™çº§",
                "suggestions": [
                    "æ£€æŸ¥åˆ†å—ç­–ç•¥é…ç½®",
                    "ä½¿ç”¨ä¼ ç»Ÿåˆ†å—ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ"
                ]
            }
    
    def _handle_collection_error(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†é›†åˆé”™è¯¯"""
        error_msg = str(error).lower()
        collection_name = context.get("collection_name", "æœªçŸ¥")
        
        if "not exist" in error_msg or "not found" in error_msg:
            return {
                "action": "create_collection",
                "success": False,
                "message": f"é›†åˆ {collection_name} ä¸å­˜åœ¨",
                "suggestions": [
                    "åˆ›å»ºæ–°é›†åˆ",
                    "æ£€æŸ¥é›†åˆåç§°æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤Milvusè¿æ¥æ­£å¸¸"
                ],
                "recovery_steps": [
                    "è‡ªåŠ¨åˆ›å»ºé›†åˆ",
                    "ä½¿ç”¨é»˜è®¤schema",
                    "é‡æ–°å°è¯•æ“ä½œ"
                ]
            }
        elif "load" in error_msg:
            return {
                "action": "retry_load_collection",
                "success": False,
                "message": f"é›†åˆ {collection_name} åŠ è½½å¤±è´¥",
                "suggestions": [
                    "é‡è¯•åŠ è½½é›†åˆ",
                    "æ£€æŸ¥MilvusæœåŠ¡çŠ¶æ€",
                    "ç¡®è®¤é›†åˆschemaæ­£ç¡®"
                ],
                "recovery_steps": [
                    "ç­‰å¾…2ç§’åé‡è¯•",
                    "æœ€å¤šé‡è¯•3æ¬¡",
                    "å¦‚æœä»å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥"
                ]
            }
        else:
            return {
                "action": "general_collection_error",
                "success": False,
                "message": f"é›†åˆ {collection_name} æ“ä½œå¤±è´¥",
                "suggestions": [
                    "æ£€æŸ¥Milvusè¿æ¥çŠ¶æ€",
                    "éªŒè¯é›†åˆé…ç½®",
                    "æŸ¥çœ‹Milvusæ—¥å¿—"
                ]
            }
    
    def _handle_connection_error(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†è¿æ¥é”™è¯¯"""
        return {
            "action": "retry_connection",
            "success": False,
            "message": "è¿æ¥å¤±è´¥",
            "suggestions": [
                "æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ",
                "éªŒè¯ç½‘ç»œè¿æ¥",
                "ç¡®è®¤ç«¯å£é…ç½®æ­£ç¡®"
            ],
            "recovery_steps": [
                "é‡è¯•è¿æ¥",
                "æ£€æŸ¥æœåŠ¡çŠ¶æ€",
                "éªŒè¯é…ç½®æ–‡ä»¶"
            ]
        }
    
    def get_error_history(self, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–é”™è¯¯å†å²"""
        return self._error_history[-limit:]
    
    def clear_error_history(self) -> None:
        """æ¸…é™¤é”™è¯¯å†å²"""
        self._error_history.clear()
        logger.info("é”™è¯¯å†å²å·²æ¸…é™¤")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="Cre_milvus æ–°æ¶æ„API", version="3.0.0")

# å…¨å±€çŠ¶æ€
_app_initialized = False
_collection_manager = None
_progress_tracker = None
_error_manager = None

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–è¿æ¥"""
    global _app_initialized, _collection_manager
    
    logger.info("=" * 60)
    logger.info("ğŸš€ APIæœåŠ¡å¯åŠ¨ï¼Œåˆå§‹åŒ–è¿æ¥...")
    logger.info("=" * 60)
    
    try:
        from System.connection_initializer import startup_initialize
        success = startup_initialize()
        
        if success:
            _app_initialized = True
            _collection_manager = CollectionStateManager()
            _progress_tracker = InsertProgressTracker()
            _error_manager = ErrorRecoveryManager()
            logger.info("âœ… APIæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            logger.info("âœ… é›†åˆçŠ¶æ€ç®¡ç†å™¨å·²åˆå§‹åŒ–")
            logger.info("âœ… æ’å…¥è¿›åº¦è·Ÿè¸ªå™¨å·²åˆå§‹åŒ–")
            logger.info("âœ… é”™è¯¯æ¢å¤ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        else:
            logger.error("âŒ APIæœåŠ¡åˆå§‹åŒ–å¤±è´¥")
            
    except Exception as e:
        logger.error(f"âŒ APIæœåŠ¡åˆå§‹åŒ–å¼‚å¸¸: {e}")
        _app_initialized = False

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Cre_milvus æ–°æ¶æ„API",
        "version": "3.0.0",
        "initialized": _app_initialized,
        "features": [
            "é¢„è¿æ¥æ¶æ„",
            "å¿«é€Ÿæ•°æ®æ’å…¥",
            "è¿æ¥çŠ¶æ€ç›‘æ§",
            "æ— é˜»å¡æ“ä½œ"
        ]
    }

@app.get("/status")
async def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    try:
        from System.new_start import get_connection_status
        status = get_connection_status()
        
        return {
            "api_initialized": _app_initialized,
            "connection_status": status,
            "timestamp": time.time()
        }
        
    except Exception as e:
        return {
            "api_initialized": _app_initialized,
            "error": str(e),
            "timestamp": time.time()
        }

@app.post("/update_config")
async def update_config(request: Request):
    """
    æ›´æ–°é…ç½®ï¼ŒåŒ…æ‹¬åŠ¨æ€æ›´æ–°Milvusè¿æ¥
    """
    try:
        config_data = await request.json()
        logger.info(f"æ”¶åˆ°é…ç½®æ›´æ–°è¯·æ±‚: {config_data}")
        
        # æ›´æ–°é…ç½®æ–‡ä»¶
        import yaml
        try:
            with open("config.yaml", "r", encoding="utf-8") as f:
                current_config = yaml.safe_load(f) or {}
        except:
            current_config = {}
        
        # æ·±åº¦åˆå¹¶é…ç½®
        def deep_merge(base, update):
            for key, value in update.items():
                if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                    deep_merge(base[key], value)
                else:
                    base[key] = value
        
        deep_merge(current_config, config_data)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        with open("config.yaml", "w", encoding="utf-8") as f:
            yaml.dump(current_config, f, default_flow_style=False, allow_unicode=True)
        
        # å¦‚æœæ›´æ–°äº†Milvusé…ç½®ï¼ŒåŠ¨æ€é‡è¿
        if "milvus" in config_data:
            milvus_config = config_data["milvus"]
            if "host" in milvus_config and "port" in milvus_config:
                host = milvus_config["host"]
                port = int(milvus_config["port"])
                
                logger.info(f"æ£€æµ‹åˆ°Milvusé…ç½®æ›´æ–°ï¼Œå°è¯•é‡æ–°è¿æ¥: {host}:{port}")
                
                from milvusBuilder.persistent_connection import get_persistent_connection
                conn = get_persistent_connection()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è¿æ¥
                if not conn.is_connection_valid_for(host, port):
                    success = conn.update_connection(host, port)
                    if success:
                        logger.info("âœ… Milvusè¿æ¥å·²æ›´æ–°")
                    else:
                        logger.warning("âš ï¸ Milvusè¿æ¥æ›´æ–°å¤±è´¥ï¼Œä½†é…ç½®å·²ä¿å­˜")
                else:
                    logger.info("âœ… Milvusè¿æ¥æ— éœ€æ›´æ–°")
        
        return {
            "message": "é…ç½®æ›´æ–°æˆåŠŸ",
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"é…ç½®æ›´æ–°å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"é…ç½®æ›´æ–°å¤±è´¥: {str(e)}"
        )


@app.post("/upload")
async def upload_file(file: UploadFile = File(...), folder_name: str = Form(None)):
    """æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†"""
    try:
        if folder_name and folder_name.strip():
            upload_dir = f"./data/upload/{folder_name.strip()}"
            logger.info(f"ä½¿ç”¨æŒ‡å®šæ–‡ä»¶å¤¹: {upload_dir}")
        else:
            upload_dir = "./data/upload"
            logger.info(f"ä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹: {upload_dir}")
        os.makedirs(upload_dir, exist_ok=True)
        
        uploaded_files = []
        file_types = {}
        
        if file.filename:
            file_path = os.path.join(upload_dir, file.filename)
            
            content = await file.read()
            with open(file_path, "wb") as buffer:
                buffer.write(content)
            
            uploaded_files.append(file.filename)
            file_extension = os.path.splitext(file.filename)[1].lower()
            file_types[file_extension] = file_types.get(file_extension, 0) + 1
            
            logger.info(f"æ–‡ä»¶å·²ä¿å­˜: {file.filename}")
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ å®Œæˆ: {len(uploaded_files)} ä¸ªæ–‡ä»¶")
        
        try:
            logger.info("å¼€å§‹å‘é‡åŒ–å­˜å‚¨...")
            
            with open("config.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            
            insert_mode = config.get("system", {}).get("insert_mode", "overwrite")
            collection_name = config.get("milvus", {}).get("collection_name", "Test_one")
            
            if insert_mode == "append":
                logger.info(f"ä½¿ç”¨appendæ¨¡å¼ï¼Œæ£€æŸ¥é›†åˆ {collection_name} çŠ¶æ€")
                
                if _collection_manager:
                    collection_ready = _collection_manager.ensure_collection_loaded(collection_name)
                    if not collection_ready:
                        logger.error(f"é›†åˆ {collection_name} åŠ è½½å¤±è´¥")
                        return {
                            "message": f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œä½†é›†åˆåŠ è½½å¤±è´¥",
                            "files": uploaded_files,
                            "upload_dir": upload_dir,
                            "file_types": file_types,
                            "vectorized": False,
                            "error": f"é›†åˆ {collection_name} åŠ è½½å¤±è´¥",
                            "status": "partial_success"
                        }
                    else:
                        logger.info(f"é›†åˆ {collection_name} å·²å‡†å¤‡å°±ç»ª")
                else:
                    logger.warning("é›†åˆçŠ¶æ€ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            else:
                logger.info(f"ä½¿ç”¨overwriteæ¨¡å¼ï¼Œå°†é‡æ–°åˆ›å»ºé›†åˆ {collection_name}")
            
            if folder_name:
                if "data" not in config:
                    config["data"] = {}
                config["data"]["data_location"] = upload_dir
                logger.info(f"æ›´æ–°æ•°æ®è·¯å¾„ä¸º: {upload_dir}")
                
                with open("config.yaml", "w", encoding="utf-8") as f:
                    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            
            tracking_id = None
            if _progress_tracker:
                tracking_id = _progress_tracker.start_tracking(len(uploaded_files))
                logger.info(f"å¼€å§‹è·Ÿè¸ªå‘é‡åŒ–è¿›åº¦: {tracking_id}")
            
            from System.new_start import fast_vector_database_build_from_config
            
            start_time = time.time()
            
            if tracking_id:
                _progress_tracker.update_progress(tracking_id, 0, 0, "å¼€å§‹å‘é‡åŒ–å­˜å‚¨")
            
            try:
                result = fast_vector_database_build_from_config(config)
                end_time = time.time()
                
                logger.info(f"å‘é‡åŒ–å­˜å‚¨å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
                
                if tracking_id:
                    success = result.get("status") == "success"
                    final_message = f"å‘é‡åŒ–å­˜å‚¨{'æˆåŠŸ' if success else 'å¤±è´¥'}ï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’"
                    _progress_tracker.finish_tracking(tracking_id, success, final_message)
                    
            except Exception as build_error:
                end_time = time.time()
                logger.error(f"å‘é‡åŒ–æ„å»ºè¿‡ç¨‹å¤±è´¥: {build_error}")
                
                if tracking_id:
                    _progress_tracker.add_error(tracking_id, str(build_error))
                    _progress_tracker.finish_tracking(tracking_id, False, f"å‘é‡åŒ–æ„å»ºå¤±è´¥: {str(build_error)}")
                
                raise build_error
            
            if result.get("status") == "success":
                return {
                    "success": True,
                    "message": f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶å¹¶å®Œæˆå‘é‡åŒ–å­˜å‚¨",
                    "filename": file.filename,
                    "size": len(content),
                    "path": file_path,
                    "folder_name": folder_name,
                    "files": uploaded_files,
                    "upload_dir": upload_dir,
                    "file_types": file_types,
                    "vectorized": True,
                    "vectorization_result": result,
                    "processing_time": end_time - start_time,
                    "tracking_id": tracking_id,
                    "status": "success"
                }
            else:
                return {
                    "success": True,
                    "message": f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œä½†å‘é‡åŒ–å­˜å‚¨å¤±è´¥",
                    "filename": file.filename,
                    "size": len(content),
                    "path": file_path,
                    "folder_name": folder_name,
                    "files": uploaded_files,
                    "upload_dir": upload_dir,
                    "file_types": file_types,
                    "vectorized": False,
                    "error": result.get("msg", "æœªçŸ¥é”™è¯¯"),
                    "tracking_id": tracking_id,
                    "status": "partial_success"
                }
                
        except Exception as vector_error:
            logger.error(f"å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {vector_error}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            return {
                "success": True,
                "message": f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œä½†å‘é‡åŒ–å­˜å‚¨å¤±è´¥",
                "filename": file.filename,
                "size": len(content) if 'content' in locals() else 0,
                "path": file_path if 'file_path' in locals() else "",
                "folder_name": folder_name,
                "files": uploaded_files,
                "upload_dir": upload_dir,
                "file_types": file_types,
                "vectorized": False,
                "error": str(vector_error),
                "tracking_id": tracking_id if 'tracking_id' in locals() else None,
                "status": "partial_success"
            }
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
        )

@app.post("/search")
async def search_documents(request: Request):
    """
    æœç´¢æ–‡æ¡£ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«èšç±»å¯è§†åŒ–æ•°æ®ï¼‰
    """
    if not _app_initialized:
        raise HTTPException(
            status_code=503, 
            detail="æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·ç­‰å¾…åˆå§‹åŒ–å®Œæˆ"
        )
    
    try:
        data = await request.json()
        question = data.get("question", "")
        col_choice = data.get("col_choice", "hdbscan")
        collection_name = data.get("collection_name", "Test_one")
        enable_visualization = data.get("enable_visualization", True)
        
        if not question:
            raise HTTPException(status_code=400, detail="é—®é¢˜ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"æ”¶åˆ°æœç´¢è¯·æ±‚: {question}, èšç±»æ–¹æ³•: {col_choice}")
        
        # åŠ è½½é…ç½®
        with open("config.yaml", "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        
        # ä½¿ç”¨åŸæœ‰çš„æœç´¢åŠŸèƒ½
        from System.start import Cre_Search
        
        start_time = time.time()
        result = Cre_Search(config, question)
        search_time = time.time() - start_time
        
        logger.info(f"åŸºç¡€æœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time:.2f}ç§’")
        
        # å¦‚æœå¯ç”¨å¯è§†åŒ–ä¸”æœ‰èšç±»ç»“æœï¼Œæ·»åŠ å¯è§†åŒ–æ•°æ®
        if enable_visualization and "clusters" in result and result["clusters"]:
            try:
                from Search.clustering import create_clustering_service
                clustering_service = create_clustering_service()
                
                # è½¬æ¢èšç±»æ•°æ®æ ¼å¼
                clusters = []
                for cluster_data in result["clusters"]:
                    from Search.clustering import Cluster, SearchResult
                    
                    # è½¬æ¢æ–‡æ¡£æ•°æ®
                    documents = []
                    for doc in cluster_data.get("documents", []):
                        search_result = SearchResult(
                            id=str(doc.get("id", "")),
                            content=doc.get("content", ""),
                            url=doc.get("url"),
                            distance=float(doc.get("distance", 0.0)),
                            embedding=doc.get("embedding", []),
                            metadata=doc.get("metadata", {})
                        )
                        documents.append(search_result)
                    
                    # åˆ›å»ºèšç±»å¯¹è±¡
                    cluster = Cluster(
                        cluster_id=cluster_data.get("cluster_id", 0),
                        documents=documents,
                        centroid=cluster_data.get("centroid"),
                        size=len(documents),
                        avg_distance=cluster_data.get("avg_distance", 0.0),
                        keywords=cluster_data.get("keywords", [])
                    )
                    clusters.append(cluster)
                
                # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
                viz_start_time = time.time()
                
                scatter_plot_data = clustering_service.create_cluster_scatter_plot(clusters)
                size_chart_data = clustering_service.create_cluster_size_chart(clusters)
                heatmap_data = clustering_service.create_cluster_heatmap(clusters)
                cluster_summary = clustering_service.generate_cluster_summary(clusters)
                cluster_metrics = clustering_service.calculate_cluster_metrics(clusters)
                
                viz_time = time.time() - viz_start_time
                logger.info(f"å¯è§†åŒ–æ•°æ®ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {viz_time:.2f}ç§’")
                
                # æ·»åŠ å¯è§†åŒ–æ•°æ®åˆ°ç»“æœä¸­
                result["visualization_data"] = {
                    "scatter_plot": scatter_plot_data,
                    "size_chart": size_chart_data,
                    "heatmap": heatmap_data,
                    "cluster_summary": cluster_summary,
                    "cluster_metrics": cluster_metrics
                }
                
                # æ›´æ–°æ‰§è¡Œæ—¶é—´
                result["execution_time"] = search_time + viz_time
                result["search_time"] = search_time
                result["visualization_time"] = viz_time
                
                logger.info(f"å¢å¼ºæœç´¢å®Œæˆï¼Œæ€»è€—æ—¶: {result['execution_time']:.2f}ç§’")
                
            except Exception as viz_error:
                logger.error(f"ç”Ÿæˆå¯è§†åŒ–æ•°æ®å¤±è´¥: {viz_error}")
                # å¯è§†åŒ–å¤±è´¥ä¸å½±å“åŸºç¡€æœç´¢ç»“æœ
                result["visualization_error"] = str(viz_error)
        
        # æ·»åŠ è´¨é‡æŒ‡æ ‡ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if "quality_metrics" not in result and "clusters" in result:
            try:
                result["quality_metrics"] = _calculate_search_quality_metrics(result)
            except Exception as e:
                logger.warning(f"è®¡ç®—è´¨é‡æŒ‡æ ‡å¤±è´¥: {e}")
        
        return result
        
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"æœç´¢å¤±è´¥: {str(e)}"
        )


def _calculate_search_quality_metrics(search_result: Dict[str, Any]) -> Dict[str, float]:
    """è®¡ç®—æœç´¢è´¨é‡æŒ‡æ ‡"""
    try:
        clusters = search_result.get("clusters", [])
        if not clusters:
            return {"relevance_score": 0.0, "diversity_score": 0.0, "coverage_score": 0.0}
        
        total_docs = sum(len(cluster.get("documents", [])) for cluster in clusters)
        if total_docs == 0:
            return {"relevance_score": 0.0, "diversity_score": 0.0, "coverage_score": 0.0}
        
        # ç›¸å…³æ€§åˆ†æ•°ï¼šåŸºäºå¹³å‡è·ç¦»ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸å…³æ€§è¶Šé«˜ï¼‰
        total_distance = 0
        for cluster in clusters:
            for doc in cluster.get("documents", []):
                total_distance += doc.get("distance", 1.0)
        
        avg_distance = total_distance / total_docs
        relevance_score = max(0, 1 - avg_distance)  # è·ç¦»è½¬æ¢ä¸ºç›¸å…³æ€§
        
        # å¤šæ ·æ€§åˆ†æ•°ï¼šåŸºäºèšç±»æ•°é‡å’Œåˆ†å¸ƒ
        num_clusters = len(clusters)
        if num_clusters <= 1:
            diversity_score = 0.0
        else:
            # è®¡ç®—èšç±»å¤§å°çš„æ ‡å‡†å·®ï¼Œæ ‡å‡†å·®è¶Šå°ï¼Œåˆ†å¸ƒè¶Šå‡åŒ€ï¼Œå¤šæ ·æ€§è¶Šå¥½
            cluster_sizes = [len(cluster.get("documents", [])) for cluster in clusters]
            mean_size = sum(cluster_sizes) / len(cluster_sizes)
            variance = sum((size - mean_size) ** 2 for size in cluster_sizes) / len(cluster_sizes)
            std_dev = variance ** 0.5
            
            # å½’ä¸€åŒ–å¤šæ ·æ€§åˆ†æ•°
            max_possible_std = mean_size * 0.5  # å‡è®¾æœ€å¤§æ ‡å‡†å·®ä¸ºå¹³å‡å€¼çš„ä¸€åŠ
            diversity_score = max(0, 1 - (std_dev / max_possible_std)) if max_possible_std > 0 else 0
        
        # è¦†ç›–ç‡åˆ†æ•°ï¼šåŸºäºèšç±»æ•°é‡ç›¸å¯¹äºæ–‡æ¡£æ•°é‡çš„æ¯”ä¾‹
        coverage_ratio = num_clusters / total_docs if total_docs > 0 else 0
        coverage_score = min(1.0, coverage_ratio * 5)  # å‡è®¾ç†æƒ³æ¯”ä¾‹æ˜¯1:5
        
        return {
            "relevance_score": round(relevance_score, 3),
            "diversity_score": round(diversity_score, 3),
            "coverage_score": round(coverage_score, 3)
        }
        
    except Exception as e:
        logger.error(f"è®¡ç®—è´¨é‡æŒ‡æ ‡å¤±è´¥: {e}")
        return {"relevance_score": 0.0, "diversity_score": 0.0, "coverage_score": 0.0}

@app.get("/collections")
async def list_collections():
    """åˆ—å‡ºæ‰€æœ‰é›†åˆ"""
    if not _app_initialized:
        raise HTTPException(
            status_code=503, 
            detail="æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·ç­‰å¾…åˆå§‹åŒ–å®Œæˆ"
        )
    
    try:
        from milvusBuilder.fast_insert import list_collections
        collections = list_collections()
        
        return {
            "collections": collections,
            "count": len(collections),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºé›†åˆå¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"åˆ—å‡ºé›†åˆå¤±è´¥: {str(e)}"
        )

@app.get("/collections/{collection_name}/status")
async def get_collection_status(collection_name: str):
    """è·å–é›†åˆçŠ¶æ€"""
    if not _app_initialized:
        raise HTTPException(
            status_code=503, 
            detail="æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·ç­‰å¾…åˆå§‹åŒ–å®Œæˆ"
        )
    
    try:
        from milvusBuilder.fast_insert import check_collection_status
        status = check_collection_status(collection_name)
        
        return {
            "collection_name": collection_name,
            "status": status
        }
        
    except Exception as e:
        logger.error(f"è·å–é›†åˆçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–é›†åˆçŠ¶æ€å¤±è´¥: {str(e)}"
        )

@app.post("/chunking/process")
async def process_chunking(request: Request):
    """
    æ–‡æœ¬åˆ‡åˆ†å¤„ç†
    """
    try:
        data = await request.json()
        text = data.get("text", "")
        strategy = data.get("strategy", "traditional")
        params = data.get("params", {})
        
        if not text:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"æ”¶åˆ°æ–‡æœ¬åˆ‡åˆ†è¯·æ±‚: ç­–ç•¥={strategy}, æ–‡æœ¬é•¿åº¦={len(text)}")
        
        # å¯¼å…¥æ–‡æœ¬åˆ‡åˆ†æ¨¡å—
        try:
            from dataBuilder.chunking.chunking_manager import ChunkingManager
            
            chunking_manager = ChunkingManager()
            chunks = chunking_manager.chunk_text(text, strategy, params)
            
            return {
                "chunks": chunks,
                "chunk_count": len(chunks),
                "strategy": strategy,
                "status": "success"
            }
            
        except ImportError:
            # å¦‚æœåˆ‡åˆ†æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ‡åˆ†
            chunk_length = params.get("chunk_length", 512)
            overlap = params.get("overlap", 50)
            
            chunks = []
            start = 0
            while start < len(text):
                end = min(start + chunk_length, len(text))
                chunk = text[start:end]
                chunks.append(chunk)
                start = end - overlap if end < len(text) else end
            
            return {
                "chunks": chunks,
                "chunk_count": len(chunks),
                "strategy": "simple",
                "status": "success"
            }
        
    except Exception as e:
        logger.error(f"æ–‡æœ¬åˆ‡åˆ†å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"æ–‡æœ¬åˆ‡åˆ†å¤±è´¥: {str(e)}"
        )

@app.post("/multimodal/text_to_image_search")
async def text_to_image_search(request: Request):
    """
    æ–‡æœå›¾åŠŸèƒ½
    """
    try:
        data = await request.json()
        query_text = data.get("query_text", "")
        top_k = data.get("top_k", 10)
        collection_name = data.get("collection_name", "")
        
        if not query_text:
            raise HTTPException(status_code=400, detail="æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"æ”¶åˆ°æ–‡æœå›¾è¯·æ±‚: {query_text}")
        
        # æ£€æŸ¥å¤šæ¨¡æ€åŠŸèƒ½æ˜¯å¦å¯ç”¨
        try:
            with open("config.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            
            multimodal_config = config.get("multimodal", {})
            if not multimodal_config.get("enable_image", False):
                return {
                    "results": [],
                    "message": "å¤šæ¨¡æ€åŠŸèƒ½æœªå¯ç”¨",
                    "status": "disabled"
                }
            
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ–‡æœå›¾é€»è¾‘
            # ç›®å‰è¿”å›æ¨¡æ‹Ÿç»“æœ
            return {
                "results": [],
                "message": "æ–‡æœå›¾åŠŸèƒ½å¼€å‘ä¸­",
                "query_text": query_text,
                "top_k": top_k,
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"æ–‡æœå›¾å¤„ç†å¤±è´¥: {e}")
            return {
                "results": [],
                "message": f"æ–‡æœå›¾å¤±è´¥: {str(e)}",
                "status": "error"
            }
        
    except Exception as e:
        logger.error(f"æ–‡æœå›¾è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"æ–‡æœå›¾è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"
        )

@app.get("/performance/current")
async def get_performance_metrics():
    """
    è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡
    """
    try:
        import psutil
        
        # è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        metrics = {
            "cpu": {
                "percent": cpu_percent
            },
            "memory": {
                "percent": memory.percent,
                "used": memory.used,
                "total": memory.total
            },
            "disk": {
                "percent": disk.percent,
                "used": disk.used,
                "total": disk.total
            }
        }
        
        return {
            "metrics": metrics,
            "timestamp": time.time(),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {str(e)}"
        )

@app.get("/system/status")
async def get_system_status():
    """
    è·å–ç³»ç»ŸçŠ¶æ€
    """
    try:
        # è·å–è¿æ¥çŠ¶æ€
        try:
            from System.new_start import get_connection_status
            connection_status = get_connection_status()
            milvus_connected = connection_status.get("overall_ready", False)
        except:
            connection_status = {}
            milvus_connected = False
        
        # è·å–é…ç½®ä¿¡æ¯
        try:
            with open("config.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
        except:
            config = {}
        
        # æ£€æŸ¥LLMé…ç½®çŠ¶æ€
        llm_configs = config.get("llm_configs", {})
        active_llm_config_id = config.get("active_llm_config")
        active_llm_config = None
        
        if active_llm_config_id and active_llm_config_id in llm_configs:
            active_llm_config = {
                "id": active_llm_config_id,
                "provider": llm_configs[active_llm_config_id].get("provider"),
                "model": llm_configs[active_llm_config_id].get("model_name")
            }
        
        # æ„å»ºçŠ¶æ€ä¿¡æ¯ï¼ŒåŒ¹é…å‰ç«¯æœŸæœ›çš„æ ¼å¼
        status = {
            "milvus": {
                "connected": milvus_connected
            },
            "embedding_model": {
                "available": True  # å‡è®¾åµŒå…¥æ¨¡å‹æ€»æ˜¯å¯ç”¨çš„
            },
            "chunking_system": {
                "available": True
            },
            "clustering_service": {
                "available": True
            },
            "llm_config": {
                "available": active_llm_config is not None,
                "active_config": active_llm_config
            }
        }
        
        # è®¡ç®—æ•´ä½“å¥åº·çŠ¶æ€
        critical_services = [
            status["milvus"]["connected"],
            status["embedding_model"]["available"]
        ]
        
        if all(critical_services):
            overall_status = "healthy"
        elif any(critical_services):
            overall_status = "degraded"
        else:
            overall_status = "unhealthy"
        
        health = {
            "overall_status": overall_status
        }
        
        return {
            "status": status,
            "health": health,
            "config": {
                "milvus": config.get("milvus", {}),
                "multimodal": config.get("multimodal", {}),
                "chunking": config.get("chunking", {})
            },
            "connection_status": connection_status,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}"
        )

@app.post("/visualization")
async def get_visualization_data(request: Request):
    """
    è·å–å¯è§†åŒ–æ•°æ®
    """
    try:
        data = await request.json()
        collection_name = data.get("collection_name", "")
        
        if not collection_name:
            raise HTTPException(status_code=400, detail="é›†åˆåç§°ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"æ”¶åˆ°å¯è§†åŒ–è¯·æ±‚: {collection_name}")
        
        # å°è¯•è·å–å¯è§†åŒ–æ•°æ®
        try:
            from ColBuilder.visualization import get_all_embeddings_and_texts
            import hdbscan
            from umap import UMAP
            import pandas as pd
            import numpy as np
            
            # è·å–æ•°æ®
            embeddings, texts, ids = get_all_embeddings_and_texts(collection_name)
            
            if not embeddings:
                return []
            
            # UMAPé™ç»´
            umap_model = UMAP(n_components=2, random_state=42)
            embeddings_2d = umap_model.fit_transform(np.array(embeddings))
            
            # HDBSCANèšç±»
            clusterer = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=2)
            cluster_labels = clusterer.fit_predict(embeddings_2d)
            
            # æ„å»ºç»“æœ
            result = []
            for i, (x, y) in enumerate(embeddings_2d):
                result.append({
                    "x": float(x),
                    "y": float(y),
                    "cluster": int(cluster_labels[i]),
                    "text": texts[i][:100] if i < len(texts) else "",
                    "id": ids[i] if i < len(ids) else i
                })
            
            return result
            
        except ImportError as e:
            logger.warning(f"å¯è§†åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            return []
        except Exception as e:
            logger.error(f"å¯è§†åŒ–æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return []
        
    except Exception as e:
        logger.error(f"å¯è§†åŒ–è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"å¯è§†åŒ–è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"
        )

@app.get("/llm/providers")
async def get_llm_providers():
    """è·å–LLMæä¾›å•†åˆ—è¡¨"""
    try:
        providers = [
            {
                "name": "openai",
                "description": "OpenAI GPTç³»åˆ—æ¨¡å‹",
                "models": ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]
            },
            {
                "name": "claude",
                "description": "Anthropic Claudeç³»åˆ—æ¨¡å‹",
                "models": ["claude-3-haiku", "claude-3-sonnet", "claude-3-opus"]
            },
            {
                "name": "qwen",
                "description": "é˜¿é‡Œäº‘é€šä¹‰åƒé—®ç³»åˆ—æ¨¡å‹",
                "models": ["qwen-turbo", "qwen-plus", "qwen-max"]
            },
            {
                "name": "zhipu",
                "description": "æ™ºè°±AI GLMç³»åˆ—æ¨¡å‹",
                "models": ["glm-4", "glm-4-turbo", "glm-4.1v-thinking-flash"]
            },
            {
                "name": "local",
                "description": "æœ¬åœ°éƒ¨ç½²æ¨¡å‹",
                "models": ["custom-model"]
            }
        ]
        
        return {
            "providers": providers,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"è·å–LLMæä¾›å•†å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–LLMæä¾›å•†å¤±è´¥: {str(e)}"
        )

@app.get("/llm/configs")
async def get_llm_configs():
    """è·å–LLMé…ç½®åˆ—è¡¨"""
    try:
        # è¿™é‡Œåº”è¯¥ä»é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“ä¸­è¯»å–LLMé…ç½®
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
        configs = {}
        active_config = None
        
        # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
        try:
            with open("config.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
                llm_configs = config.get("llm_configs", {})
                active_config_id = config.get("active_llm_config")
                
                for config_id, config_data in llm_configs.items():
                    configs[config_id] = {
                        "provider": config_data.get("provider"),
                        "model_name": config_data.get("model_name"),
                        "api_endpoint": config_data.get("api_endpoint")
                    }
                
                if active_config_id and active_config_id in configs:
                    active_config = {
                        "id": active_config_id,
                        **configs[active_config_id]
                    }
        except:
            pass
        
        return {
            "configs": configs,
            "summary": {
                "total_configs": len(configs),
                "active_config": active_config
            },
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"è·å–LLMé…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–LLMé…ç½®å¤±è´¥: {str(e)}"
        )



@app.post("/llm/configs")
async def save_llm_config(request: Request):
    """ä¿å­˜LLMé…ç½®"""
    try:
        data = await request.json()
        config_id = data.get("config_id")
        provider = data.get("provider")
        model_name = data.get("model_name")
        api_key = data.get("api_key")
        api_endpoint = data.get("api_endpoint")
        is_active = data.get("is_active", False)
        
        if not all([config_id, provider, api_key]):
            raise HTTPException(status_code=400, detail="é…ç½®IDã€æä¾›å•†å’ŒAPIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        
        # è¯»å–ç°æœ‰é…ç½®
        try:
            with open("config.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f) or {}
        except:
            config = {}
        
        # æ›´æ–°LLMé…ç½®
        if "llm_configs" not in config:
            config["llm_configs"] = {}
        
        config["llm_configs"][config_id] = {
            "provider": provider,
            "model_name": model_name,
            "api_key": api_key,
            "api_endpoint": api_endpoint
        }
        
        # è®¾ç½®æ¿€æ´»é…ç½®
        if is_active:
            config["active_llm_config"] = config_id
        
        # ä¿å­˜é…ç½®
        with open("config.yaml", "w", encoding="utf-8") as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        return {
            "message": f"LLMé…ç½® '{config_id}' ä¿å­˜æˆåŠŸ",
            "config_id": config_id,
            "is_active": is_active,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"ä¿å­˜LLMé…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ä¿å­˜LLMé…ç½®å¤±è´¥: {str(e)}"
        )



@app.get("/chunking/strategies")
async def get_chunking_strategies():
    """è·å–åˆ†å—ç­–ç•¥åˆ—è¡¨"""
    try:
        strategies = [
            {
                "name": "traditional",
                "display_name": "ä¼ ç»Ÿå›ºå®šé•¿åº¦åˆ‡åˆ†",
                "description": "æŒ‰å›ºå®šé•¿åº¦åˆ‡åˆ†æ–‡æœ¬ï¼Œæ”¯æŒé‡å ",
                "parameters": ["chunk_length", "overlap"],
                "requires_llm": False
            },
            {
                "name": "meta_ppl",
                "display_name": "PPLå›°æƒ‘åº¦åˆ‡åˆ†",
                "description": "åŸºäºè¯­è¨€æ¨¡å‹å›°æƒ‘åº¦çš„æ™ºèƒ½åˆ‡åˆ†",
                "parameters": ["chunk_length", "ppl_threshold"],
                "requires_llm": True
            },
            {
                "name": "margin_sampling",
                "display_name": "è¾¹é™…é‡‡æ ·åˆ‡åˆ†",
                "description": "åŸºäºè¾¹é™…é‡‡æ ·çš„æ™ºèƒ½åˆ‡åˆ†",
                "parameters": ["chunk_length", "confidence_threshold"],
                "requires_llm": True
            },
            {
                "name": "msp",
                "display_name": "MSPé«˜çº§åˆ‡åˆ†",
                "description": "å¤šå°ºåº¦æ„ŸçŸ¥åˆ‡åˆ†ç­–ç•¥",
                "parameters": ["chunk_length", "confidence_threshold"],
                "requires_llm": True
            },
            {
                "name": "semantic",
                "display_name": "è¯­ä¹‰åˆ‡åˆ†",
                "description": "åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ‡åˆ†",
                "parameters": ["chunk_length", "similarity_threshold", "min_chunk_size"],
                "requires_llm": False
            }
        ]
        
        return {
            "strategies": strategies,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"è·å–åˆ†å—ç­–ç•¥å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–åˆ†å—ç­–ç•¥å¤±è´¥: {str(e)}"
        )

@app.get("/performance/export_report")
async def export_performance_report():
    """å¯¼å‡ºæ€§èƒ½æŠ¥å‘Š"""
    try:
        import psutil
        from datetime import datetime
        
        # è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        report = {
            "report_time": datetime.now().isoformat(),
            "report_type": "performance_report",
            "system_metrics": {
                "cpu": {
                    "percent": cpu_percent,
                    "count": psutil.cpu_count()
                },
                "memory": {
                    "percent": memory.percent,
                    "used_gb": round(memory.used / (1024**3), 2),
                    "total_gb": round(memory.total / (1024**3), 2)
                },
                "disk": {
                    "percent": disk.percent,
                    "used_gb": round(disk.used / (1024**3), 2),
                    "total_gb": round(disk.total / (1024**3), 2)
                }
            },
            "connection_status": {},
            "recommendations": []
        }
        
        # æ·»åŠ æ€§èƒ½å»ºè®®
        if cpu_percent > 80:
            report["recommendations"].append("CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–æŸ¥è¯¢æˆ–å¢åŠ è®¡ç®—èµ„æº")
        if memory.percent > 80:
            report["recommendations"].append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        if disk.percent > 80:
            report["recommendations"].append("ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®æ¸…ç†æ•°æ®æˆ–æ‰©å®¹")
        
        return report
        
    except Exception as e:
        logger.error(f"å¯¼å‡ºæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"å¯¼å‡ºæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {str(e)}"
        )

@app.post("/testing/start_load_test")
async def start_load_test(request: Request):
    """å¯åŠ¨å‹åŠ›æµ‹è¯•"""
    try:
        data = await request.json()
        
        # æ¨¡æ‹Ÿå‹åŠ›æµ‹è¯•å¯åŠ¨
        test_id = f"test_{int(time.time())}"
        
        return {
            "message": "å‹åŠ›æµ‹è¯•å·²å¯åŠ¨",
            "test_id": test_id,
            "status": "started",
            "parameters": data
        }
        
    except Exception as e:
        logger.error(f"å¯åŠ¨å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"å¯åŠ¨å‹åŠ›æµ‹è¯•å¤±è´¥: {str(e)}"
        )

@app.get("/testing/list_tests")
async def list_tests():
    """åˆ—å‡ºæµ‹è¯•"""
    try:
        # æ¨¡æ‹Ÿæµ‹è¯•åˆ—è¡¨
        tests = []
        
        return {
            "tests": tests,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºæµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"åˆ—å‡ºæµ‹è¯•å¤±è´¥: {str(e)}"
        )

@app.post("/testing/stop_test/{test_id}")
async def stop_test(test_id: str):
    """åœæ­¢æµ‹è¯•"""
    try:
        return {
            "message": f"æµ‹è¯• {test_id} å·²åœæ­¢",
            "test_id": test_id,
            "status": "stopped"
        }
        
    except Exception as e:
        logger.error(f"åœæ­¢æµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"åœæ­¢æµ‹è¯•å¤±è´¥: {str(e)}"
        )

@app.post("/system/integration_test")
async def system_integration_test():
    """ç³»ç»Ÿé›†æˆæµ‹è¯•"""
    try:
        test_results = []
        
        # æµ‹è¯•Milvusè¿æ¥
        try:
            from System.new_start import get_connection_status
            connection_status = get_connection_status()
            milvus_test = {
                "test_name": "Milvusè¿æ¥æµ‹è¯•",
                "status": "passed" if connection_status.get("overall_ready", False) else "failed",
                "details": connection_status
            }
            test_results.append(milvus_test)
        except Exception as e:
            test_results.append({
                "test_name": "Milvusè¿æ¥æµ‹è¯•",
                "status": "failed",
                "error": str(e)
            })
        
        # æµ‹è¯•é…ç½®æ–‡ä»¶
        try:
            with open("config.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            test_results.append({
                "test_name": "é…ç½®æ–‡ä»¶æµ‹è¯•",
                "status": "passed",
                "details": "é…ç½®æ–‡ä»¶è¯»å–æ­£å¸¸"
            })
        except Exception as e:
            test_results.append({
                "test_name": "é…ç½®æ–‡ä»¶æµ‹è¯•",
                "status": "failed",
                "error": str(e)
            })
        
        # è®¡ç®—æˆåŠŸç‡
        passed_tests = sum(1 for test in test_results if test["status"] == "passed")
        success_rate = passed_tests / len(test_results) if test_results else 0
        
        return {
            "summary": {
                "total_tests": len(test_results),
                "passed_tests": passed_tests,
                "failed_tests": len(test_results) - passed_tests,
                "success_rate": success_rate
            },
            "test_results": test_results,
            "status": "completed"
        }
        
    except Exception as e:
        logger.error(f"ç³»ç»Ÿé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ç³»ç»Ÿé›†æˆæµ‹è¯•å¤±è´¥: {str(e)}"
        )

@app.post("/system/reload_config")
async def reload_config():
    """é‡æ–°åŠ è½½é…ç½®"""
    try:
        # é‡æ–°è¯»å–é…ç½®æ–‡ä»¶
        with open("config.yaml", "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ é‡æ–°åˆå§‹åŒ–å„ä¸ªæ¨¡å—çš„é€»è¾‘
        
        return {
            "message": "ç³»ç»Ÿé…ç½®å·²é‡æ–°åŠ è½½",
            "status": "success",
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {str(e)}"
        )

@app.post("/reinitialize")
async def reinitialize_connections():
    """é‡æ–°åˆå§‹åŒ–è¿æ¥"""
    global _app_initialized
    
    try:
        logger.info("é‡æ–°åˆå§‹åŒ–è¿æ¥...")
        
        from System.connection_initializer import startup_initialize
        success = startup_initialize()
        
        if success:
            _app_initialized = True
            return {
                "message": "è¿æ¥é‡æ–°åˆå§‹åŒ–æˆåŠŸ",
                "status": "success"
            }
        else:
            _app_initialized = False
            return {
                "message": "è¿æ¥é‡æ–°åˆå§‹åŒ–å¤±è´¥",
                "status": "error"
            }
            
    except Exception as e:
        logger.error(f"é‡æ–°åˆå§‹åŒ–å¤±è´¥: {e}")
        _app_initialized = False
        raise HTTPException(
            status_code=500,
            detail=f"é‡æ–°åˆå§‹åŒ–å¤±è´¥: {str(e)}"
        )


# GLMé…ç½®ç®¡ç†ç«¯ç‚¹
@app.post("/glm/config")
async def save_glm_config(request: Request):
    """ä¿å­˜GLMé…ç½®"""
    try:
        data = await request.json()
        model_name = data.get("model_name", "glm-4.5-flash")
        api_key = data.get("api_key", "")
        
        if not api_key:
            raise HTTPException(status_code=400, detail="APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"æ”¶åˆ°GLMé…ç½®ä¿å­˜è¯·æ±‚: model={model_name}")
        
        from dataBuilder.chunking.glm_config import get_glm_config_service
        service = get_glm_config_service()
        
        success = service.save_config(model_name, api_key)
        
        if success:
            # è·å–é…ç½®æ‘˜è¦
            summary = service.get_config_summary()
            
            # åŒæ—¶æ›´æ–°åˆ°ç³»ç»Ÿé…ç½®ä¸­ï¼Œé›†æˆåˆ°ç°æœ‰çš„LLMé…ç½®ç³»ç»Ÿ
            try:
                with open("config.yaml", "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f) or {}
                
                # æ›´æ–°LLMé…ç½®
                if "llm_configs" not in config:
                    config["llm_configs"] = {}
                
                config["llm_configs"]["glm_default"] = {
                    "provider": "zhipu",
                    "model_name": model_name,
                    "api_key": api_key,
                    "api_endpoint": "https://open.bigmodel.cn/api/paas/v4/chat/completions"
                }
                
                # è®¾ç½®ä¸ºæ¿€æ´»é…ç½®
                config["active_llm_config"] = "glm_default"
                
                # ä¿å­˜é…ç½®
                with open("config.yaml", "w", encoding="utf-8") as f:
                    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
                
                logger.info("GLMé…ç½®å·²é›†æˆåˆ°ç³»ç»Ÿé…ç½®")
                
            except Exception as e:
                logger.warning(f"é›†æˆGLMé…ç½®åˆ°ç³»ç»Ÿé…ç½®å¤±è´¥: {e}")
            
            return {
                "status": "success",
                "message": "GLMé…ç½®ä¿å­˜æˆåŠŸ",
                "config": summary
            }
        else:
            raise HTTPException(status_code=400, detail="GLMé…ç½®ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¿å­˜GLMé…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜GLMé…ç½®å¤±è´¥: {str(e)}")


@app.get("/glm/config")
async def get_glm_config():
    """è·å–GLMé…ç½®"""
    try:
        from dataBuilder.chunking.glm_config import get_glm_config_service
        service = get_glm_config_service()
        
        return {
            "status": "success",
            "config": service.get_config_summary()
        }
        
    except Exception as e:
        logger.error(f"è·å–GLMé…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–GLMé…ç½®å¤±è´¥: {str(e)}")


@app.post("/glm/test-connection")
async def test_glm_connection():
    """æµ‹è¯•GLMè¿æ¥"""
    try:
        from dataBuilder.chunking.glm_config import get_glm_config_service
        service = get_glm_config_service()
        
        is_valid, message = service.test_connection()
        
        if is_valid:
            # æ›´æ–°éªŒè¯æ—¶é—´
            service.update_validation_time()
            
        return {
            "status": "success" if is_valid else "error",
            "connected": is_valid,
            "valid": is_valid,
            "message": message,
            "config": service.get_config_summary()
        }
        
    except Exception as e:
        logger.error(f"æµ‹è¯•GLMè¿æ¥å¤±è´¥: {e}")
        return {
            "status": "error",
            "connected": False,
            "valid": False,
            "message": f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"
        }


@app.post("/glm/validate-key")
async def validate_glm_api_key(request: Request):
    """éªŒè¯GLM APIå¯†é’¥"""
    try:
        data = await request.json()
        api_key = data.get("api_key", "")
        
        if not api_key:
            raise HTTPException(status_code=400, detail="APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        
        from dataBuilder.chunking.glm_config import get_glm_config_service
        service = get_glm_config_service()
        
        is_valid, message = service.validate_api_key(api_key)
        
        return {
            "status": "success",
            "valid": is_valid,
            "message": message
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"éªŒè¯GLM APIå¯†é’¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"éªŒè¯GLM APIå¯†é’¥å¤±è´¥: {str(e)}")


@app.delete("/glm/config")
async def clear_glm_config():
    """æ¸…é™¤GLMé…ç½®"""
    try:
        from dataBuilder.chunking.glm_config import get_glm_config_service
        service = get_glm_config_service()
        
        success = service.clear_config()
        
        if success:
            return {
                "status": "success",
                "message": "GLMé…ç½®å·²æ¸…é™¤"
            }
        else:
            raise HTTPException(status_code=500, detail="æ¸…é™¤GLMé…ç½®å¤±è´¥")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¸…é™¤GLMé…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤GLMé…ç½®å¤±è´¥: {str(e)}")


# å‹æµ‹ç®¡ç†ç«¯ç‚¹
@app.post("/load-test/start")
async def start_load_test(request: Request):
    """å¯åŠ¨å‹åŠ›æµ‹è¯•"""
    try:
        data = await request.json()
        
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = manager.create_test_config(data)
        
        # å¯åŠ¨æµ‹è¯•
        test_id = manager.start_load_test(config)
        
        # è·å–Webç•Œé¢URL
        web_url = manager.get_locust_web_url(test_id)
        
        return {
            "status": "success",
            "test_id": test_id,
            "web_url": web_url,
            "message": "å‹åŠ›æµ‹è¯•å·²å¯åŠ¨"
        }
        
    except Exception as e:
        logger.error(f"å¯åŠ¨å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨å‹åŠ›æµ‹è¯•å¤±è´¥: {str(e)}")


@app.get("/load-test/status/{test_id}")
async def get_load_test_status(test_id: str):
    """è·å–å‹åŠ›æµ‹è¯•çŠ¶æ€"""
    try:
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        status = manager.get_test_status(test_id)
        
        if status:
            return {
                "status": "success",
                "test_status": status
            }
        else:
            raise HTTPException(status_code=404, detail="æµ‹è¯•ä¸å­˜åœ¨")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æµ‹è¯•çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æµ‹è¯•çŠ¶æ€å¤±è´¥: {str(e)}")


@app.post("/load-test/stop/{test_id}")
async def stop_load_test(test_id: str):
    """åœæ­¢å‹åŠ›æµ‹è¯•"""
    try:
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        success = manager.stop_test(test_id)
        
        if success:
            return {
                "status": "success",
                "message": "å‹åŠ›æµ‹è¯•å·²åœæ­¢"
            }
        else:
            raise HTTPException(status_code=404, detail="æµ‹è¯•ä¸å­˜åœ¨æˆ–å·²åœæ­¢")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åœæ­¢æµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢æµ‹è¯•å¤±è´¥: {str(e)}")


@app.get("/load-test/web-url/{test_id}")
async def get_load_test_web_url(test_id: str):
    """è·å–Locust Webç•Œé¢URL"""
    try:
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        web_url = manager.get_locust_web_url(test_id)
        
        if web_url:
            return {
                "status": "success",
                "web_url": web_url
            }
        else:
            raise HTTPException(status_code=404, detail="æµ‹è¯•ä¸å­˜åœ¨æˆ–Webç•Œé¢ä¸å¯ç”¨")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–Webç•Œé¢URLå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–Webç•Œé¢URLå¤±è´¥: {str(e)}")


@app.get("/load-test/list")
async def list_load_tests():
    """åˆ—å‡ºæ‰€æœ‰å‹åŠ›æµ‹è¯•"""
    try:
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        tests = manager.list_active_tests()
        
        return {
            "status": "success",
            "tests": tests,
            "count": len(tests)
        }
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºæµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºæµ‹è¯•å¤±è´¥: {str(e)}")

@app.get("/progress/{tracking_id}")
async def get_insert_progress(tracking_id: str):
    """è·å–æ’å…¥è¿›åº¦çŠ¶æ€"""
    if not _app_initialized or not _progress_tracker:
        raise HTTPException(
            status_code=503, 
            detail="æœåŠ¡æœªåˆå§‹åŒ–"
        )
    
    try:
        progress_status = _progress_tracker.get_progress_status(tracking_id)
        return progress_status
        
    except Exception as e:
        logger.error(f"è·å–æ’å…¥è¿›åº¦å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ’å…¥è¿›åº¦å¤±è´¥: {str(e)}"
        )

@app.post("/progress/cleanup")
async def cleanup_old_progress():
    """æ¸…ç†æ—§çš„è¿›åº¦è·Ÿè¸ªæ•°æ®"""
    if not _app_initialized or not _progress_tracker:
        raise HTTPException(
            status_code=503, 
            detail="æœåŠ¡æœªåˆå§‹åŒ–"
        )
    
    try:
        _progress_tracker.cleanup_old_tracking()
        return {"message": "æ—§çš„è¿›åº¦è·Ÿè¸ªæ•°æ®å·²æ¸…ç†", "status": "success"}
        
    except Exception as e:
        logger.error(f"æ¸…ç†è¿›åº¦è·Ÿè¸ªæ•°æ®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"æ¸…ç†è¿›åº¦è·Ÿè¸ªæ•°æ®å¤±è´¥: {str(e)}"
        )

@app.get("/errors/history")
async def get_error_history(limit: int = 50):
    """è·å–é”™è¯¯å†å²"""
    if not _app_initialized or not _error_manager:
        raise HTTPException(
            status_code=503, 
            detail="æœåŠ¡æœªåˆå§‹åŒ–"
        )
    
    try:
        error_history = _error_manager.get_error_history(limit)
        return {
            "error_history": error_history,
            "count": len(error_history),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"è·å–é”™è¯¯å†å²å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–é”™è¯¯å†å²å¤±è´¥: {str(e)}"
        )

@app.post("/errors/clear")
async def clear_error_history():
    """æ¸…é™¤é”™è¯¯å†å²"""
    if not _app_initialized or not _error_manager:
        raise HTTPException(
            status_code=503, 
            detail="æœåŠ¡æœªåˆå§‹åŒ–"
        )
    
    try:
        _error_manager.clear_error_history()
        return {"message": "é”™è¯¯å†å²å·²æ¸…é™¤", "status": "success"}
        
    except Exception as e:
        logger.error(f"æ¸…é™¤é”™è¯¯å†å²å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"æ¸…é™¤é”™è¯¯å†å²å¤±è´¥: {str(e)}"
        )


# ==================== æ–‡æœ¬åˆ†å—ç›¸å…³æ¥å£ ====================

# å¯¼å…¥åˆ†å—ç›¸å…³æ¨¡å—
try:
    from dataBuilder.chunking import (
        ChunkingProcessRequest, ChunkingProcessResponse, ChunkingErrorResponse,
        ChunkingManager, create_success_response, create_error_response,
        calculate_chunking_metrics, validate_text_input, ProcessingTimer,
        format_error_message, get_strategy_display_name, get_available_strategies,
        ChunkingErrorHandler, ResponseFormatter, ErrorType, global_error_handler
    )
    from dataBuilder.chunking.chunk_strategies import ChunkingStrategyResolver
    from dataBuilder.chunking.meta_chunking import DependencyChecker
    CHUNKING_AVAILABLE = True
except ImportError as e:
    logger.warning(f"åˆ†å—æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    CHUNKING_AVAILABLE = False

# å…¨å±€åˆ†å—æœåŠ¡å˜é‡
chunking_manager = None
dependency_checker = None

def initialize_chunking_services():
    """åˆå§‹åŒ–åˆ†å—æœåŠ¡"""
    global chunking_manager, dependency_checker
    
    if not CHUNKING_AVAILABLE:
        logger.warning("åˆ†å—æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
        return False
    
    try:
        # åˆå§‹åŒ–ä¾èµ–æ£€æŸ¥å™¨
        dependency_checker = DependencyChecker()
        
        # åˆå§‹åŒ–åˆ†å—ç®¡ç†å™¨
        chunking_manager = ChunkingManager()
        
        logger.info("åˆ†å—æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        return True
        
    except Exception as e:
        logger.error(f"åˆ†å—æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

@app.on_event("startup")
async def startup_chunking_services():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–åˆ†å—æœåŠ¡"""
    if CHUNKING_AVAILABLE:
        initialize_chunking_services()

@app.get("/chunking/strategies")
async def get_chunking_strategies():
    """è·å–å¯ç”¨çš„åˆ†å—ç­–ç•¥"""
    if not CHUNKING_AVAILABLE:
        raise HTTPException(status_code=503, detail="åˆ†å—æœåŠ¡ä¸å¯ç”¨")
    
    try:
        strategies = get_available_strategies()
        
        # æ·»åŠ ç­–ç•¥å¯ç”¨æ€§æ£€æŸ¥
        if dependency_checker:
            for strategy in strategies:
                strategy["available"] = True  # ç®€åŒ–å¤„ç†
        
        return {
            "success": True,
            "strategies": strategies,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–ç­–ç•¥åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç­–ç•¥åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.post("/chunking/process")
async def process_text_chunking(request: ChunkingProcessRequest):
    """
    å¤„ç†æ–‡æœ¬åˆ†å—è¯·æ±‚
    
    æ”¯æŒæ‰€æœ‰äº”ç§åˆ†å—ç­–ç•¥ï¼štraditional, meta_ppl, margin_sampling, msp, semantic
    """
    if not CHUNKING_AVAILABLE or not chunking_manager:
        error_response = global_error_handler.handle_strategy_unavailable_error(
            strategy="service",
            reason="åˆ†å—æœåŠ¡æœªåˆå§‹åŒ–æˆ–ä¸å¯ç”¨"
        )
        raise HTTPException(
            status_code=503, 
            detail=ResponseFormatter.format_error_response(error_response)
        )
    
    # éªŒè¯è¾“å…¥æ–‡æœ¬
    is_valid, error_msg = validate_text_input(request.text)
    if not is_valid:
        error_response = global_error_handler.handle_text_validation_error(
            text=request.text,
            validation_error=error_msg
        )
        raise HTTPException(
            status_code=400, 
            detail=ResponseFormatter.format_error_response(error_response)
        )
    
    # å¼€å§‹å¤„ç†
    with ProcessingTimer() as timer:
        try:
            logger.info(f"å¼€å§‹å¤„ç†åˆ†å—è¯·æ±‚: ç­–ç•¥={request.strategy.value}, æ–‡æœ¬é•¿åº¦={len(request.text)}")
            
            # æ£€æŸ¥ç­–ç•¥å¯ç”¨æ€§
            if dependency_checker:
                resolver = ChunkingStrategyResolver(dependency_checker)
                actual_strategy = resolver.resolve_strategy(
                    request.strategy.value, 
                    {"glm_configured": True}  # ç®€åŒ–é…ç½®æ£€æŸ¥
                )
                
                if actual_strategy != request.strategy.value:
                    logger.warning(f"ç­–ç•¥é™çº§: {request.strategy.value} -> {actual_strategy}")
            else:
                actual_strategy = request.strategy.value
            
            # æ‰§è¡Œåˆ†å—
            chunks = chunking_manager.chunk_text(
                text=request.text,
                strategy=actual_strategy,
                **request.params
            )
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            processing_time = timer.get_elapsed_time()
            metrics = None
            warnings = []
            
            # æ·»åŠ ç­–ç•¥é™çº§è­¦å‘Š
            if actual_strategy != request.strategy.value:
                warnings.append(f"ç­–ç•¥å·²ä» {request.strategy.value} é™çº§åˆ° {actual_strategy}")
            
            if request.enable_metrics:
                metrics = calculate_chunking_metrics(
                    chunks=chunks,
                    processing_time=processing_time,
                    strategy_used=actual_strategy,
                    fallback_occurred=(actual_strategy != request.strategy.value)
                )
            
            # åˆ›å»ºæˆåŠŸå“åº”
            response = create_success_response(
                request=request,
                chunks=chunks,
                actual_strategy=actual_strategy,
                processing_time=processing_time,
                warnings=warnings if warnings else None,
                metrics=metrics
            )
            
            logger.info(f"åˆ†å—å¤„ç†å®Œæˆ: {len(chunks)} ä¸ªåˆ†å—, è€—æ—¶ {processing_time:.3f}s")
            
            # æ ¼å¼åŒ–å“åº”
            formatted_response = ResponseFormatter.format_success_response(response.dict())
            return formatted_response
            
        except TimeoutError as e:
            processing_time = timer.get_elapsed_time()
            error_response = global_error_handler.handle_timeout_error(
                strategy=request.strategy.value,
                timeout=request.timeout,
                actual_time=processing_time
            )
            raise HTTPException(
                status_code=408, 
                detail=ResponseFormatter.format_error_response(error_response)
            )
            
        except ValueError as e:
            # å‚æ•°é”™è¯¯
            error_response = global_error_handler.handle_parameter_error(
                strategy=request.strategy.value,
                invalid_params=request.params,
                validation_errors=[str(e)]
            )
            raise HTTPException(
                status_code=400, 
                detail=ResponseFormatter.format_error_response(error_response)
            )
            
        except Exception as e:
            # å†…éƒ¨é”™è¯¯
            processing_time = timer.get_elapsed_time()
            error_response = global_error_handler.handle_internal_error(
                error=e,
                context="åˆ†å—å¤„ç†",
                strategy=request.strategy.value
            )
            raise HTTPException(
                status_code=500, 
                detail=ResponseFormatter.format_error_response(error_response)
            )

@app.get("/chunking/status")
async def get_chunking_service_status():
    """è·å–åˆ†å—æœåŠ¡çŠ¶æ€"""
    if not CHUNKING_AVAILABLE:
        return {
            "service_available": False,
            "error": "åˆ†å—æ¨¡å—ä¸å¯ç”¨",
            "timestamp": datetime.now().isoformat()
        }
    
    status = {
        "service_available": True,
        "chunking_manager_available": chunking_manager is not None,
        "dependency_checker_available": dependency_checker is not None,
        "timestamp": datetime.now().isoformat()
    }
    
    # è·å–ä¾èµ–çŠ¶æ€
    if dependency_checker:
        try:
            status["dependencies"] = dependency_checker.check_ppl_dependencies()
            status["ppl_chunking_available"] = dependency_checker.is_ppl_chunking_available()
            status["dependency_status_message"] = dependency_checker.get_dependency_status_message()
        except Exception as e:
            status["dependency_error"] = str(e)
    
    # è·å–LLMçŠ¶æ€
    if chunking_manager:
        try:
            status["llm_status"] = chunking_manager.get_llm_status()
        except Exception as e:
            status["llm_status_error"] = str(e)
    
    return status

@app.get("/chunking/config/{strategy}")
async def get_chunking_strategy_config(strategy: str):
    """è·å–ç‰¹å®šç­–ç•¥çš„é…ç½®å‚æ•°"""
    if not CHUNKING_AVAILABLE or not chunking_manager:
        raise HTTPException(status_code=503, detail="åˆ†å—æœåŠ¡ä¸å¯ç”¨")
    
    try:
        config = chunking_manager.get_strategy_config(strategy)
        
        response_data = {
            "strategy": strategy,
            "display_name": get_strategy_display_name(strategy),
            "config": config
        }
        
        return ResponseFormatter.format_success_response(response_data)
        
    except Exception as e:
        logger.error(f"è·å–ç­–ç•¥é…ç½®å¤±è´¥: {e}")
        error_response = global_error_handler.handle_parameter_error(
            strategy=strategy,
            invalid_params={"strategy": strategy},
            validation_errors=[f"è·å–ç­–ç•¥é…ç½®å¤±è´¥: {str(e)}"]
        )
        raise HTTPException(
            status_code=400, 
            detail=ResponseFormatter.format_error_response(error_response)
        )

@app.get("/chunking/errors/statistics")
async def get_error_statistics():
    """è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯"""
    if not CHUNKING_AVAILABLE:
        raise HTTPException(status_code=503, detail="åˆ†å—æœåŠ¡ä¸å¯ç”¨")
    
    try:
        stats = global_error_handler.get_error_statistics()
        return ResponseFormatter.format_success_response({"statistics": stats})
        
    except Exception as e:
        logger.error(f"è·å–é”™è¯¯ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–é”™è¯¯ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.post("/chunking/validate")
async def validate_chunking_request(request: ChunkingProcessRequest):
    """éªŒè¯åˆ†å—è¯·æ±‚å‚æ•°"""
    if not CHUNKING_AVAILABLE:
        raise HTTPException(status_code=503, detail="åˆ†å—æœåŠ¡ä¸å¯ç”¨")
    
    validation_errors = []
    
    # éªŒè¯æ–‡æœ¬
    is_valid, error_msg = validate_text_input(request.text)
    if not is_valid:
        validation_errors.append(f"æ–‡æœ¬éªŒè¯å¤±è´¥: {error_msg}")
    
    # éªŒè¯ç­–ç•¥å‚æ•°
    try:
        if chunking_manager:
            config = chunking_manager.get_strategy_config(request.strategy.value)
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„å‚æ•°éªŒè¯é€»è¾‘
    except Exception as e:
        validation_errors.append(f"ç­–ç•¥å‚æ•°éªŒè¯å¤±è´¥: {str(e)}")
    
    # æ£€æŸ¥ä¾èµ–
    if dependency_checker and request.strategy.value in ["meta_ppl", "msp", "margin_sampling"]:
        if not dependency_checker.is_ppl_chunking_available():
            missing_deps = dependency_checker.get_missing_dependencies()
            validation_errors.append(f"ç­–ç•¥ä¾èµ–ç¼ºå¤±: {', '.join(missing_deps)}")
    
    validation_response = ResponseFormatter.format_validation_response(
        is_valid=len(validation_errors) == 0,
        errors=validation_errors
    )
    
    return validation_response

# ==================== æ–‡æœ¬åˆ†å—æ¥å£ç»“æŸ ====================

if __name__ == "__main__":
    import uvicorn
    logger.info("å¯åŠ¨æ–°æ¶æ„APIæœåŠ¡...")
    uvicorn.run(app, host="0.0.0.0", port=8505)  # ä½¿ç”¨åŸæ¥çš„ç«¯å£