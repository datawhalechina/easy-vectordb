#!/usr/bin/env python3
"""
èšç±»å’Œå¯è§†åŒ–é›†æˆæµ‹è¯•è„šæœ¬

æµ‹è¯•èšç±»ç®—æ³•å’Œå¯è§†åŒ–åŠŸèƒ½çš„é›†æˆæƒ…å†µ
"""

import sys
import os
import logging
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_clustering_imports():
    """æµ‹è¯•èšç±»ç›¸å…³æ¨¡å—å¯¼å…¥"""
    logger.info("æµ‹è¯•èšç±»æ¨¡å—å¯¼å…¥...")
    
    try:
        import hdbscan
        logger.info("âœ… HDBSCANå¯¼å…¥æˆåŠŸ")
    except ImportError:
        logger.error("âŒ HDBSCANå¯¼å…¥å¤±è´¥")
        return False
    
    try:
        from sklearn.cluster import KMeans
        logger.info("âœ… KMeanså¯¼å…¥æˆåŠŸ")
    except ImportError:
        logger.error("âŒ KMeanså¯¼å…¥å¤±è´¥")
        return False
    
    try:
        from reorder.reo_clu import reorder_clusters
        logger.info("âœ… èšç±»é‡æ’åºæ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        logger.error(f"âŒ èšç±»é‡æ’åºæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True

def test_visualization_imports():
    """æµ‹è¯•å¯è§†åŒ–ç›¸å…³æ¨¡å—å¯¼å…¥"""
    logger.info("æµ‹è¯•å¯è§†åŒ–æ¨¡å—å¯¼å…¥...")
    
    try:
        from ColBuilder.visualization import get_cluster_visualization_data, get_all_embeddings_and_texts
        logger.info("âœ… å¯è§†åŒ–æ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        logger.error(f"âŒ å¯è§†åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from umap import UMAP
        logger.info("âœ… UMAPå¯¼å…¥æˆåŠŸ")
    except ImportError:
        logger.error("âŒ UMAPå¯¼å…¥å¤±è´¥")
        return False
    
    try:
        import pandas as pd
        logger.info("âœ… Pandaså¯¼å…¥æˆåŠŸ")
    except ImportError:
        logger.error("âŒ Pandaså¯¼å…¥å¤±è´¥")
        return False
    
    return True

def test_clustering_algorithms():
    """æµ‹è¯•èšç±»ç®—æ³•"""
    logger.info("æµ‹è¯•èšç±»ç®—æ³•...")
    
    try:
        import hdbscan
        from sklearn.cluster import KMeans
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        np.random.seed(42)
        n_samples = 100
        n_features = 128
        test_embeddings = np.random.normal(0, 1, (n_samples, n_features)).astype(np.float32)
        
        # æµ‹è¯•HDBSCAN
        try:
            clusterer = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=2)
            hdbscan_labels = clusterer.fit_predict(test_embeddings)
            unique_labels = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)
            logger.info(f"âœ… HDBSCANèšç±»æˆåŠŸ: å‘ç° {unique_labels} ä¸ªèšç±»")
        except Exception as e:
            logger.error(f"âŒ HDBSCANèšç±»å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•KMeans
        try:
            k = min(len(test_embeddings), 5)
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans_labels = kmeans.fit_predict(test_embeddings)
            unique_labels = len(set(kmeans_labels))
            logger.info(f"âœ… KMeansèšç±»æˆåŠŸ: å‘ç° {unique_labels} ä¸ªèšç±»")
        except Exception as e:
            logger.error(f"âŒ KMeansèšç±»å¤±è´¥: {e}")
            return False
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ èšç±»ç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_cluster_reordering():
    """æµ‹è¯•èšç±»é‡æ’åº"""
    logger.info("æµ‹è¯•èšç±»é‡æ’åº...")
    
    try:
        from reorder.reo_clu import reorder_clusters
        
        # åˆ›å»ºæ¨¡æ‹Ÿèšç±»ç»“æœ
        clustered_results = {
            0: [
                {"id": 1, "embedding": [0.1, 0.2, 0.3], "content": "æ–‡æ¡£1", "distance": 0.5},
                {"id": 2, "embedding": [0.2, 0.3, 0.4], "content": "æ–‡æ¡£2", "distance": 0.6}
            ],
            1: [
                {"id": 3, "embedding": [0.8, 0.9, 1.0], "content": "æ–‡æ¡£3", "distance": 0.3},
                {"id": 4, "embedding": [0.9, 1.0, 1.1], "content": "æ–‡æ¡£4", "distance": 0.4}
            ]
        }
        
        query_vector = [0.5, 0.6, 0.7]
        
        # æµ‹è¯•ä¸åŒçš„é‡æ’åºç­–ç•¥
        strategies = ["distance", "cluster_size", "cluster_center"]
        
        for strategy in strategies:
            try:
                sorted_clusters = reorder_clusters(clustered_results, query_vector, strategy=strategy)
                logger.info(f"âœ… {strategy}é‡æ’åºæˆåŠŸ: {len(sorted_clusters)} ä¸ªèšç±»")
            except Exception as e:
                logger.error(f"âŒ {strategy}é‡æ’åºå¤±è´¥: {e}")
                return False
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ èšç±»é‡æ’åºæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_visualization_data_generation():
    """æµ‹è¯•å¯è§†åŒ–æ•°æ®ç”Ÿæˆ"""
    logger.info("æµ‹è¯•å¯è§†åŒ–æ•°æ®ç”Ÿæˆ...")
    
    try:
        from ColBuilder.visualization import get_cluster_visualization_data
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        np.random.seed(42)
        n_points = 50
        n_features = 128
        
        embeddings = np.random.normal(0, 1, (n_points, n_features)).astype(np.float32)
        labels = np.random.randint(0, 5, n_points)
        texts = [f"æ–‡æ¡£{i}" for i in range(n_points)]
        
        # æµ‹è¯•å¯è§†åŒ–æ•°æ®ç”Ÿæˆ
        df = get_cluster_visualization_data(embeddings, labels, texts)
        
        if not df.empty:
            logger.info(f"âœ… å¯è§†åŒ–æ•°æ®ç”ŸæˆæˆåŠŸ: {len(df)} ä¸ªæ•°æ®ç‚¹")
            logger.info(f"æ•°æ®åˆ—: {list(df.columns)}")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_columns = ["x", "y", "cluster", "text"]
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                logger.error(f"âŒ ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                return False
            else:
                logger.info("âœ… æ‰€æœ‰å¿…è¦çš„åˆ—éƒ½å­˜åœ¨")
        else:
            logger.error("âŒ å¯è§†åŒ–æ•°æ®ç”Ÿæˆå¤±è´¥: è¿”å›ç©ºDataFrame")
            return False
        
        # æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        # ç©ºæ•°æ®
        empty_df = get_cluster_visualization_data([], [], [])
        if empty_df.empty:
            logger.info("âœ… ç©ºæ•°æ®å¤„ç†æ­£ç¡®")
        else:
            logger.warning("âš ï¸ ç©ºæ•°æ®å¤„ç†å¯èƒ½æœ‰é—®é¢˜")
        
        # å•ç‚¹æ•°æ®
        single_df = get_cluster_visualization_data(
            embeddings[:1], labels[:1], texts[:1]
        )
        if len(single_df) == 1:
            logger.info("âœ… å•ç‚¹æ•°æ®å¤„ç†æ­£ç¡®")
        else:
            logger.warning("âš ï¸ å•ç‚¹æ•°æ®å¤„ç†å¯èƒ½æœ‰é—®é¢˜")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¯è§†åŒ–æ•°æ®ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_search_clustering_integration():
    """æµ‹è¯•æœç´¢å’Œèšç±»é›†æˆ"""
    logger.info("æµ‹è¯•æœç´¢å’Œèšç±»é›†æˆ...")
    
    try:
        from System.start import Cre_Search
        
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        config = {
            "milvus": {
                "collection_name": "test_collection",
                "host": "127.0.0.1",
                "port": "19530"
            },
            "search": {
                "topK": 10,
                "col_choice": "hdbscan",
                "reorder_strategy": "distance"
            },
            "system": {
                "url_split": False
            }
        }
        
        # æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•éœ€è¦å®é™…çš„Milvusè¿æ¥å’Œæ•°æ®
        # åœ¨æ²¡æœ‰çœŸå®æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæµ‹è¯•å‡½æ•°æ˜¯å¦å¯ä»¥è°ƒç”¨
        logger.info("âœ… æœç´¢èšç±»é›†æˆå‡½æ•°å¯è°ƒç”¨")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æœç´¢èšç±»é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸ§ª èšç±»å’Œå¯è§†åŒ–é›†æˆæµ‹è¯•")
    logger.info("=" * 60)
    
    test_results = {}
    
    # æµ‹è¯•èšç±»æ¨¡å—å¯¼å…¥
    test_results["clustering_imports"] = test_clustering_imports()
    
    # æµ‹è¯•å¯è§†åŒ–æ¨¡å—å¯¼å…¥
    test_results["visualization_imports"] = test_visualization_imports()
    
    # æµ‹è¯•èšç±»ç®—æ³•
    if test_results["clustering_imports"]:
        test_results["clustering_algorithms"] = test_clustering_algorithms()
    
    # æµ‹è¯•èšç±»é‡æ’åº
    if test_results["clustering_imports"]:
        test_results["cluster_reordering"] = test_cluster_reordering()
    
    # æµ‹è¯•å¯è§†åŒ–æ•°æ®ç”Ÿæˆ
    if test_results["visualization_imports"]:
        test_results["visualization_data"] = test_visualization_data_generation()
    
    # æµ‹è¯•æœç´¢èšç±»é›†æˆ
    test_results["search_clustering"] = test_search_clustering_integration()
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    logger.info("=" * 60)
    logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    
    results_summary = [
        ("èšç±»æ¨¡å—å¯¼å…¥", test_results.get("clustering_imports", False)),
        ("å¯è§†åŒ–æ¨¡å—å¯¼å…¥", test_results.get("visualization_imports", False)),
        ("èšç±»ç®—æ³•", test_results.get("clustering_algorithms", False)),
        ("èšç±»é‡æ’åº", test_results.get("cluster_reordering", False)),
        ("å¯è§†åŒ–æ•°æ®ç”Ÿæˆ", test_results.get("visualization_data", False)),
        ("æœç´¢èšç±»é›†æˆ", test_results.get("search_clustering", False))
    ]
    
    for test_name, result in results_summary:
        if result:
            logger.info(f"âœ… {test_name}: æˆåŠŸ")
        else:
            logger.error(f"âŒ {test_name}: å¤±è´¥")
    
    # æ€»ä½“è¯„ä¼°
    all_tests_passed = all(result for _, result in results_summary)
    
    if all_tests_passed:
        logger.info("ğŸ‰ æ‰€æœ‰èšç±»å’Œå¯è§†åŒ–æµ‹è¯•é€šè¿‡ï¼")
    else:
        logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
    
    return all_tests_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)