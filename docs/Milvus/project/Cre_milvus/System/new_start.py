"""
æ–°çš„å¯åŠ¨æ¨¡å—
ä½¿ç”¨é¢„è¿æ¥æ¶æ„ï¼Œé¿å…è¿æ¥é˜»å¡é—®é¢˜
"""

import logging
import os
import time
from typing import Dict, Any, List
from .connection_initializer import get_initializer, is_initialized

logger = logging.getLogger(__name__)

def validate_data_list(data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """éªŒè¯å’Œæ¸…ç†æ•°æ®åˆ—è¡¨"""
    if not data_list:
        return []
    
    valid_data = []
    for i, data in enumerate(data_list):
        if not isinstance(data, dict):
            logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®é¡¹ {i}: ä¸æ˜¯å­—å…¸æ ¼å¼")
            continue
        
        if "embedding" not in data or not data["embedding"]:
            logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®é¡¹ {i}: ç¼ºå°‘embedding")
            continue
        
        if "content" not in data or not data["content"]:
            logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®é¡¹ {i}: ç¼ºå°‘content")
            continue
        
        if "id" not in data:
            data["id"] = i  # è‡ªåŠ¨åˆ†é…ID
        
        valid_data.append(data)
    
    logger.info(f"æ•°æ®éªŒè¯å®Œæˆ: {len(valid_data)}/{len(data_list)} æ¡æœ‰æ•ˆ")
    return valid_data

def process_data_with_config(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """æ ¹æ®é…ç½®å¤„ç†æ•°æ®"""
    try:
        data_config = config.get("data", {})
        data_location = data_config.get("data_location", "./data/upload")
        
        if not os.path.exists(data_location):
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_location}")
        
        logger.info(f"å¼€å§‹å¤„ç†æ•°æ®ç›®å½•: {data_location}")
        
        # å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å—
        from dataBuilder.data import data_process
        
        # è·å–é…ç½®å‚æ•°
        system_config = config.get("system", {})
        chunking_config = config.get("chunking", {})
        multimodal_config = config.get("multimodal", {})
        
        chunking_params = {
            "chunk_length": chunking_config.get("chunk_length", 512),
            "ppl_threshold": chunking_config.get("ppl_threshold", 0.3),
            "confidence_threshold": chunking_config.get("confidence_threshold", 0.7),
            "similarity_threshold": chunking_config.get("similarity_threshold", 0.8),
            "overlap": chunking_config.get("overlap", 50),
            "language": chunking_config.get("language", "zh")
        }
        
        # å¤„ç†æ•°æ®
        start_time = time.time()
        data_list = data_process(
            data_location=data_location,
            url_split=system_config.get("url_split", False),
            chunking_strategy=chunking_config.get("strategy", "traditional"),
            chunking_params=chunking_params,
            enable_multimodal=multimodal_config.get("enable_image", False)
        )
        end_time = time.time()
        
        logger.info(f"æ•°æ®å¤„ç†å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        logger.info(f"å¤„ç†ç»“æœ: {len(data_list) if data_list else 0} æ¡æ•°æ®")
        
        return data_list or []
        
    except Exception as e:
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return []

def fast_vector_database_build(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    å¿«é€Ÿå‘é‡æ•°æ®åº“æ„å»º
    ä½¿ç”¨é¢„è¿æ¥æ¶æ„ï¼Œé¿å…è¿æ¥é˜»å¡
    """
    try:
        logger.info("=" * 60)
        logger.info("å¼€å§‹å¿«é€Ÿå‘é‡æ•°æ®åº“æ„å»º")
        logger.info("=" * 60)
        
        # 1. æ£€æŸ¥è¿æ¥çŠ¶æ€
        if not is_initialized():
            return {
                "status": "error",
                "msg": "ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨startup_initialize()"
            }
        
        logger.info("âœ… è¿æ¥çŠ¶æ€æ£€æŸ¥é€šè¿‡")
        
        # 2. å¤„ç†æ•°æ®
        logger.info("å¼€å§‹æ•°æ®å¤„ç†...")
        data_list = process_data_with_config(config)
        
        if not data_list:
            return {
                "status": "error",
                "msg": "æ•°æ®å¤„ç†å¤±è´¥æˆ–æ— æœ‰æ•ˆæ•°æ®"
            }
        
        # 3. éªŒè¯æ•°æ®
        valid_data = validate_data_list(data_list)
        if not valid_data:
            return {
                "status": "error",
                "msg": "æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä»¥æ’å…¥"
            }
        
        logger.info(f"æœ‰æ•ˆæ•°æ®é‡: {len(valid_data)}")
        
        # 4. å¿«é€Ÿæ’å…¥æ•°æ®
        logger.info("å¼€å§‹å¿«é€Ÿæ•°æ®æ’å…¥...")
        
        # è·å–é…ç½®
        milvus_config = config.get("milvus", {})
        system_config = config.get("system", {})
        
        collection_name = milvus_config.get("collection_name", "default_collection")
        index_name = milvus_config.get("index_name", "IVF_FLAT")
        replica_num = milvus_config.get("replica_num", 1)
        url_split = system_config.get("url_split", False)
        insert_mode = system_config.get("insert_mode", "è¦†ç›–ï¼ˆåˆ é™¤åŸæœ‰æ•°æ®ï¼‰")
        
        # æ„å»ºç´¢å¼•å‚æ•°
        from IndexParamBuilder.indexparam import indexParamBuilder
        index_param = indexParamBuilder(milvus_config.get("index_device", "cpu"), index_name)
        
        # ä½¿ç”¨å¿«é€Ÿæ’å…¥
        from milvusBuilder.fast_insert import fast_milvus_insert
        
        host = milvus_config.get("host", "127.0.0.1")
        port = int(milvus_config.get("port", "19530"))
        
        start_time = time.time()
        result = fast_milvus_insert(
            collection_name=collection_name,
            index_param=index_param,
            replica_num=replica_num,
            data_list=valid_data,
            url_split=url_split,
            insert_mode=insert_mode,
            milvus_host=host,
            milvus_port=port
        )
        end_time = time.time()
        
        logger.info(f"æ•°æ®æ’å…¥å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        if result.get("status") == "success":
            final_result = {
                "status": "success",
                "message": "å¿«é€Ÿå‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ",
                "processed_files": len(valid_data),
                "insert_result": result,
                "total_time": end_time - start_time
            }
            logger.info("ğŸ‰ å¿«é€Ÿå‘é‡æ•°æ®åº“æ„å»ºæˆåŠŸ!")
            return final_result
        else:
            logger.error(f"æ•°æ®æ’å…¥å¤±è´¥: {result}")
            return {
                "status": "error",
                "msg": f"æ•°æ®æ’å…¥å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}"
            }
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿå‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return {
            "status": "error",
            "msg": str(e)
        }

def fast_vector_database_build_from_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»é…ç½®æ„å»ºå¿«é€Ÿå‘é‡æ•°æ®åº“
    è¿™æ˜¯æ–°çš„å…¥å£å‡½æ•°ï¼Œæ›¿ä»£åŸæ¥çš„Cre_VectorDataBaseStart_from_config
    """
    try:
        logger.info("ä½¿ç”¨æ–°æ¶æ„è¿›è¡Œå‘é‡æ•°æ®åº“æ„å»º")
        return fast_vector_database_build(config)
        
    except Exception as e:
        logger.error(f"é…ç½®è§£æå¤±è´¥: {e}")
        return {
            "status": "error",
            "msg": str(e)
        }

def get_connection_status() -> Dict[str, Any]:
    """è·å–è¿æ¥çŠ¶æ€"""
    try:
        from milvusBuilder.persistent_connection import check_milvus_connection_status
        
        initializer = get_initializer()
        init_status = initializer.get_status()
        milvus_status = check_milvus_connection_status()
        
        return {
            "initializer_status": init_status,
            "milvus_connection": milvus_status,
            "overall_ready": is_initialized()
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "overall_ready": False
        }