# milvus pdf å¤šæ¨¡å‹åµŒå…¥å®æˆ˜

## ä¸€ã€ç¯å¢ƒå‡†å¤‡

éœ€è¦æå–å®‰è£…å¥½milvusçš„ç¯å¢ƒï¼Œæ¨èä½¿ç”¨ç‹¬ç«‹éƒ¨ç½²çš„ç‰ˆæœ¬ï¼Œæ€§èƒ½ç›¸å¯¹æ¥è¯´ä¼šæ›´å¥½ä¸€ç‚¹ã€‚

[milvus Standaloneç‰ˆéƒ¨ç½²](https://github.com/datawhalechina/easy-vectordb/blob/main/docs/Milvus%20Standaloneéƒ¨ç½².md) 

milvusæ•°æ®åº“ä¸€èˆ¬åœ¨19530è¿™ä¸ªç«¯å£ä¸Š

## äºŒã€æ¨¡å‹å‡†å¤‡

åµŒå…¥æ¨¡å‹å¯ä»¥é€šè¿‡[é­”å¡”ç¤¾åŒº]()å»ä¸‹è½½ï¼Œæœ¬æ–‡é€‰æ‹©äº†3ä¸ªä¸åŒçš„åµŒå…¥æ¨¡å‹

```python
models = {
    "MiniLM": "sentence-transformers/all-MiniLM-L6-v2",
    "Jina": "jinaai/jina-embeddings-v2-base-zh",
    "GTE": "iic/nlp_gte_sentence-embedding_chinese-base"
    }
```

å¯ä»¥ç›´æ¥åˆ©ç”¨ç¬¬ä¸‰æ–¹åŒ…è¿›è¡Œä¸‹è½½

```python
from modelscope import snapshot_download

def download(model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
             local_dir: str ="./"
             ):
    """
    ä½¿ç”¨é­”å¡”ç¤¾åŒºä¸‹è½½
    """
    logging.info(f"æ£€æµ‹åˆ°ä¿å­˜çš„æ–‡ä»¶å¤¹{local_dir}")
    #åˆ¤æ–­æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    folder_path=Path(local_dir) / model_name

    if folder_path.exists():
        logging.info(f"æ¨¡å‹å·²ç»å­˜åœ¨ï¼Œè·¯å¾„ä¸º {local_dir}")
    else:
        model_dir = snapshot_download(model_name,local_dir=folder_path)
        logging.info(f"æ¨¡å‹ä¸‹è½½æˆåŠŸï¼Œè·¯å¾„ä¸º {local_dir}")
        
models = {
    "MiniLM": "sentence-transformers/all-MiniLM-L6-v2",
    "Jina": "jinaai/jina-embeddings-v2-base-zh",
    "GTE": "iic/nlp_gte_sentence-embedding_chinese-base"
    }

for _,value in models.items():
    download(model_name=value,local_dir=Path(__file__).parent.absolute())
```

## ä¸‰ã€å¤„ç†pdf

å¯¹pdfæ–‡æ¡£è¿›è¡Œè¯»å–åï¼Œå®Œæˆåç»­çš„chunkç›¸å…³çš„æ“ä½œ

```python
# # 2.è¯»pdf
    pdf_path="./Datawhaleç¤¾åŒºä»‹ç».pdf"

    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )    
    # åˆ†å‰²æ–‡æ¡£
    doc_chunks = []
    for doc in documents:
        chunks = text_splitter.split_text(doc.page_content)
        for chunk in chunks:
            doc_chunks.append({
                'text': chunk,
                'source': pdf_path,
                'page': doc.metadata.get('page', 0)
            })

    texts = [doc["text"] for doc in doc_chunks]
    metas = [(doc["source"], doc["page"]) for doc in doc_chunks]
```

## å››ã€å¤šä¸ªæ¨¡å‹åµŒå…¥

åœ¨è¯­ä¹‰å¬å›ä¸­ï¼Œæœ‰æ—¶å•ä¸ªè¯­ä¹‰æ²¡æœ‰åŠæ³•å¾ˆå‡†ç¡®çš„å¬å›ç”¨æˆ·æŸ¥è¯¢çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¸åŒçš„åµŒå…¥æ¨¡å‹çš„ç»´åº¦å¤§å°ä¸åŒï¼Œå¯¹äºåŒä¸€ä¸ªé—®é¢˜ï¼Œä¸åŒçš„ç»´åº¦åœ¨ç›¸ä¼¼æ€§åŒ¹é…æ—¶é€Ÿåº¦ä¸åŒï¼Œå¯¹è¯­ä¹‰å™ªéŸ³çš„å®¹å¿åº¦ä¸åŒï¼Œå› æ­¤å¯ä»¥å¯¹åŒä¸€ä¸ªpdfæ–‡æœ¬æ„å»ºå¤šä¸ªä¸åŒçš„æ¨¡å‹åµŒå…¥ï¼Œæ ¹æ®å®é™…çš„æ€§èƒ½ä»¥åŠå‡†ç¡®åº¦çš„è¦æ±‚è¿›è¡Œå•æ¨¡å‹æˆ–è€…å¤šæ¨¡å‹çš„é€‰æ‹©

```python
from pymilvus import connections, utility, Collection, FieldSchema, CollectionSchema, DataType


connections.connect("default", host="localhost", port="19530")
    for name, model_path in models.items():
        print(f"ğŸ” æ­£åœ¨åŠ è½½æ¨¡å‹ {name}...")
        model = SentenceTransformer(model_path)

        print(f"ğŸ”„ æ­£åœ¨è¿›è¡ŒåµŒå…¥ï¼š{name}")
        vectors = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)

        dim = vectors.shape[1]
        collection_name = f"rag_{name.lower()}"

        # å¦‚æœå­˜åœ¨æ—§ collectionï¼Œå…ˆåˆ æ‰é‡å»º
        if utility.has_collection(collection_name):
            Collection(collection_name).drop()

        print(f"ğŸ“¦ åˆ›å»º Milvus collectionï¼š{collection_name}")

        # åˆ›å»º schema
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=10000),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="page", dtype=DataType.INT64)
        ]
        schema = CollectionSchema(fields=fields, description=f"{name} embedding collection")
        collection = Collection(name=collection_name, schema=schema)
        collection.create_index("embedding", {"index_type": "IVF_FLAT",
                                               "metric_type": "COSINE", 
                                               "params": {"nlist": 128}}
                                               )
        collection.load()

        # æ’å…¥æ•°æ®
        print(f"ğŸ“¥ å†™å…¥ {len(texts)} æ¡æ•°æ®åˆ° Milvusï¼ˆ{collection_name}ï¼‰")

        collection.insert(
        data = [
            vectors.tolist(),
            texts,
            [s for s, _ in metas],
            [p for _, p in metas],
        ],
        columns=["embedding", "text", "source", "page"]
        )
        print(f"âœ… [{name}] å·²å®Œæˆå†™å…¥ï¼")
```

å¯ä»¥çœ‹åˆ°milvusåœ¨defaultåº“ä¸­å»ºäº†3ä¸ªè¡¨

![fig10](/docs/src/fig10.png)

## äº”ã€æ¨¡å‹å¬å›ä¸é‡æ’

å…ˆå°†ä½¿ç”¨å‘é‡åº“çš„searchæœç´¢ï¼Œæ‰¾åˆ°ç²—å¬å›çš„ç›¸å…³èµ„æ–™ï¼Œç„¶åä½¿ç”¨rerankerå¯¹å†…å®¹è¿›è¡ŒäºŒæ¬¡æ’åºï¼Œæä¾›ç²¾ç¡®åº¦ã€‚

é‡æ’æ˜¯ä½¿ç”¨ä¸“ç”¨çš„é‡æ’æ¨¡å‹å¯¹å¬å›çš„å†…å®¹è¿›è¡Œæ¯”è¾ƒï¼Œç›¸å¯¹æ¥è¯´å‡†ç¡®åº¦ä¼šæ›´é«˜

```python
from FlagEmbedding import FlagReranker

reranker = FlagReranker('./BAAI/bge-reranker-base', use_fp16=True)  # use_fp16=False å¯åœ¨ CPU ä¸Šè¿è¡Œ

query = "é‡å­è®¡ç®—çš„åº”ç”¨åœºæ™¯"
documents = [
    "é‡å­è®¡ç®—æœºçš„å·¥ä½œåŸç†",
    "äººå·¥æ™ºèƒ½å‘å±•ç®€å²",
    "é‡å­åŠ å¯†æŠ€æœ¯çš„æœ€æ–°è¿›å±•"
]

# ç»„æˆå¥å¯¹
pairs = [[query, doc] for doc in documents]

# è®¡ç®—å¾—åˆ†
scores = reranker.compute_score(pairs)

# è¾“å‡ºæ’åºç»“æœ
results = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
for doc, score in results:
    print(f"å¾—åˆ†: {score:.4f} | æ–‡æ¡£: {doc}")
    
###########ç»“æœ##############
#å¾—åˆ†: 1.6082 | æ–‡æ¡£: é‡å­è®¡ç®—æœºçš„å·¥ä½œåŸç†
#å¾—åˆ†: -1.7742 | æ–‡æ¡£: é‡å­åŠ å¯†æŠ€æœ¯çš„æœ€æ–°è¿›å±•
#å¾—åˆ†: -3.8244 | æ–‡æ¡£: äººå·¥æ™ºèƒ½å‘å±•ç®€å²
```

ä½¿ç”¨collection.searchè¿›è¡Œæ•°æ®æœç´¢ï¼Œä½¿ç”¨rerankerè¿›è¡ŒäºŒæ¬¡çš„å‡†ç¡®åº¦è®¡ç®—ã€‚

```python
def search_question(reranker,query: str, top_k: int = 5):
    all_results = []
    
    for name, collection in collections.items():
        print(f"ğŸ” ä½¿ç”¨æ¨¡å‹ [{name}] æŸ¥è¯¢...")

        # ç”ŸæˆæŸ¥è¯¢ embedding
        embedding = models[name].encode(query, normalize_embeddings=True).tolist()

        # å‘é‡æ£€ç´¢
        res = collection.search(
            data=[embedding],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"nprobe": 10}},
            limit=top_k,
            output_fields=["text", "source", "page"]
        )

        for hit in res[0]:
            all_results.append({
                "model": name,
                "text": hit.entity.get("text"),
                "source": hit.entity.get("source"),
                "page": hit.entity.get("page"),
                "score": hit.distance
            })

    # å»é‡ï¼ˆä»¥æ–‡æœ¬ä¸ºå‡†ï¼‰
    unique = {}
    for r in all_results:
        if r["text"] not in unique or r["score"] < unique[r["text"]]["score"]:
            unique[r["text"]] = r

    deduped_results = list(unique.values())
    # === é‡æ’å¼€å§‹ ===
    pairs = [[query, r["text"]] for r in deduped_results]
    rerank_scores = reranker.compute_score(pairs)
    for i in range(len(deduped_results)):
        deduped_results[i]["rerank_score"] = rerank_scores[i]

    # æ’åº
    final_results = sorted(deduped_results, key=lambda x: x["rerank_score"], reverse=True)
    return final_results[:top_k]

```

çœ‹ä¸€ä¸‹æœ€ç»ˆçš„å¬å›ç»“æœ

![fig11](/docs/src/fig11.png)
