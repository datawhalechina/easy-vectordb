# åŸºäºMilvuså’ŒArangoDBçš„RAGç³»ç»Ÿ

> ipynbå¯æ‰§è¡Œä»£ç è¯·ç‚¹å‡»ï¼š[åŸºäºMilvuså’ŒArangoDBçš„RAGç³»ç»Ÿ.ipynb](https://github.com/datawhalechina/easy-vectordb/blob/main/docs/projects/project3/project3.ipynb)

å¾ˆå¤šå­¦ä¹ è€…æ²¡æœ‰äº†è§£è¿‡ArangoDBè¿™ä¸ªæ•°æ®åº“ï¼Œä¸‹é¢ï¼Œä½ å¯ä»¥é€šè¿‡è¯¥éƒ¨åˆ†ç³»ç»Ÿæ€§çš„äº†è§£è¿™ä¸ªæ•°æ®åº“ï¼Œä¹Ÿå¯ä»¥è·³è¿‡è¿™éƒ¨åˆ†ç›´æ¥çœ‹è®¾è®¡æ€æƒ³éƒ¨åˆ†ã€‚

## 1. ä»€ä¹ˆæ˜¯ ArangoDBï¼Ÿ(Core Concept)

åœ¨ä¼ ç»Ÿæ¶æ„ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ä¸€ä¸ª **MongoDB** å­˜æ–‡æ¡£ï¼Œå†ç”¨ä¸€ä¸ª **Neo4j** å­˜å›¾å…³ç³»ã€‚
**ArangoDB** çš„æ ¸å¿ƒç†å¿µæ˜¯ **Multi-Modelï¼ˆå¤šæ¨¡æ€ï¼‰**ï¼š
> **â€œOne Engine, One Query Language, Multiple Data Modelsâ€**

å®ƒåœ¨ä¸€ä¸ªæ•°æ®åº“å¼•æ“ä¸­åŒæ—¶æ”¯æŒï¼š
1.  **æ–‡æ¡£ (Documents)**ï¼šåƒ MongoDB ä¸€æ ·å­˜å‚¨ JSON æ•°æ®ï¼ˆæˆ‘ä»¬çš„ Chunks æ­£æ–‡ï¼‰ã€‚
2.  **å›¾ (Graphs)**ï¼šåƒ Neo4j ä¸€æ ·å­˜å‚¨èŠ‚ç‚¹å…³ç³»ï¼ˆæˆ‘ä»¬çš„ Context ä¸Šä¸‹æ–‡ï¼‰ã€‚
3.  **é”®å€¼ (Key-Value)**ï¼šåƒ Redis ä¸€æ ·å¿«é€Ÿè¯»å†™ï¼ˆæˆ‘ä»¬çš„ Cacheï¼‰ã€‚

### ä¸ºä»€ä¹ˆæœ¬é¡¹ç›®çš„ RAG ç³»ç»Ÿé€‰æ‹© ArangoDBï¼Ÿ
*   **å­˜ç®—åˆ†ç¦»**ï¼šMilvus è¿™ç§æ˜‚è´µçš„æ˜¾å­˜èµ„æºåªå­˜â€œç´¢å¼•å‘é‡â€ï¼Œè€Œæµ·é‡çš„â€œæ–‡æœ¬è‚‰èº«â€éœ€è¦ä¸€ä¸ªæ”¯æŒå€’æ’ç´¢å¼•çš„æ•°æ®åº“æ¥å­˜ï¼ŒArangoDB çš„æ–‡æ¡£å­˜å‚¨éå¸¸é€‚åˆã€‚
*   **ä¸Šä¸‹æ–‡å¬å›**ï¼šå½“ RAG æ£€ç´¢åˆ°ä¸€æ®µè¯æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ¯«ç§’çº§æ‰¾å›å®ƒçš„**â€œä¸Šä¸€æ®µè¯â€**æˆ–**â€œçˆ¶æ ‡é¢˜â€**ã€‚ä¼ ç»Ÿæ•°æ®åº“åšå…³è”æŸ¥è¯¢ï¼ˆJoinï¼‰å¾ˆæ…¢ï¼Œè€Œ ArangoDB çš„**åŸç”Ÿå›¾éå† (Graph Traversal)** æå¿«ã€‚
*   **ç»Ÿä¸€æŸ¥è¯¢ (AQL)**ï¼šæˆ‘ä»¬å¯ä»¥ç”¨ç±»ä¼¼ SQL çš„è¯­æ³•ï¼ˆAQLï¼‰åŒæ—¶å®Œæˆâ€œè¿‡æ»¤ Cluster IDâ€å’Œâ€œæŸ¥æ‰¾å›¾é‚»å±…â€ä¸¤ä¸ªæ“ä½œã€‚

---

## 2. æ ¸å¿ƒæœ¯è¯­ (Key Terminology)

åœ¨ä»£ç ä¸­ä½ ä¼šé¢‘ç¹é‡åˆ°ä»¥ä¸‹ä¸‰ä¸ªæ¦‚å¿µï¼Œè¯·åŠ¡å¿…åˆ†æ¸…ï¼š

| æœ¯è¯­ | å¯¹åº”å…³ç³»å‹æ•°æ®åº“ | æˆ‘ä»¬çš„é¡¹ç›®ç”¨é€” | ç¤ºä¾‹ |
| :--- | :--- | :--- | :--- |
| **Collection** | è¡¨ (Table) | å­˜æ”¾æ•°æ®çš„å®¹å™¨ | `rag_chunks` |
| **Document** | è¡Œ (Row) | å®é™…çš„æ•°æ®è®°å½• (JSON) | `{ "text": "...", "cluster_id": 101 }` |
| **Edge** | å…³è”è¡¨ (Join Table) | **è¿æ¥çº¿**ï¼Œç‰¹æ®Šçš„ Documentï¼Œå¿…é¡»åŒ…å« `_from` å’Œ `_to` | `{ "_from": "Chunk/A", "_to": "Chunk/B", "type": "NEXT_TO" }` |
| **Graph** | è§†å›¾ (View) | å®šä¹‰å“ªäº› Collection å’Œ Edge ç»„æˆä¸€å¼ ç½‘ | `rag_knowledge_graph` |

---

## 3. åŸºç¡€ä½¿ç”¨æ–¹æ³•

æœ¬é¡¹ç›®ä½¿ç”¨ `python-arango` é©±åŠ¨ã€‚ä»¥ä¸‹æ˜¯ä½ åœ¨é¡¹ç›®ä¸­å¿…é¡»æŒæ¡çš„ **CRUD** å’Œ **å›¾æ“ä½œ** æ¨¡æ¿ã€‚

### 3.1 è¿æ¥ä¸åˆå§‹åŒ–

```python
from arango import ArangoClient

# 1. å»ºç«‹è¿æ¥
client = ArangoClient(hosts='http://127.0.0.1:8529')

# 2. è¿æ¥/åˆ›å»ºæ•°æ®åº“
sys_db = client.db('_system', username='root', password='pass123')
if not sys_db.has_database('rag_db'):
    sys_db.create_database('rag_db')

db = client.db('rag_db', username='root', password='pass123')
```

### 3.2 å­˜å‚¨æ–‡æ¡£ 

è¿™æ˜¯æˆ‘ä»¬å­˜å‚¨ Chunk æ­£æ–‡çš„åœ°æ–¹ã€‚

```python
# åˆ›å»ºé›†åˆ (ç±»ä¼¼å»ºè¡¨)
if not db.has_collection('rag_chunks'):
    chunks = db.create_collection('rag_chunks')
else:
    chunks = db.collection('rag_chunks')

# æ’å…¥æ•°æ®
doc = {
    "_key": "uuid_1",  # æŒ‡å®šä¸»é”®ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
    "text": "è¿™æ˜¯ç¬¬ä¸€æ®µè¯ã€‚",
    "cluster_id": 101
}
chunks.insert(doc, overwrite=True) # overwrite=True ç±»ä¼¼ Upsert

# æŸ¥è¯¢æ•°æ® (é€šè¿‡ Key)
result = chunks.get("uuid_1")
print(result['text'])
```

### 3.3 åˆ›å»ºå…³ç³» (Edge Operation)

è¿™æ˜¯æˆ‘ä»¬æ„å»ºâ€œä¸Šä¸‹æ–‡é“¾æ¡â€çš„å…³é”®ã€‚

```python
# åˆ›å»ºè¾¹é›†åˆ (å¿…é¡»æŒ‡å®š edge=True)
if not db.has_collection('rag_relations'):
    edges = db.create_collection('rag_relations', edge=True)

# æ’å…¥ä¸€æ¡è¾¹ï¼šè¡¨ç¤º uuid_1 çš„ä¸‹ä¸€æ®µæ˜¯ uuid_2
edge_data = {
    "_from": "rag_chunks/uuid_1",  # å¿…é¡»å¸¦é›†åˆå‰ç¼€
    "_to":   "rag_chunks/uuid_2",
    "type":  "NEXT_TO"
}
edges.insert(edge_data)
```

### 3.4 é«˜çº§æŸ¥è¯¢ï¼šAQL (ArangoDB Query Language)

è¿™æ˜¯ ArangoDB æœ€å¼ºå¤§çš„åœ°æ–¹ã€‚AQL çœ‹èµ·æ¥å¾ˆåƒ SQLã€‚

**åœºæ™¯ 1ï¼šæ™®é€šæŸ¥è¯¢**
*â€œç»™æˆ‘æ‰¾å‡ºå±äºèšç±» 101 çš„æ‰€æœ‰æ–‡æ¡£ã€‚â€*

```python
aql = """
FOR doc IN rag_chunks
    FILTER doc.cluster_id == 101
    RETURN { id: doc._key, content: doc.text }
"""
cursor = db.aql.execute(aql)
for item in cursor:
    print(item)
```

**åœºæ™¯ 2ï¼šå›¾éå†ï¼ˆä¸Šä¸‹æ–‡æ‰©å±•ï¼‰**
*â€œæ‰¾åˆ° uuid_1 è¿™æ®µè¯ï¼Œå¹¶ä¸”é¡ºç€ 'NEXT_TO' å…³ç³»ï¼ŒæŠŠå®ƒåé¢çš„ä¸€æ®µè¯ä¹Ÿæ‰¾å‡ºæ¥ã€‚â€*

```python
graph_aql = """
FOR v, e, p IN 1..1 OUTBOUND 'rag_chunks/uuid_1' rag_relations
    FILTER e.type == 'NEXT_TO'
    RETURN v.text
"""
# 1..1 OUTBOUND: å‘å¤–èµ° 1 æ­¥
# v: vertex (èŠ‚ç‚¹/ä¸‹ä¸€æ®µè¯)
# e: edge (è¾¹)
# p: path (è·¯å¾„)

cursor = db.aql.execute(graph_aql)
next_text = [doc for doc in cursor]
print(f"ä¸‹æ–‡æ˜¯: {next_text}")
```

---

## 4. åœ¨æœ¬é¡¹ç›®ä¸­çš„æ•°æ®æµè½¬å›¾

ä¸ºäº†è®©ä½ å½»åº•ç†è§£ï¼Œè¯·çœ‹è¿™å¼ æ•°æ®åœ¨ ArangoDB å†…éƒ¨çš„æµè½¬å›¾ï¼š

```mermaid
graph LR
    subgraph ArangoDB
        direction TB
        
        C1[Chunk A] -- NEXT_TO --> C2[Chunk B]
        C2 -- NEXT_TO --> C3[Chunk C]
        
        Header[æ ‡é¢˜: è´¢åŠ¡æŠ¥è¡¨] -- PARENT_OF --> C1
        Header -- PARENT_OF --> C2
        
        style C1 fill:#e1f5fe,stroke:#01579b
        style C2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px
        style C3 fill:#e1f5fe,stroke:#01579b
    end
    
    Milvus(Milvus è·¯ç”±) -.->|Cluster ID: 101| C2
    
    classDef highlight fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;
```

1.  **Milvus** å‘Šè¯‰æˆ‘ä»¬è¦æ‰¾ **Chunk B**ï¼ˆé»„è‰²é«˜äº®ï¼‰ã€‚
2.  æˆ‘ä»¬ç›´æ¥å» ArangoDB æ‹¿åˆ° **Chunk B** çš„æ­£æ–‡ã€‚
3.  é€šè¿‡ **AQL å›¾éå†**ï¼Œæˆ‘ä»¬ç¬é—´æŠ“å–åˆ°ï¼š
    *   `OUTBOUND` -> **Chunk C** (ä¸‹æ–‡)
    *   `INBOUND` -> **Chunk A** (ä¸Šæ–‡)
    *   `INBOUND` -> **Header** (çˆ¶æ ‡é¢˜)

## è®¾è®¡æ€æƒ³
ä¼ ç»Ÿçš„RAGï¼ˆåˆ‡ç‰‡+å‘é‡åº“ï¼‰å­˜åœ¨ä¸¤ä¸ªè‡´å‘½é—®é¢˜ï¼š
1. æ–­ç« å–ä¹‰ï¼šæŠŠæ–‡æ¡£åˆ‡å‰²ä¸º500å­—ä¸€æ®µï¼Œä¸¢å¤±äº†â€è¿™æ®µè¯å±äºå“ªä¸ªæ ‡é¢˜â€œçš„ä¸Šä¸‹æ–‡ã€‚
2. æ˜¾å­˜æ˜‚è´µï¼šå‡ ç™¾ä¸‡æ¡å‘é‡å…¨éƒ¨å¡å…¥Milvusï¼Œå†…å­˜å ç”¨å·¨å¤§ï¼Œä¸”åŒ…å«å¤§é‡æ— å…³å™ªéŸ³ã€‚

æˆ‘ä»¬è¿™é‡Œæå‡ºäº†ä¸€ç§æ–°çš„è®¾è®¡æ–¹æ¡ˆï¼šFusionGraph RAGï¼šåŸºäºèšç±»è·¯ç”±ä¸å›¾è°±å¢å¼ºçš„æ£€ç´¢ç³»ç»Ÿ

æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆï¼š
1. Meta-chunkingï¼šåˆ©ç”¨OCRç‰ˆé¢åˆ†æï¼ŒæŒ‰ç…§æ ‡é¢˜å’Œè¯­ä¹‰åˆ‡åˆ†ï¼Œè€Œä¸æ˜¯æŒ‰å­—æ•°ã€‚(æœ¬æ•™ç¨‹ä¸ºäº†å­¦ä¹ è€…æ›´å¥½çš„è·‘é€šæµç¨‹ï¼Œå¹¶æœªä½¿ç”¨OCR)
2. FusionANNSï¼š
    * Milvuså˜èº«ä¸ºè·¯ç”±è¡¨ï¼Œåªå­˜å‚¨èšç±»ä¸­å¿ƒï¼Œæåº¦èŠ‚çœå†…å­˜
    * ArangoDBå˜ä¸ºè—ä¹¦é˜ï¼Œå­˜å‚¨åŸå§‹æ–‡æœ¬å’Œå›¾å…³ç³»ã€‚
3. Graph Expansionï¼ˆå›¾è°±æ‰©å±•ï¼‰ï¼šæ£€ç´¢åˆ°ä¸€å¥è¯æ—¶ï¼Œé¡ºç€å›¾å…³ç³»æŠŠä»–ä»¬çš„çˆ¶æ ‡é¢˜å’Œå‰ä¸€æ®µä¸€èµ·æå‡ºæ¥ï¼Œç»™LLMçœ‹å®Œæ•´çš„ä¸Šä¸‹æ–‡ã€‚


| é˜¶æ®µ (Stage) | è¾“å…¥ (Input) | æ ¸å¿ƒåŠ¨ä½œ (Action) | è¾“å‡º (Output) | æ‰¿è½½ç»„ä»¶ |
| :--- | :--- | :--- | :--- | :--- |
| **1. ETL æ•°æ®å¤„ç†å±‚** | åŸå§‹ PDF | **OCR ç‰ˆé¢åˆ†æ** + **Meta-Chunking** (è¯­ä¹‰åˆ‡åˆ†) | ç»“æ„åŒ–çš„ Chunks (å« Header è·¯å¾„) | PaddleOCR, Python |
| **2. ç´¢å¼•æ„å»ºå±‚** | Chunks | **BGE-M3 å‘é‡åŒ–** + **FAISS èšç±»** | è´¨å¿ƒå‘é‡ (Centroids) + èšç±» ID | FAISS (GPU), BGE-M3 |
| **3. å­˜å‚¨ä¸æœåŠ¡å±‚** | è´¨å¿ƒ & Chunks | **åŒå†™åˆ†å‘ (Dual Ingestion)** | Milvus (è·¯ç”±) + ArangoDB (å†…å®¹) | Milvus, ArangoDB |

## è¯¦ç»†è¡¨ç»“æ„è®¾è®¡
ä¸¥æ ¼éµå¾ªå­˜ç®—åˆ†ç¦»åŸåˆ™ï¼šMilvusåªå­˜å‚¨ç´¢å¼•å¤´ï¼ŒArangoDBå­˜å‚¨å…¨é‡æ•°æ®

### 1.Milvusè¡¨ç»“æ„è®¾è®¡
**è®¾è®¡åŸåˆ™**
æåº¦ç˜¦èº«ï¼Œåªå­˜å‚¨è¯é¢˜ç°‡çš„ä¸­å¿ƒç‚¹ï¼Œç”¨äºå¿«é€Ÿå®šä½ç”¨æˆ·é—®é¢˜å±äºå“ªä¸ªé¢†åŸŸï¼ˆä¾‹å¦‚ï¼šè´¢åŠ¡ã€æŠ€æœ¯ã€è¿åŠ¨ï¼‰

Collection Nameï¼šrag_cluster_centorids
| å­—æ®µå | æ•°æ®ç±»å‹ | å±æ€§ | è§£é‡Šä¸è®¾è®¡ç†ç”± |
| :--- | :--- | :--- | :--- |
| **`cluster_id`** | **Int64** | **Primary Key** | **æ ¸å¿ƒè¿æ¥ç‚¹**ã€‚è¿™æ˜¯è¿æ¥ Milvus å’Œ ArangoDB çš„å”¯ä¸€é’¥åŒ™ã€‚ä¾‹å¦‚ï¼š`101`ã€‚ |
| **`vector`** | **FloatVector** | Dim=1024 | **è´¨å¿ƒå‘é‡**ã€‚è¯¥èšç±»ä¸‹æ‰€æœ‰ Chunk å‘é‡çš„å¹³å‡å€¼ã€‚æŸ¥è¯¢æ—¶ç”¨å®ƒåšç›¸ä¼¼åº¦åŒ¹é…ã€‚ |
| `member_count` | Int32 | Scalar | è¯¥èšç±»åŒ…å«å¤šå°‘ä¸ª Chunkã€‚ç”¨äºåç»­å¯èƒ½çš„æƒé‡è°ƒæ•´ï¼ˆæ¯”å¦‚å¤§èšç±»é™æƒï¼‰ã€‚ |
* ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾è®¡ï¼Ÿ
    * çœé’±ï¼šå‡è®¾æœ‰1000ä¸‡ä¸ªchunkï¼Œèšæˆä¸€ä¸‡ä¸ªç±»ï¼ŒMilvusåªå­˜å‚¨1ä¸‡æ¡å‘é‡ï¼Œå†…å­˜å ç”¨ç›´æ¥å˜ä¸ºåŸæ¥çš„ä¸€åƒåˆ†ä¹‹ä¸€
    * å¿«ï¼šåœ¨ä¸€ä¸‡æ¡æ•°æ®é‡Œæœç´¢Top-5ï¼Œæ¯”åœ¨1000ä¸‡æ¡é‡Œæœï¼Œé€Ÿåº¦å¿«å‡ ä¸ªæ•°é‡çº§

### 2.ArangoDBè¡¨ç»“æ„è®¾è®¡
**è®¾è®¡åŸåˆ™**å­˜å‚¨å…¨é‡æ•°æ®ï¼Œå¹¶åˆ©ç”¨å›¾å…³ç³»è§£å†³ä¸Šä¸‹æ–‡ä¸¢å¤±é—®é¢˜

#### A. èŠ‚ç‚¹é›†åˆ (Document Collection): `rag_chunks`

| å­—æ®µå | ç±»å‹ | ç´¢å¼•ç±»å‹ | è§£é‡Šä¸è®¾è®¡ç†ç”± |
| :--- | :--- | :--- | :--- |
| **`_key`** | String | Primary | **Chunk UUID**ã€‚å”¯ä¸€æ ‡è¯†ä¸€ä¸ªæ–‡æœ¬å—ã€‚ |
| **`cluster_id`** | Int64 | **Persistent Index** | **å¤–é”®**ã€‚å¯¹åº” Milvus é‡Œçš„ IDã€‚**æŸ¥è¯¢æ—¶é€šè¿‡æ­¤å­—æ®µæ¯«ç§’çº§æ‹‰å–è¯¥ç°‡æ‰€æœ‰æ•°æ®ã€‚** |
| `text` | String | None | **æ­£æ–‡**ã€‚æœ€å ç©ºé—´çš„æ•°æ®ï¼Œå­˜åœ¨ç£ç›˜ä¸Šï¼Œä¸å å®è´µçš„æ˜¾å­˜ã€‚ |
| `header_path`| List[Str]| None | **å±‚çº§è·¯å¾„**ã€‚ä¾‹å¦‚ `['2023å¹´æŠ¥', 'è´¢åŠ¡æ•°æ®']`ã€‚ç”¨äºç”Ÿæˆå¼•ç”¨æ¥æºã€‚ |
| `metadata` | JSON | None | å­˜ Page, BBox ç­‰å…ƒæ•°æ®ï¼Œç”¨äºå‰ç«¯é«˜äº®æ˜¾ç¤ºã€‚ |

#### B. è¾¹é›†åˆ (Edge Collection): `rag_relations`

| å­—æ®µå | ç±»å‹ | è§£é‡Šä¸è®¾è®¡ç†ç”± |
| :--- | :--- | :--- |
| **`_from`** | String | èµ·å§‹èŠ‚ç‚¹ ID (ä¾‹å¦‚ `rag_chunks/uuid_A`) |
| **`_to`** | String | ç›®æ ‡èŠ‚ç‚¹ ID (ä¾‹å¦‚ `rag_chunks/uuid_B`) |
| **`type`** | String | **å…³ç³»ç±»å‹**ã€‚æ ¸å¿ƒè®¾è®¡ç‚¹ï¼Œç”¨äºä¸Šä¸‹æ–‡æ‰©å±•ã€‚ |

*   **å…³ç³»ç±»å‹ (`type`) è¯¦è§£**ï¼š
    *   `NEXT_TO`: è¡¨ç¤ºé˜…è¯»é¡ºåºã€‚æŸ¥è¯¢åˆ° Chunk A æ—¶ï¼Œé¡ºç€è¿™æ¡è¾¹èƒ½æ‰¾åˆ° Chunk Bï¼ˆä¸‹ä¸€æ®µï¼‰ã€‚
    *   `PARENT_OF`: è¡¨ç¤ºå±‚çº§ã€‚æŸ¥è¯¢åˆ° Chunk A æ—¶ï¼Œåå‘éå†è¿™æ¡è¾¹èƒ½æ‰¾åˆ°å®ƒçš„çˆ¶æ ‡é¢˜èŠ‚ç‚¹ï¼ˆå¦‚æœæœ‰ç‹¬ç«‹æ ‡é¢˜èŠ‚ç‚¹è®¾è®¡ï¼‰ã€‚

*   **ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾è®¡ï¼Ÿ**
    *   **ä¸Šä¸‹æ–‡æ•‘æ˜Ÿ**ï¼šå½“ LLM çœ‹åˆ°ä¸€å¥â€œå‡€åˆ©æ¶¦å¢é•¿ 10%â€æ—¶ï¼Œå®ƒä¸çŸ¥é“æ˜¯è°çš„ã€‚é€šè¿‡å›¾éå† `PARENT_OF` æ‰¾åˆ°çˆ¶æ ‡é¢˜â€œåä¸œåˆ†å…¬å¸â€ï¼ŒLLM å°±èƒ½ç²¾å‡†å›ç­”ã€‚
    *   **çµæ´»**ï¼šMilvus è¿™ç§å‘é‡åº“å¾ˆéš¾å­˜è¿™ç§ç½‘çŠ¶å…³ç³»ï¼Œå›¾æ•°æ®åº“æ˜¯æœ€ä½³é€‰æ‹©ã€‚

## æ•°æ®æ ¼å¼

### åˆ†æ®µåçš„æ•°æ®æ ¼å¼
```json
{
  "id": "a1b2c3d4-5678-90ef-...",  // UUID v4 æˆ– MD5
  "text": "2023å¹´å…¬å¸å‡€åˆ©æ¶¦ä¸º2.5äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿15%...",
  "metadata": {
    "source_file": "2023_financial_report.pdf",
    "page_number": 5,
    "chunk_index": 42,             // åœ¨å…¨æ–‡ä¸­çš„åºå·ï¼Œç”¨äºæ’åº
    "bbox": [100, 200, 500, 600],  // PaddleOCR æä¾›çš„åæ ‡ [x1, y1, x2, y2]
    "type": "text"                 // text / table
  },
  "header_path": ["2023å¹´åº¦æŠ¥å‘Š", "ç¬¬å››ç«  è´¢åŠ¡æ•°æ®", "ä¸»è¦ä¼šè®¡æ•°æ®"], // æ ¸å¿ƒï¼šå±‚çº§è·¯å¾„
  
  // é¢„ç•™å­—æ®µï¼Œç¨åè®¡ç®—å¡«å……
  "vector": null,                  // ç­‰å¾… BGE-M3 å¡«å……
  "sparse_vector": null,           // ç­‰å¾… BGE-M3 å¡«å……
  "cluster_id": -1,                // ç­‰å¾… FAISS å¡«å……
  "prev_chunk_id": "e5f6...",      // æŒ‡å‘ç¬¬ 41 å· Chunk çš„ ID
  "next_chunk_id": "g7h8..."       // æŒ‡å‘ç¬¬ 43 å· Chunk çš„ ID
}
```
### ArangoDBæ•°æ®æ ¼å¼
#### èŠ‚ç‚¹è¡¨-å­˜æ­£æ–‡
```json
// Document Example
{
  "_key": "a1b2c3d4...",           // ç›´æ¥å¤ç”¨ Chunk çš„ UUID
  "text": "2023å¹´å…¬å¸å‡€åˆ©æ¶¦...",
  "source": "2023_financial_report.pdf",
  "page": 5,
  "cluster_id": 105,               // å…³é”®ç´¢å¼•å­—æ®µï¼šç”¨äºä» Milvus è·¯ç”±è¿‡æ¥
  "header_path_str": "2023å¹´åº¦æŠ¥å‘Š > è´¢åŠ¡æ•°æ®", // æ–¹ä¾¿äººç±»é˜…è¯»çš„å­—ç¬¦ä¸²
  "embedding_status": true
}
```
#### èŠ‚ç‚¹è¡¨-å­˜æ ‡é¢˜
```json
// Document Example
{
  "_key": "md5_of_header_text",    // æ ‡é¢˜å†…å®¹çš„ Hash
  "text": "ä¸»è¦ä¼šè®¡æ•°æ®",
  "level": 3                       // H3 æ ‡é¢˜
}
```
#### è¾¹è¡¨-å­˜å…³ç³»
```json
{
  "_from": "rag_chunks/chunk_42",
  "_to": "rag_chunks/chunk_43",
  "type": "NEXT_TO"
}
```
## æ•°æ®å¤„ç†

[document.pdf](https://github.com/Anduin2017/HowToCook)æ˜¯ä¸€ä¸ªåšé¥­çš„æ–‡æ¡£ï¼Œé‡Œé¢æœ‰800å¤šé¡µï¼Œç°åœ¨æˆ‘ä»¬å¯¹å…¶è¿›è¡Œå¤„ç†ï¼Œä¿å­˜ä¸ºæˆ‘ä»¬éœ€è¦çš„æ•°æ®æ ¼å¼ï¼Œæœ‰æ¡ä»¶çš„å¯ä»¥ç”¨OCRï¼Œä½†è¿™é‡Œæˆ‘ç”¨çš„PyMuPDF+Rayï¼Œå°½å¯èƒ½æŠŠCPUè·‘æ»¡ï¼Œæ•ˆç‡æœ€å¤§åŒ–,PyMuPDFæ˜¯Pythonç•Œæœ€å¿«çš„PDFè§£æåº“ï¼Œåº•å±‚åŸºäºC++å®ç°ï¼Œå¯ä»¥ç›´æ¥æå–æ–‡æœ¬çš„å­—ä½“å¤§å°å’Œä½ç½® ã€‚

è¿™é‡Œæˆ‘ä»¬è¦å¯¹ä¹‹å‰çš„ç­–ç•¥è¿›è¡Œè°ƒæ•´ï¼šä»¥å‰æˆ‘ä»¬é OCRå‘Šè¯‰æˆ‘ä»¬å“ªé‡Œæ˜¯æ ‡é¢˜ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦é å­—ä½“å¤§å°çŒœæµ‹æ ‡é¢˜ä½ç½®ï¼ˆæ¯”å¦‚ï¼šå­—å· > æ­£æ–‡å¹³å‡å­—å·çš„ 1.2 å€ -> è§†ä¸º H1/H2ï¼‰ã€‚

```shell
pip install ray pymupdf langchain langchain-community zhipuai faiss-cpu numpy tqdm
```
1.  **Ray + PyMuPDF**: å¹¶è¡Œæå– 800 é¡µ PDF çš„çº¯æ–‡æœ¬ã€‚
2.  **LangChain Splitter**: ä½¿ç”¨æ ‡å‡†çš„ `RecursiveCharacterTextSplitter` è¿›è¡Œåˆ‡ç‰‡ã€‚
3.  **ZhipuAI API**: è°ƒç”¨äº‘ç«¯æ¨¡å‹ç”Ÿæˆå‘é‡ã€‚
4.  **FAISS (CPU)**: æœ¬åœ°å¿«é€Ÿèšç±»ã€‚


1.  **Ray è´Ÿè´£è„æ´»ç´¯æ´»**ï¼š
    *   PDF è§£ææ˜¯ CPU å¯†é›†å‹ä»»åŠ¡ï¼ŒRay åˆ©ç”¨å¤šæ ¸å¹¶è¡Œå¤„ç†ï¼Œé€Ÿåº¦æå¿«ã€‚
    *   åªæå–çº¯æ–‡æœ¬ï¼Œå†…å­˜å ç”¨æä½ã€‚

2.  **LangChain è´Ÿè´£è§„èŒƒåŒ–**ï¼š
    *   ä½¿ç”¨äº† `RecursiveCharacterTextSplitter`ã€‚è¿™æ˜¯ç›®å‰æœ€é€šç”¨çš„åˆ†æ®µæ–¹å¼ï¼Œè™½ç„¶å®ƒä¸å¦‚ Meta-Chunking æ™ºèƒ½ï¼ˆä¼šä¸¢å¤± Header å±‚çº§ä¿¡æ¯ï¼‰ï¼Œä½†**å…¼å®¹æ€§æœ€å¥½ï¼Œä¸Šæ‰‹æœ€å¿«**ã€‚
    *   å®ƒä¼šè‡ªåŠ¨å¤„ç†æ ‡ç‚¹ç¬¦å·åˆ‡åˆ†ï¼Œä¿è¯å¥å­å°½é‡å®Œæ•´ã€‚

3.  **æ™ºè°± API è´Ÿè´£æ ¸å¿ƒç®—åŠ›**ï¼š
    *   ä½¿ç”¨äº† `ZhipuAIEmbeddings`ã€‚ä½ åªéœ€è¦å¡« Keyï¼Œå‰©ä¸‹çš„äº¤ç»™äº‘ç«¯ã€‚
    *   **æ³¨æ„**ï¼š800 é¡µ PDF å¯èƒ½ä¼šç”Ÿæˆ 5000-8000 ä¸ª Chunkã€‚æ™ºè°± API æ˜¯æ”¶è´¹çš„ï¼ˆè™½ç„¶ embedding-2 å¾ˆä¾¿å®œï¼‰ï¼Œè¯·å…³æ³¨ä½ çš„ Token ç”¨é‡ã€‚

4.  **æ•°æ®ç»“æ„ä¿æŒå…¼å®¹**ï¼š
    *   è™½ç„¶å› ä¸ºä½¿ç”¨äº† LangChain Splitterï¼Œæˆ‘ä»¬ä¸¢å¤±äº† `header_path` çš„è‡ªåŠ¨æå–ï¼ˆç°åœ¨ä¸ºç©ºåˆ—è¡¨ `[]`ï¼‰ï¼Œä½† `cluster_id` å’Œå›¾è°±æ‰€éœ€çš„ `prev/next` é“¾è¡¨å…³ç³»ä¾ç„¶ä¿ç•™ã€‚è¿™ä¸å½±å“æˆ‘ä»¬åç»­æ„å»ºâ€œå­˜ç®—åˆ†ç¦»â€æ¶æ„ï¼Œåªæ˜¯å›¾è°±é‡Œå°‘äº†ä¸€ç§â€œçˆ¶å­å…³ç³»â€è¾¹è€Œå·²ã€‚

```python
import ray
import fitz  # PyMuPDF
import os
import json
import uuid
import numpy as np
import faiss
from typing import List, Dict
from tqdm import tqdm

# LangChain ç»„ä»¶
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import ZhipuAIEmbeddings

# ==================================================
# é…ç½®åŒºåŸŸ
# ==================================================
ZHIPU_API_KEY = ""  # ä½ çš„æ™ºè°±APIKey
PDF_PATH = "document.pdf"   # ä½ çš„PDFè·¯å¾„

# ==================================================
# 1. Ray Actor: PDF æ–‡æœ¬æå–å·¥å…µ
# ==================================================
@ray.remote
class PDFTextExtractor:
    def __init__(self):
        pass

    def extract_text(self, pdf_path, start_page, end_page):
        """
        åªè´Ÿè´£æå–æ–‡æœ¬ï¼Œä¸è´Ÿè´£åˆ†æ®µã€‚
        è¿”å›: List[Dict] -> [{'page': 1, 'text': '...'}, ...]
        """
        doc = fitz.open(pdf_path)
        results = []
        
        # é˜²æ­¢é¡µç è¶Šç•Œ
        total = len(doc)
        
        for p_num in range(start_page, end_page):
            if p_num >= total: break
            
            page = doc[p_num]
            text = page.get_text("text") # ç›´æ¥æå–çº¯æ–‡æœ¬
            
            # ç®€å•çš„æ¸…æ´—ï¼Œå»æ‰è¿‡å¤šçš„ç©ºè¡Œ
            clean_text = "\n".join([line.strip() for line in text.split('\n') if line.strip()])
            
            if clean_text:
                results.append({
                    "page": p_num + 1,
                    "text": clean_text
                })
                
        doc.close()
        return results

# ==================================================
# 2. LangChain åˆ†æ®µé€»è¾‘
# ==================================================
def split_text_with_langchain(raw_pages: List[Dict]):
    print("--- æ­£åœ¨ä½¿ç”¨ LangChain RecursiveCharacterTextSplitter åˆ†æ®µ ---")
    
    # åˆå§‹åŒ– LangChain åˆ†å‰²å™¨
    # chunk_size: æ¯ä¸ªå—çš„å­—ç¬¦æ•°
    # chunk_overlap: é‡å éƒ¨åˆ†ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡ä¸¢å¤±
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
    )
    
    final_chunks = []
    
    # éå†æ¯ä¸€é¡µè¿›è¡Œåˆ‡åˆ†
    # æ³¨æ„ï¼šLangChain é€šå¸¸å¤„ç†çº¯æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ page metadata å¸¦è¿›å»
    for page_data in raw_pages:
        page_num = page_data['page']
        content = page_data['text']
        
        # è°ƒç”¨ LangChain åˆ‡åˆ†
        # create_documents ä¼šè¿”å› Document å¯¹è±¡åˆ—è¡¨
        docs = text_splitter.create_documents([content])
        
        for i, doc in enumerate(docs):
            chunk_dict = {
                "id": str(uuid.uuid4()),
                "text": doc.page_content,
                "header_path": [], # çº¯æ–‡æœ¬æå–ä¸¢å¤±äº† Header ä¿¡æ¯ï¼Œè¿™é‡Œç•™ç©ºæˆ–åç»­è¡¥
                "metadata": {
                    "source": "doc.pdf",
                    "page": page_num,
                    "chunk_index_in_page": i,
                    "type": "text"
                },
                # é¢„ç•™å­—æ®µ
                "vector": None,
                "cluster_id": -1,
                "prev_chunk_id": None,
                "next_chunk_id": None
            }
            final_chunks.append(chunk_dict)
            
    # å»ºç«‹é“¾è¡¨å…³ç³» (Next/Prev)
    for i in range(len(final_chunks)):
        if i > 0:
            final_chunks[i]['prev_chunk_id'] = final_chunks[i-1]['id']
        if i < len(final_chunks) - 1:
            final_chunks[i]['next_chunk_id'] = final_chunks[i+1]['id']
            
    return final_chunks

# ==================================================
# 3. ZhipuAI å‘é‡åŒ– & FAISS èšç±»
# ==================================================
def process_embeddings_and_clusters(chunks):
    print("--- æ­£åœ¨è°ƒç”¨æ™ºè°± API ç”Ÿæˆå‘é‡ ---")
    
    if "ä½ çš„API_KEY" in ZHIPU_API_KEY:
        raise ValueError("è¯·å…ˆåœ¨ä»£ç é¡¶éƒ¨å¡«å…¥æ­£ç¡®çš„ ZHIPU_API_KEY")

    # åˆå§‹åŒ– LangChain çš„æ™ºè°± Embeddings
    embeddings_model = ZhipuAIEmbeddings(
        model="embedding-2", # æ™ºè°±ç›®å‰çš„é€šç”¨ Embedding æ¨¡å‹
        api_key=ZHIPU_API_KEY
    )
    
    texts = [c['text'] for c in chunks]
    vectors = []
    
    batch_size = 10 
    
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding Progress"):
        batch = texts[i : i + batch_size]
        try:
            batch_vecs = embeddings_model.embed_documents(batch)
            vectors.extend(batch_vecs)
        except Exception as e:
            print(f"Batch {i} failed: {e}")
            vectors.extend([ [0.0]*1024 for _ in range(len(batch)) ])

    # è½¬æ¢ä¸º numpy æ ¼å¼
    np_vectors = np.array(vectors).astype('float32')
    
    print("--- æ­£åœ¨è¿›è¡Œæœ¬åœ°èšç±» (FAISS) ---")
    # èšç±»æ•°ï¼šå‡è®¾æ¯ 30 ä¸ª Chunk æ˜¯ä¸€ä¸ªè¯é¢˜ç°‡
    num_clusters = max(int(len(chunks) / 30), 2)
    d = np_vectors.shape[1] # 1024
    
    # è®­ç»ƒ K-Means
    kmeans = faiss.Kmeans(d, num_clusters, niter=20, verbose=True)
    kmeans.train(np_vectors)
    
    # å¯»æ‰¾å½’å±
    D, I = kmeans.index.search(np_vectors, 1)
    cluster_ids = I.flatten().tolist()
    centroids = kmeans.centroids
    
    # å›å¡«
    for k, chunk in enumerate(chunks):
        chunk['vector'] = vectors[k]
        chunk['cluster_id'] = int(cluster_ids[k])
        
    return chunks, centroids

# ==================================================
# 4. ä¸»ç¨‹åº
# ==================================================
def main():
    ray.init(ignore_reinit_error=True)
    
    if not os.path.exists(PDF_PATH):
        print(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {PDF_PATH}")
        return

    doc = fitz.open(PDF_PATH)
    total_pages = len(doc)
    doc.close()
    
    print(f"æ–‡æ¡£å…± {total_pages} é¡µï¼Œå¯åŠ¨ Ray å¹¶è¡Œè§£æ...")
    
    # 1. Ray æå–æ–‡æœ¬
    num_actors = 8
    chunk_size = 50 # æ¯æ¬¡å¤„ç† 50 é¡µ
    
    actors = [PDFTextExtractor.remote() for _ in range(num_actors)]
    futures = []
    
    for i in range(0, total_pages, chunk_size):
        actor = actors[ (i // chunk_size) % num_actors ]
        futures.append(
            actor.extract_text.remote(PDF_PATH, i, min(i+chunk_size, total_pages))
        )
        
    results_nested = ray.get(futures)
    
    # å±•å¹³å¹¶æŒ‰é¡µç æ’åº
    all_pages = []
    for res in results_nested:
        all_pages.extend(res)
    all_pages.sort(key=lambda x: x['page'])
    
    print(f"æå–å®Œæˆï¼Œå…± {len(all_pages)} é¡µæœ‰æ•ˆæ–‡æœ¬")
    
    # 2. LangChain åˆ†æ®µ
    final_chunks = split_text_with_langchain(all_pages)
    print(f"LangChain åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(final_chunks)} ä¸ª Chunks")
    
    # 3. æ™ºè°± Embedding + èšç±»
    processed_chunks, centroids = process_embeddings_and_clusters(final_chunks)
    
    # 4. ä¿å­˜ç»“æœ
    output_file = "ready_for_db_zhipu.json"
    data = {
        "centroids": centroids.tolist(),
        "chunks": processed_chunks
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³ {output_file}")

if __name__ == "__main__":
    main()
```
ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†æ ¸å¿ƒæ•°æ®äº†ï¼Œæ¥ä¸‹æ¥çš„ä»»åŠ¡å°±æ˜¯æŠŠè¿™äº›æ•°æ®å„å½’å…¶ä½ã€‚

## åŒå†™å­˜åº“
è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼Œæˆ‘ä»¬å°†å®ç°ï¼š
1. Milvuså»ºè¡¨å†™å…¥ï¼šæŠŠè´¨å¿ƒå‘é‡å­˜è¿›å»ï¼Œç®€å†HNSWç´¢å¼•
2. ArangoDBå»ºå›¾ä¸å†™å…¥ï¼šæŠŠChunkå­˜ä¸ºèŠ‚ç‚¹ï¼ŒæŠŠNext_toå…³ç³»å­˜ä¸ºè¾¹

é»˜è®¤ä½ å·²ç»å®‰è£…å¥½äº†Milvuså’ŒAttuï¼Œè¯·å…ˆåœ¨docker-desktopå¯åŠ¨milvusï¼Œ
å¦‚æœä½ æ²¡æœ‰å®‰è£…ArangoDBï¼Œè¯·æ‰§è¡Œï¼š
```shell
docker run -d -p 8529:8529 -e ARANGO_ROOT_PASSWORD=pass123 --name arango swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/arangodb/arangodb:3.11
```

ç„¶åæ‰§è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼Œå®‰è£…æ‰€éœ€ä¾èµ–

```shell
pip install pymilvus python-arango
```
å…¨éƒ¨æ‰§è¡ŒæˆåŠŸåï¼Œè¯·æ‰§è¡Œä¸‹é¢çš„ä»£ç ï¼Œä¸‹é¢çš„ä»£ç ä¿è¯äº†å¹‚ç­‰æ€§ï¼šå¦‚æœè¡¨å·²å­˜åœ¨ä¼šå…ˆåˆ é™¤å†é‡å»ºï¼Œç¡®ä¿è°ƒè¯•çš„æ—¶å€™æ•°æ®ä¸ä¼šé‡å¤

```python
import json
import time
from pymilvus import (
    connections, FieldSchema, CollectionSchema, DataType, 
    Collection, utility
)
from arango import ArangoClient

# ==========================================
# é…ç½®åŒºåŸŸ
# ==========================================
JSON_FILE = "ready_for_db_zhipu.json"

# Milvus é…ç½®
MILVUS_HOST = "127.0.0.1"
MILVUS_PORT = "19530"
MILVUS_COLLECTION = "rag_cluster_centroids"
DIMENSION = 1024 

# ArangoDB é…ç½®
ARANGO_URL = "http://127.0.0.1:8529"
ARANGO_USER = "root"
ARANGO_PASS = "pass123" 
ARANGO_DB_NAME = "rag_db"
ARANGO_GRAPH_NAME = "rag_knowledge_graph" # å›¾åç§°

def load_json_data():
    print(f"æ­£åœ¨è¯»å– {JSON_FILE} ...")
    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {JSON_FILE}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        exit(1)

# ==========================================
# 1. Milvus å…¥åº“é€»è¾‘ (å­˜è·¯ç”±)
# ==========================================
def ingest_to_milvus(centroids):
    print("\n=== å¼€å§‹å†™å…¥ Milvus (è·¯ç”±å±‚) ===")
    
    # 1. è¿æ¥
    try:
        connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
    except Exception as e:
        print(f"Milvus è¿æ¥å¤±è´¥: {e}")
        return

    # 2. æ¸…ç†æ—§è¡¨
    if utility.has_collection(MILVUS_COLLECTION):
        utility.drop_collection(MILVUS_COLLECTION)
        print(f"å·²åˆ é™¤æ—§è¡¨: {MILVUS_COLLECTION}")
        
    # 3. å®šä¹‰ Schema
    fields = [
        FieldSchema(name="cluster_id", dtype=DataType.INT64, is_primary=True),
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)
    ]
    schema = CollectionSchema(fields, description="RAG Cluster Centroids")
    
    # 4. åˆ›å»ºé›†åˆ
    collection = Collection(name=MILVUS_COLLECTION, schema=schema)
    print("è¡¨ç»“æ„åˆ›å»ºæˆåŠŸ")
    
    # 5. æ’å…¥æ•°æ®
    ids = [i for i in range(len(centroids))]
    vectors = centroids
    
    if len(vectors) > 0 and len(vectors[0]) != DIMENSION:
        print(f"é”™è¯¯: å‘é‡ç»´åº¦ {len(vectors[0])} ä¸é…ç½® {DIMENSION} ä¸ç¬¦ï¼")
        return

    mr = collection.insert([ids, vectors])
    print(f"æ’å…¥è¯·æ±‚å·²æäº¤ï¼Œå—å½±å“è¡Œæ•°: {mr.insert_count}")
    
    print("æ­£åœ¨åˆ·ç›˜ (Flush)...")
    collection.flush()
    
    # 7. åˆ›å»ºç´¢å¼•
    index_params = {
        "metric_type": "COSINE", 
        "index_type": "HNSW",
        "params": {"M": 16, "efConstruction": 200}
    }
    print("æ­£åœ¨æ„å»ºç´¢å¼•...")
    collection.create_index(field_name="vector", index_params=index_params)
    utility.index_building_progress(MILVUS_COLLECTION)
    
    # 8. Load
    collection.load()
    print("Milvus Collection Loaded âœ…")

# ==========================================
# 2. ArangoDB å…¥åº“é€»è¾‘ (å­˜å†…å®¹å›¾è°±)
# ==========================================
def ingest_to_arango(chunks):
    print("\n=== å¼€å§‹å†™å…¥ ArangoDB (å†…å®¹å±‚) ===")
    
    # 1. è¿æ¥
    try:
        sys_client = ArangoClient(hosts=ARANGO_URL)
        sys_db = sys_client.db('_system', username=ARANGO_USER, password=ARANGO_PASS)
        
        # åˆ›å»ºæ•°æ®åº“
        if not sys_db.has_database(ARANGO_DB_NAME):
            sys_db.create_database(ARANGO_DB_NAME)
        
        db = sys_client.db(ARANGO_DB_NAME, username=ARANGO_USER, password=ARANGO_PASS)
    except Exception as e:
        print(f"ArangoDB è¿æ¥å¤±è´¥: {e}")
        return
    
    COL_CHUNKS = "rag_chunks"
    COL_RELATIONS = "rag_relations"
    
    # 2. æ¸…ç†ç¯å¢ƒ (åˆ é™¤æ—§çš„ Graph å®šä¹‰å’Œ Collections)
    # å…ˆåˆ  Graphï¼Œå†åˆ  Collectionï¼Œå¦åˆ™ä¼šæŠ¥é”™
    if db.has_graph(ARANGO_GRAPH_NAME):
        db.delete_graph(ARANGO_GRAPH_NAME)
    if db.has_collection(COL_RELATIONS): db.delete_collection(COL_RELATIONS)
    if db.has_collection(COL_CHUNKS): db.delete_collection(COL_CHUNKS)
    
    # 3. åˆ›å»º Collection
    chunks_col = db.create_collection(COL_CHUNKS)
    relations_col = db.create_collection(COL_RELATIONS, edge=True)
    
    # 4. å‡†å¤‡æ•°æ®
    batch_docs = []
    batch_edges = []
    
    print("æ­£åœ¨é¢„å¤„ç†æ•°æ®...")
    for chunk in chunks:
        # ç¡®ä¿ cluster_id å­˜åœ¨ï¼Œå¦åˆ™ç»™é»˜è®¤å€¼ -1
        cid = chunk.get("cluster_id", -1)
        if cid is None: cid = -1

        doc = {
            "_key": chunk["id"], 
            "text": chunk["text"],
            "cluster_id": int(cid),
            "page": chunk["metadata"].get("page", -1),
            "source": chunk["metadata"].get("source", "unknown")
        }
        batch_docs.append(doc)
        
        if chunk.get("next_chunk_id"):
            edge = {
                "_from": f"{COL_CHUNKS}/{chunk['id']}",
                "_to": f"{COL_CHUNKS}/{chunk['next_chunk_id']}",
                "type": "NEXT_TO"
            }
            batch_edges.append(edge)

    # 5. æ‰¹é‡å†™å…¥ 
    print(f"æ­£åœ¨æ‰¹é‡å†™å…¥ {len(batch_docs)} ä¸ª Chunk...")
    # on_duplicate="replace" ä¿è¯é‡å¤è¿è¡Œä¸æŠ¥é”™
    res_docs = chunks_col.import_bulk(batch_docs, on_duplicate="replace") 
    if res_docs['errors'] > 0:
        print(f"è­¦å‘Š: Chunk å†™å…¥å‡ºç° {res_docs['errors']} ä¸ªé”™è¯¯! è¯¦æƒ…: {res_docs['details']}")
    
    print(f"æ­£åœ¨æ‰¹é‡å†™å…¥ {len(batch_edges)} æ¡å…³ç³»...")
    res_edges = relations_col.import_bulk(batch_edges, on_duplicate="replace")
    if res_edges['errors'] > 0:
        print(f"è­¦å‘Š: å…³ç³»å†™å…¥å‡ºç° {res_edges['errors']} ä¸ªé”™è¯¯! (å¯èƒ½æ˜¯ next_id ä¸å­˜åœ¨)")

    # 6. åˆ›å»º Graph å®šä¹‰ (æ–¹ä¾¿åœ¨ Web UI æŸ¥çœ‹)
    print("æ­£åœ¨åˆ›å»ºå›¾è°±å®šä¹‰ (Graph Definition)...")
    edge_definitions = [
        {
            "edge_collection": COL_RELATIONS,
            "from_vertex_collections": [COL_CHUNKS],
            "to_vertex_collections": [COL_CHUNKS]
        }
    ]
    db.create_graph(ARANGO_GRAPH_NAME, edge_definitions=edge_definitions)
    
    # 7. åˆ›å»ºç´¢å¼•
    chunks_col.add_persistent_index(fields=["cluster_id"])
    print("ArangoDB å…¥åº“ & å›¾è°±æ„å»ºå®Œæˆ âœ…")

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
if __name__ == "__main__":
    data = load_json_data()
    centroids = data.get("centroids", [])
    chunks = data.get("chunks", [])
    
    if not centroids or not chunks:
        print("é”™è¯¯: JSON æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
        exit(1)

    print(f"æ•°æ®åŠ è½½å®Œæ¯•: {len(centroids)} ä¸ªè´¨å¿ƒ, {len(chunks)} ä¸ªæ–‡æœ¬å—")
    
    # æ‰§è¡Œå†™å…¥
    ingest_to_milvus(centroids)
    ingest_to_arango(chunks)
```

### æ€ä¹ˆéªŒè¯æˆåŠŸäº†ï¼Ÿ

1.  **ArangoDB**: æµè§ˆå™¨æ‰“å¼€ `http://127.0.0.1:8529`ï¼Œç™»å½• root/pass123ã€‚
    *   é€‰æ‹©æ•°æ®åº“ `rag_db`ã€‚

        ![alt](/images/arango_login.png)
        
    *   ç‚¹å·¦ä¾§ **COLLECTIONS** -> `rag_chunks`ã€‚ä½ åº”è¯¥èƒ½çœ‹åˆ°æ‰€æœ‰çš„æ–‡æœ¬å—ã€‚

        ![alt](/images/arango_rag_chunks.png)

    *   ç‚¹ **GRAPHS** -> Create Graph -> é€‰åˆšæ‰é‚£ä¸¤ä¸ªè¡¨ -> éšä¾¿ç‚¹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œçœ‹çœ‹èƒ½ä¸èƒ½çœ‹åˆ°è¿çº¿ï¼ˆNEXT_TOï¼‰ã€‚
2.  **Milvus**: ä½¿ç”¨ `Attu` (Milvus çš„å¯è§†åŒ–å·¥å…·) æˆ–è€…å•çº¯çœ‹ Python è„šæœ¬æœ€åæœ‰æ²¡æœ‰æ‰“å° `Milvus Collection Loaded âœ…`ã€‚


## æ¢³ç†

ä¸‹é¢å°±å¯ä»¥è¿›è¡Œæœç´¢äº†ï¼Œä½†åœ¨è¿›è¡Œæœç´¢ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ¢³ç†ä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾è®¡ï¼Œè®¾è®¡çš„ç»†èŠ‚ä½ æ˜¯å¦ç†è§£ï¼Œå­˜å‚¨çš„æ—¶å€™ï¼Œmilvuså’Œarangodbæ˜¯å¦‚ä½•å…³è”ä¸Šçš„ï¼Œå­˜å‚¨çš„æµç¨‹æ˜¯æ€ä¹ˆæ ·çš„ï¼Œå…ˆmilvuså†arangodbå—ï¼Œç„¶åå­˜å‚¨çš„æ—¶å€™ï¼Œè¿™æ ·è®¾è®¡æ˜¯ä¸ºäº†æ–¹ä¾¿æœç´¢å—ï¼Œè§£å†³äº†ä»€ä¹ˆé—®é¢˜


æˆ‘ä»¬ç°åœ¨åœä¸‹æ¥ï¼Œä¸å†™ä»£ç ï¼Œä¸“é—¨æŠŠ â€œä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾è®¡â€ å’Œ â€œå®ƒä»¬æ˜¯æ€ä¹ˆå…³è”çš„â€ è¿™ä¸¤ä¸ªé—®é¢˜å½»åº•æ‹†è§£æ¸…æ¥šã€‚

**1. æ ¸å¿ƒç–‘é—®ï¼šMilvuså’ŒArangoDBæ˜¯å¦‚ä½•å…³è”ä¸Šçš„ï¼Ÿ**
ç­”æ¡ˆï¼šé€šè¿‡cluster_idå…³è”çš„ã€‚


åœ¨å­˜å‚¨çš„æ—¶å€™ï¼š
* æˆ‘ä»¬ç®—å‡ºè¿™ä¸ªNä¸ªChunkå¯ä»¥åˆ†ä¸ºkä¸ªç±»ï¼ˆèšç±»ï¼‰ã€‚
* æˆ‘ä»¬ç»™å…¶ä¸­çš„æŸä¸ªç±»åˆ†é…äº†ä¸€ä¸ªå·ç ç‰Œæ¯”å¦‚ï¼šcluster_id = 101
* æˆ‘ä»¬å‘Šè¯‰**Milvus**ï¼šå·ç ç‰Œ101çš„ç‰¹å¾æ˜¯[0.1,0.9,0.3...]ï¼ˆèšç±»ä¸­å¿ƒçš„è´¨å¿ƒå‘é‡ï¼‰
* æˆ‘ä»¬å‘Šè¯‰**ArangoDB**ï¼šè¿™äº›chunkéƒ½æ˜¯å±äº101å·çš„ï¼Œç»™ä»–ä»¬è´´ä¸Šcluster_id:101çš„æ ‡ç­¾

åœ¨æŸ¥è¯¢çš„æ—¶å€™ï¼š
* ç”¨æˆ·é—®æŸä¸ªç±»çš„åç§°
* Milvusç®—äº†ä¸€ä¸‹ï¼Œè¯´ï¼šå»101å·æ‰¾
* ä½ èµ°åˆ°äº†ArangoDBï¼Œå–Šäº†ä¸€å£°ï¼šé‚£äº›chunkæ˜¯å±äº101å·çš„ï¼Œç«™å‡ºæ¥ï¼

ç»“è®ºï¼šMilvuså’ŒArangoDBé€šè¿‡èšç±»idç›¸å…³è”ï¼ŒMilvuså­˜ID->å‘é‡ï¼ŒArangoDBå­˜ID->æ­£æ–‡

**2. å­˜å‚¨æµç¨‹ï¼šä¸ºä»€ä¹ˆæ˜¯å…ˆç®—åå­˜ï¼Ÿ**

å¯èƒ½æœ‰ç§ç–‘æƒ‘ï¼šå…ˆMilvuså†ArangoDBå—ï¼Ÿ

å…¶å®æ›´å‡†ç¡®çš„è¯´æ³•æ˜¯ï¼šå…ˆå†…å­˜è®¡ç®—ï¼Œå†åŒå†™åˆ†å‘ã€‚

æµç¨‹å¤ç›˜ï¼š
* åœ¨Pythonå†…å­˜é‡Œï¼šè¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼Œæ•°æ®è¿˜æ²¡æœ‰è¿›å…¥æ•°æ®åº“ä¹‹å‰ï¼Œå·²ç»åœ¨Pythonä¸­è¿›è¡Œäº†å‘é‡åŒ–å’Œèšç±»
    * æ­¤æ—¶ï¼Œæ¯ä¸ªchunkå¯¹è±¡åœ¨å†…å­˜ä¸­å·²ç»æœ‰äº†cluster_idè¿™ä¸ªå±æ€§
* åˆ†å‘ï¼š
    * åªè¦å†…å­˜é‡Œçš„æ•°æ®æœ‰äº†cluster_idï¼Œé‚£å…ˆå†™å…¥è°å¹¶ä¸é‡è¦ã€‚
    * ä¸ºäº†ä»£ç é€»è¾‘æ¸…æ™°ï¼Œé€šå¸¸å¹¶è¡Œå†™å…¥ï¼Œæˆ–è€…å…ˆå†™å…¥Milvuså»ºç«‹ç›®å½•ï¼Œç„¶åå†å†™å…¥ArangoDBã€‚

**3. è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

æ—¢ç„¶æœ‰Milvusäº†ï¼Œä¸ºä»€ä¹ˆä¸æŠŠtextç›´æ¥å­˜åœ¨Milvusçš„payloadä¸­ï¼Œä¸ºä»€ä¹ˆè¦æä¿©æ•°æ®åº“ï¼Ÿ

é—®é¢˜1ï¼šæ˜¾å­˜å¤ªè´µï¼ŒHNSWç´¢å¼•å¤ªå ç”¨å†…å­˜
* ä¼ ç»Ÿæ–¹æ³•ï¼šæŠŠ1000ä¸‡æ¡Chunkçš„å‘é‡å…¨éƒ¨å»ºç«‹HNSWç´¢å¼•ã€‚
    * åæœï¼šHNSWç´¢å¼•éœ€è¦æŠŠå‘é‡åŠ è½½åˆ°å†…å­˜ï¼Œ1000ä¸‡æ¡ x 1024ç»´ x 4å­—èŠ‚ â‰ˆ 40GBå†…å­˜ã€‚
* æˆ‘ä»¬çš„åšæ³•ï¼šMilvusåªå­˜1ä¸‡ä¸ªèšç±»è´¨å¿ƒ
    * åæœï¼š1ä¸‡æ¡å‘é‡ â‰ˆ 40MBå†…å­˜
    * ä¼˜åŠ¿ï¼šå†…å­˜å ç”¨é™ä½1000å€

é—®é¢˜2ï¼šè¯­ä¹‰æ£€ç´¢çš„è¿‘è§†çœ¼é—®é¢˜
* ä¼ ç»Ÿåšæ³•ï¼šMilvusæœå‡ºæ¥ä¸€å¥è¯ï¼šå‡€åˆ©æ¶¦å¢é•¿äº†10%
    * åæœï¼šLLMæ‹¿åˆ°è¿™å¥è¯æ˜¯æ‡µçš„ã€‚è°çš„å‡€åˆ©æ¶¦ï¼Ÿæ˜¯é›†å›¢çš„è¿˜æ˜¯å­å…¬å¸çš„ï¼Ÿæ˜¯2023å¹´çš„è¿˜æ˜¯2022å¹´çš„ï¼Ÿ
* æˆ‘ä»¬çš„åšæ³•ï¼šArangoDBå­˜å‚¨äº†NEXT_TOå’ŒPARENT_OFå…³ç³»ã€‚
    * ä¼˜åŠ¿ï¼šåœ¨æŸ¥è¯¢åˆ°è¿™å¥è¯çš„åŒæ—¶ï¼Œé¡ºç€å›¾å…³ç³»ï¼ŒæŠŠå®ƒçš„çˆ¶æ ‡é¢˜ï¼ˆåä¸œåˆ†å…¬å¸ï¼‰å’Œå‰ä¸€æ®µï¼ˆ2023å¹´è´¢æŠ¥æ‘˜è¦ï¼‰ä¸€èµ·æŠ“å‡ºæ¥ã€‚LLMç¬é—´çœ‹æ‡‚äº†ï¼ŒMilvusåšä¸åˆ°è¿™ç§å›¾éå†æŸ¥è¯¢

é—®é¢˜3ï¼šæœç´¢çš„æ¼æ–—æ•ˆåº”
* ä¼ ç»Ÿåšæ³•ï¼šTop-Kæœç´¢ï¼ˆæ¯”å¦‚K= 5ï¼‰
    * åæœï¼šå¦‚æœç›¸å…³å†…å®¹æœ‰20æ®µï¼Œä½ åªå¬å›äº†5æ®µï¼Œå‰©ä¸‹çš„15æ®µä¸¢äº†ï¼Œè¿™å«ä½å¬å›ç‡ã€‚
* æˆ‘ä»¬çš„åšæ³•ï¼šå…ˆæ‰¾ç›¸å…³çš„è¯é¢˜ç°‡
    * ä¼˜åŠ¿ï¼šæ¯”å¦‚å‘½ä¸­è´¢åŠ¡ç°‡ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªç°‡é‡Œçš„50ä¸ªChunkå…¨éƒ¨æ‹¿å‡ºæ¥
    * ç»“æœï¼šå¬å›èŒƒå›´æ‰©å¤§äº†ï¼ˆRecallæå‡ï¼‰ï¼Œç„¶åå†ç”¨Rerankæ¨¡å‹ç²¾æŒ‘ç»†é€‰ï¼Œè¿™æ˜¯ä¸€ç§å¹¿æ’’ç½‘ï¼Œç²¾æ•æçš„ç­–ç•¥ï¼Œé€‚åˆé•¿æ–‡æ¡£é—®ç­”ã€‚

## æ£€ç´¢

è¿™é‡Œæˆ‘ä»¬ä¼šç”¨åˆ°BGE-Reranker-Baseé‡æ’åºæ¨¡å‹
* å¤§å°ï¼šæ¨¡å‹æ–‡ä»¶åªæœ‰1GB
* æ•ˆæœï¼šæ˜¯ç›®å‰å¼€æºç•Œæ€§ä»·æ¯”æœ€é«˜çš„é‡æ’åºæ¨¡å‹ä¹‹ä¸€
* æ¥æºï¼šæˆ‘ä»¬è¿™é‡Œç”¨modelscopeæ‹‰å–


```shell
pip install modelscope
```

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from modelscope import snapshot_download
import numpy as np
from pymilvus import connections, Collection
from arango import ArangoClient
from langchain_community.embeddings import ZhipuAIEmbeddings

# ==========================================
# é…ç½®åŒºåŸŸ
# ==========================================
ZHIPU_API_KEY = ""

# Milvus & ArangoDB (ä¿æŒä¸å˜)
MILVUS_HOST = "127.0.0.1"
MILVUS_PORT = "19530"
MILVUS_COLLECTION = "rag_cluster_centroids"
ARANGO_URL = "http://127.0.0.1:8529"
ARANGO_USER = "root"
ARANGO_PASS = "pass123"
ARANGO_DB_NAME = "rag_db"
COL_CHUNKS = "rag_chunks"

# æœç´¢å‚æ•°
TOP_K_CLUSTERS = 3
TOP_K_FINAL = 5

# ==========================================
# 1. åˆå§‹åŒ–èµ„æº (æ–°å¢ Reranker åŠ è½½)
# ==========================================
class RerankerEngine:
    def __init__(self):
        print("æ­£åœ¨ä¸‹è½½/åŠ è½½ BGE-Reranker æ¨¡å‹ (çº¦1GB)...")
        # ä» ModelScope ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜
        model_dir = snapshot_download('Xorbits/bge-reranker-base', revision='master')
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)
        self.model.eval() # è¯„ä¼°æ¨¡å¼
        
        # å¦‚æœæœ‰Nå¡å°±ç”¨cudaï¼Œæ²¡æœ‰å°±cpu
        self.device = 'cpu' 
        self.model.to(self.device)
        print("Reranker æ¨¡å‹åŠ è½½å®Œæˆ âœ…")

    def compute_score(self, query, candidates):
        """
        è®¡ç®— (query, text) å¯¹çš„ç›¸å…³æ€§åˆ†æ•°
        """
        pairs = [[query, doc['text'][:512]] for doc in candidates] # æˆªæ–­ä¸€ä¸‹é˜²æ­¢çˆ†æ˜¾å­˜
        
        with torch.no_grad():
            inputs = self.tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            scores = self.model(**inputs, return_dict=True).logits.view(-1,).float()
            
        return scores.cpu().numpy()

def init_resources():
    # 1. Milvus & Arango 
    connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
    milvus_col = Collection(MILVUS_COLLECTION)
    milvus_col.load()
    
    client = ArangoClient(hosts=ARANGO_URL)
    db = client.db(ARANGO_DB_NAME, username=ARANGO_USER, password=ARANGO_PASS)
    
    # 2. Embedding
    embed_model = ZhipuAIEmbeddings(model="embedding-2", api_key=ZHIPU_API_KEY)
    
    # 3. Reranker
    reranker = RerankerEngine()
    
    return milvus_col, db, embed_model, reranker

# ==========================================
# 2. èåˆæ£€ç´¢ (å¸¦ Rerank)
# ==========================================
def fusion_search(query: str, milvus_col, db, embed_model, reranker):
    print(f"\nğŸ” ç”¨æˆ·æé—®: ã€{query}ã€‘")
    
    # --- Step 1 & 2: ç²—æ’ (Routing) ---
    print(">>> 1. Milvus è·¯ç”±...")
    q_vec = embed_model.embed_query(query)
    
    res = milvus_col.search(
        data=[q_vec], anns_field="vector", 
        param={"metric_type": "COSINE", "params": {"ef": 64}}, 
        limit=TOP_K_CLUSTERS, output_fields=["cluster_id"]
    )
    target_ids = [hit.id for hit in res[0]]
    
    # --- Step 3: å¬å› (Retrieval) ---
    print(f">>> 2. ArangoDB å¬å› (Cluster IDs: {target_ids})...")
    aql = f"""
    FOR c IN {COL_CHUNKS}
        FILTER c.cluster_id IN @ids
        RETURN {{ id: c._key, text: c.text, cluster_id: c.cluster_id }}
    """
    candidates = list(db.aql.execute(aql, bind_vars={"ids": target_ids}))
    print(f"å€™é€‰é›†æ•°é‡: {len(candidates)}")
    
    if not candidates: return []

    # --- Step 4: ç²¾æ’ (Rerank) ---
    print(f">>> 3. BGE-Reranker ç²¾æ’ä¸­...")
    scores = reranker.compute_score(query, candidates)
    
    # æŠŠåˆ†æ•°å†™å›å»
    for i, doc in enumerate(candidates):
        doc['score'] = float(scores[i])
        
    # æŒ‰åˆ†æ•°é™åº
    candidates.sort(key=lambda x: x['score'], reverse=True)
    final_results = candidates[:TOP_K_FINAL]

    # --- Step 5: ä¸Šä¸‹æ–‡æ‰©å±• ---
    print(f">>> 4. å›¾è°±ä¸Šä¸‹æ–‡æ‰©å±•...")
    top_doc = final_results[0]
    context_aql = """
    FOR v, e, p IN 1..1 OUTBOUND @start_node rag_relations
        FILTER e.type == 'NEXT_TO'
        RETURN v.text
    """
    start_node_id = f"{COL_CHUNKS}/{top_doc['id']}" 
    
    next_texts = list(db.aql.execute(context_aql, bind_vars={"start_node": start_node_id}))
    top_doc['next_text'] = next_texts[0] if next_texts else "(æ— åæ–‡)"

    return final_results

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
if __name__ == "__main__":
    col, db, embed, rerank = init_resources()
    
    while True:
        q = input("\nè¯·è¾“å…¥é—®é¢˜ (qé€€å‡º): ").strip()
        if q == 'q': break
        if not q: continue
        
        try:
            results = fusion_search(q, col, db, embed, rerank)
            
            print("\n" + "="*60)
            for i, res in enumerate(results):
                print(f"Rank {i+1} | Score: {res['score']:.4f} | Cluster: {res['cluster_id']}")
                print(f"Content: {res['text'][:100]}...") 
                if i == 0:
                    print(f"Context+: {res.get('next_text', '')[:100]}...")
                print("-" * 30)
                
        except Exception as e:
            print(f"Error: {e}")
```

```python 
import os
from zhipuai import ZhipuAI

# ==========================================
# é…ç½®
# ==========================================
ZHIPU_API_KEY = ""

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªåŸºäºå›¾è°±å¢å¼ºçš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„ã€å‚è€ƒä¸Šä¸‹æ–‡ã€‘å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è¯·æ³¨æ„ï¼š
1. ä»…æ ¹æ®å‚è€ƒä¿¡æ¯å›ç­”ï¼Œä¸è¦ç¼–é€ ã€‚
2. å¦‚æœå‚è€ƒä¿¡æ¯é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´â€œæ ¹æ®ç°æœ‰æ–‡æ¡£æ— æ³•å›ç­”â€ã€‚
3. å›ç­”è¦æ¡ç†æ¸…æ™°ï¼Œå¦‚æœå‚è€ƒäº†ä¸Šä¸‹æ–‡çš„æ‰©å±•å†…å®¹ï¼Œè¯·è‡ªç„¶åœ°èåˆåœ¨å›ç­”ä¸­ã€‚
"""

# ==========================================
# RAG ç”Ÿæˆé€»è¾‘
# ==========================================
def generate_answer(client, query, search_results):
    """
    ç»„è£… Prompt å¹¶è°ƒç”¨ GLM-4 ç”Ÿæˆå›ç­”
    """
    if not search_results:
        return "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"

    # 1. æ„å»ºä¸Šä¸‹æ–‡ (Context)
    # æˆ‘ä»¬å– Top-3 çš„ç»“æœï¼Œå¹¶æŠŠ Top-1 çš„æ‰©å±•ä¸Šä¸‹æ–‡ä¹Ÿæ‹¼è¿›å»
    context_str = ""
    
    for i, res in enumerate(search_results[:3]):
        context_str += f"--- å‚è€ƒç‰‡æ®µ {i+1} ---\n"
        context_str += f"{res['text']}\n"
        
        # å¦‚æœæ˜¯ Top-1 ä¸”æœ‰æ‰©å±•å†…å®¹ï¼ŒåŠ ä¸Šå»
        if i == 0 and res.get('next_text') and res['next_text'] != "(æ— åæ–‡)":
            context_str += f"[åç»­å†…å®¹]: {res['next_text']}\n"
    
    # 2. ç»„è£…æœ€ç»ˆ Prompt
    user_prompt = f"""
    ç”¨æˆ·é—®é¢˜: {query}
    
    ã€å‚è€ƒä¸Šä¸‹æ–‡ã€‘:
    {context_str}
    
    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    """
    
    # 3. è°ƒç”¨å¤§æ¨¡å‹ (GLM-4)
    print(">>> 5. æ­£åœ¨æ€è€ƒå¹¶ç”Ÿæˆå›ç­” (GLM-4)...")
    response = client.chat.completions.create(
        model="glm-4",  # æˆ–è€… glm-4-flash (æ›´ä¾¿å®œ)
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        stream=True, #ä»¥æ­¤è·å¾—æ‰“å­—æœºæ•ˆæœ
    )
    
    # 4. æµå¼è¾“å‡º
    print("\n" + "="*20 + " AI å›ç­” " + "="*20)
    full_answer = ""
    for chunk in response:
        content = chunk.choices[0].delta.content
        print(content, end="", flush=True)
        full_answer += content
    print("\n" + "="*50)
    
    return full_answer

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–æ£€ç´¢å™¨ (Milvus, Arango, Reranker)
    # æ³¨æ„ï¼šè¿™æ­¥æ¯”è¾ƒæ…¢ï¼Œå› ä¸ºè¦åŠ è½½ Reranker æ¨¡å‹ï¼Œè€å¿ƒç­‰å¾…
    print("æ­£åœ¨å¯åŠ¨ RAG ç³»ç»Ÿï¼ŒåŠ è½½æ¨¡å‹ä¸­...")
    milvus_col, db, embed_model, reranker = init_resources()
    
    # 2. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    llm_client = ZhipuAI(api_key=ZHIPU_API_KEY)
    
    print("\n RAG ç³»ç»Ÿå·²å°±ç»ªï¼(è¾“å…¥ 'q' é€€å‡º)")
    
    while True:
        query = input("\n è¯·è¾“å…¥é—®é¢˜: ").strip()
        if query.lower() == 'q': break
        if not query: continue
        
        try:
            # Step A: æ£€ç´¢ (Retrieval)
            # å¤ç”¨ä¹‹å‰å†™å¥½çš„èåˆæ£€ç´¢å‡½æ•°
            results = fusion_search(query, milvus_col, db, embed_model, reranker)
            
            # Step B: ç”Ÿæˆ (Generation)
            generate_answer(llm_client, query, results)
            
        except Exception as e:
            print(f" å‘ç”Ÿé”™è¯¯: {e}")
```

## å¯è§†åŒ–ç•Œé¢

è¿è¡Œgradioç¨‹åºï¼Œå‡ºç°äº†é—®é¢˜ï¼Œè¯·ç¬¬ä¸€æ—¶é—´æ€€ç–‘gradioçš„ç‰ˆæœ¬å’Œpythonç‰ˆæœ¬å†²çª

```shell
pip install gradio==4.44.1
pip install --upgrade gradio gradio_client pydantic fastapi uvicorn
```
```python
import os
import gradio as gr
from zhipuai import ZhipuAI

# 1. è®¾ç½®ç¯å¢ƒå˜é‡ (é˜²æ­¢ä»£ç†æ‹¦æˆª localhost)
os.environ["NO_PROXY"] = "localhost,127.0.0.1,::1"

# 2. å°è¯•å¯¼å…¥æ£€ç´¢æ¨¡å—
try:
    print(">>> æ­£åœ¨åŠ è½½æ¨¡å‹å’Œæ•°æ®åº“...")
    milvus_col, db, embed_model, reranker = init_resources()
    print(" èµ„æºåŠ è½½æˆåŠŸ")
except ImportError:
    milvus_col = db = embed_model = reranker = None
except Exception as e:
    print(f" èµ„æºåŠ è½½å‡ºé”™: {e}")
    milvus_col = db = embed_model = reranker = None

# 3. é…ç½® API
ZHIPU_API_KEY = ""
llm_client = ZhipuAI(api_key=ZHIPU_API_KEY)

# 4. æ ¸å¿ƒé€»è¾‘å‡½æ•° 
def chat_function(message, history):
    """
    history ç°åœ¨çš„æ ¼å¼æ˜¯: 
    [{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': 'hello'}]
    """
    if not message:
        yield history, ""
        return
    
    # è¿½åŠ ç”¨æˆ·æ¶ˆæ¯ (å­—å…¸æ ¼å¼)
    history.append({"role": "user", "content": message})
    yield history, "æ­£åœ¨æ€è€ƒ..."
    
    log_content = f"ğŸ” ç”¨æˆ·æé—®: {message}\n" + "-"*30 + "\n"
    
    try:
        # A. æ£€ç´¢
        results = []
        if milvus_col:
            log_content += ">>> Fusion Search...\n"
            results = fusion_search(message, milvus_col, db, embed_model, reranker)
            
        if not results:
            log_content += "âš ï¸ æœªå¬å›ç›¸å…³å†…å®¹ã€‚\n"
            context_str = ""
        else:
            # è®°å½• Top-1
            top1 = results[0]
            log_content += f"âœ… Top-1 Score: {top1.get('score', 0):.4f} | Cluster: {top1['cluster_id']}\n"
            log_content += f"ğŸ“„ ç‰‡æ®µ: {top1['text'][:50]}...\n"
            if top1.get('next_text'):
                log_content += f"ğŸ”— æ‰©å±•: {top1['next_text'][:30]}...\n"
            
            # æ„å»º Context
            context_str = ""
            for i, res in enumerate(results[:3]):
                context_str += f"å‚è€ƒ {i+1}: {res['text']}\n"
                if i == 0 and res.get('next_text'):
                    context_str += f"[ä¸‹æ–‡]: {res['next_text']}\n"

        # B. ç”Ÿæˆ
        if context_str:
            user_prompt = f"ç”¨æˆ·é—®é¢˜: {message}\n\nã€å‚è€ƒèµ„æ–™ã€‘:\n{context_str}\n\nè¯·æ ¹æ®èµ„æ–™å›ç­”ã€‚"
        else:
            user_prompt = message # æ²¡æŸ¥åˆ°å°±ç›´æ¥é—®

        log_content += ">>> GLM-4 Generating...\n"
        
        # è¿½åŠ ä¸€ä¸ªç©ºçš„ AI æ¶ˆæ¯å ä½ç¬¦
        history.append({"role": "assistant", "content": ""})
        yield history, log_content
        
        response = llm_client.chat.completions.create(
            model="glm-4",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªRAGåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": user_prompt},
            ],
            stream=True,
        )
        
        partial_answer = ""
        for chunk in response:
            if chunk.choices[0].delta.content:
                partial_answer += chunk.choices[0].delta.content
                # æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
                history[-1]["content"] = partial_answer
                yield history, log_content

    except Exception as e:
        error_msg = f"âŒ Error: {str(e)}"
        # å‡ºé”™ä¹Ÿæ›´æ–°è¿›ç•Œé¢
        if history[-1]["role"] == "assistant":
            history[-1]["content"] = error_msg
        else:
            history.append({"role": "assistant", "content": error_msg})
        yield history, log_content + "\n" + error_msg

# 5. æ„å»ºç•Œé¢å¯¹è±¡
with gr.Blocks(title="FusionGraph RAG", theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ğŸ§  FusionGraph RAG")
    
    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=600, label="å¯¹è¯")
            msg = gr.Textbox(label="é—®é¢˜", placeholder="è¯·è¾“å…¥...")
            clear = gr.Button("æ¸…ç©º")

        with gr.Column(scale=1):
            log_box = gr.TextArea(label="æ€ç»´é“¾æ—¥å¿—", lines=25, interactive=False)

    # ç»‘å®šäº‹ä»¶
    msg.submit(chat_function, [msg, chatbot], [chatbot, log_box])
    # æ¸…ç©ºæ—¶è¿”å›ç©ºåˆ—è¡¨
    clear.click(lambda: [], None, chatbot, queue=False)

# 6. å¯åŠ¨
if __name__ == "__main__":
    print("å¯åŠ¨ä¸­...")
    demo.queue().launch(
        server_name="127.0.0.1", 
        server_port=9200, 
        share=False,
        inbrowser=True
    )
```