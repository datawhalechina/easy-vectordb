{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa91d7ee",
   "metadata": {},
   "source": [
    "# Âü∫‰∫éMilvusÂíåArangoDBÁöÑRAGÁ≥ªÁªü\n",
    "\n",
    "ÂæàÂ§öÂ≠¶‰π†ËÄÖÊ≤°Êúâ‰∫ÜËß£ËøáArangoDBËøô‰∏™Êï∞ÊçÆÂ∫ìÔºå‰∏ãÈù¢Ôºå‰Ω†ÂèØ‰ª•ÈÄöËøáËØ•ÈÉ®ÂàÜÁ≥ªÁªüÊÄßÁöÑ‰∫ÜËß£Ëøô‰∏™Êï∞ÊçÆÂ∫ìÔºå‰πüÂèØ‰ª•Ë∑≥ËøáËøôÈÉ®ÂàÜÁõ¥Êé•ÁúãËÆæËÆ°ÊÄùÊÉ≥ÈÉ®ÂàÜ„ÄÇ\n",
    "\n",
    "## 1. ‰ªÄ‰πàÊòØ ArangoDBÔºü(Core Concept)\n",
    "\n",
    "Âú®‰º†ÁªüÊû∂ÊûÑ‰∏≠ÔºåÊàë‰ª¨ÈÄöÂ∏∏ÈúÄË¶Å‰∏Ä‰∏™ **MongoDB** Â≠òÊñáÊ°£ÔºåÂÜçÁî®‰∏Ä‰∏™ **Neo4j** Â≠òÂõæÂÖ≥Á≥ª„ÄÇ\n",
    "**ArangoDB** ÁöÑÊ†∏ÂøÉÁêÜÂøµÊòØ **Multi-ModelÔºàÂ§öÊ®°ÊÄÅÔºâ**Ôºö\n",
    "> **‚ÄúOne Engine, One Query Language, Multiple Data Models‚Äù**\n",
    "\n",
    "ÂÆÉÂú®‰∏Ä‰∏™Êï∞ÊçÆÂ∫ìÂºïÊìé‰∏≠ÂêåÊó∂ÊîØÊåÅÔºö\n",
    "1.  **ÊñáÊ°£ (Documents)**ÔºöÂÉè MongoDB ‰∏ÄÊ†∑Â≠òÂÇ® JSON Êï∞ÊçÆÔºàÊàë‰ª¨ÁöÑ Chunks Ê≠£ÊñáÔºâ„ÄÇ\n",
    "2.  **Âõæ (Graphs)**ÔºöÂÉè Neo4j ‰∏ÄÊ†∑Â≠òÂÇ®ËäÇÁÇπÂÖ≥Á≥ªÔºàÊàë‰ª¨ÁöÑ Context ‰∏ä‰∏ãÊñáÔºâ„ÄÇ\n",
    "3.  **ÈîÆÂÄº (Key-Value)**ÔºöÂÉè Redis ‰∏ÄÊ†∑Âø´ÈÄüËØªÂÜôÔºàÊàë‰ª¨ÁöÑ CacheÔºâ„ÄÇ\n",
    "\n",
    "### ‰∏∫‰ªÄ‰πàÊú¨È°πÁõÆÁöÑ RAG Á≥ªÁªüÈÄâÊã© ArangoDBÔºü\n",
    "*   **Â≠òÁÆóÂàÜÁ¶ª**ÔºöMilvus ËøôÁßçÊòÇË¥µÁöÑÊòæÂ≠òËµÑÊ∫êÂè™Â≠ò‚ÄúÁ¥¢ÂºïÂêëÈáè‚ÄùÔºåËÄåÊµ∑ÈáèÁöÑ‚ÄúÊñáÊú¨ËÇâË∫´‚ÄùÈúÄË¶Å‰∏Ä‰∏™ÊîØÊåÅÂÄíÊéíÁ¥¢ÂºïÁöÑÊï∞ÊçÆÂ∫ìÊù•Â≠òÔºåArangoDB ÁöÑÊñáÊ°£Â≠òÂÇ®ÈùûÂ∏∏ÈÄÇÂêà„ÄÇ\n",
    "*   **‰∏ä‰∏ãÊñáÂè¨Âõû**ÔºöÂΩì RAG Ê£ÄÁ¥¢Âà∞‰∏ÄÊÆµËØùÊó∂ÔºåÊàë‰ª¨ÈúÄË¶ÅÊØ´ÁßíÁ∫ßÊâæÂõûÂÆÉÁöÑ**‚Äú‰∏ä‰∏ÄÊÆµËØù‚Äù**Êàñ**‚ÄúÁà∂Ê†áÈ¢ò‚Äù**„ÄÇ‰º†ÁªüÊï∞ÊçÆÂ∫ìÂÅöÂÖ≥ËÅîÊü•ËØ¢ÔºàJoinÔºâÂæàÊÖ¢ÔºåËÄå ArangoDB ÁöÑ**ÂéüÁîüÂõæÈÅçÂéÜ (Graph Traversal)** ÊûÅÂø´„ÄÇ\n",
    "*   **Áªü‰∏ÄÊü•ËØ¢ (AQL)**ÔºöÊàë‰ª¨ÂèØ‰ª•Áî®Á±ª‰ºº SQL ÁöÑËØ≠Ê≥ïÔºàAQLÔºâÂêåÊó∂ÂÆåÊàê‚ÄúËøáÊª§ Cluster ID‚ÄùÂíå‚ÄúÊü•ÊâæÂõæÈÇªÂ±Ö‚Äù‰∏§‰∏™Êìç‰Ωú„ÄÇ\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Ê†∏ÂøÉÊúØËØ≠ (Key Terminology)\n",
    "\n",
    "Âú®‰ª£Á†Å‰∏≠‰Ω†‰ºöÈ¢ëÁπÅÈÅáÂà∞‰ª•‰∏ã‰∏â‰∏™Ê¶ÇÂøµÔºåËØ∑Âä°ÂøÖÂàÜÊ∏ÖÔºö\n",
    "\n",
    "| ÊúØËØ≠ | ÂØπÂ∫îÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ì | Êàë‰ª¨ÁöÑÈ°πÁõÆÁî®ÈÄî | Á§∫‰æã |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Collection** | Ë°® (Table) | Â≠òÊîæÊï∞ÊçÆÁöÑÂÆπÂô® | `rag_chunks` |\n",
    "| **Document** | Ë°å (Row) | ÂÆûÈôÖÁöÑÊï∞ÊçÆËÆ∞ÂΩï (JSON) | `{ \"text\": \"...\", \"cluster_id\": 101 }` |\n",
    "| **Edge** | ÂÖ≥ËÅîË°® (Join Table) | **ËøûÊé•Á∫ø**ÔºåÁâπÊÆäÁöÑ DocumentÔºåÂøÖÈ°ªÂåÖÂê´ `_from` Âíå `_to` | `{ \"_from\": \"Chunk/A\", \"_to\": \"Chunk/B\", \"type\": \"NEXT_TO\" }` |\n",
    "| **Graph** | ËßÜÂõæ (View) | ÂÆö‰πâÂì™‰∫õ Collection Âíå Edge ÁªÑÊàê‰∏ÄÂº†ÁΩë | `rag_knowledge_graph` |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Âü∫Á°Ä‰ΩøÁî®ÊñπÊ≥ï\n",
    "\n",
    "Êú¨È°πÁõÆ‰ΩøÁî® `python-arango` È©±Âä®„ÄÇ‰ª•‰∏ãÊòØ‰Ω†Âú®È°πÁõÆ‰∏≠ÂøÖÈ°ªÊéåÊè°ÁöÑ **CRUD** Âíå **ÂõæÊìç‰Ωú** Ê®°Êùø„ÄÇ\n",
    "\n",
    "### 3.1 ËøûÊé•‰∏éÂàùÂßãÂåñ\n",
    "\n",
    "```python\n",
    "from arango import ArangoClient\n",
    "\n",
    "# 1. Âª∫Á´ãËøûÊé•\n",
    "client = ArangoClient(hosts='http://127.0.0.1:8529')\n",
    "\n",
    "# 2. ËøûÊé•/ÂàõÂª∫Êï∞ÊçÆÂ∫ì\n",
    "sys_db = client.db('_system', username='root', password='pass123')\n",
    "if not sys_db.has_database('rag_db'):\n",
    "    sys_db.create_database('rag_db')\n",
    "\n",
    "db = client.db('rag_db', username='root', password='pass123')\n",
    "```\n",
    "\n",
    "### 3.2 Â≠òÂÇ®ÊñáÊ°£ \n",
    "\n",
    "ËøôÊòØÊàë‰ª¨Â≠òÂÇ® Chunk Ê≠£ÊñáÁöÑÂú∞Êñπ„ÄÇ\n",
    "\n",
    "```python\n",
    "# ÂàõÂª∫ÈõÜÂêà (Á±ª‰ººÂª∫Ë°®)\n",
    "if not db.has_collection('rag_chunks'):\n",
    "    chunks = db.create_collection('rag_chunks')\n",
    "else:\n",
    "    chunks = db.collection('rag_chunks')\n",
    "\n",
    "# ÊèíÂÖ•Êï∞ÊçÆ\n",
    "doc = {\n",
    "    \"_key\": \"uuid_1\",  # ÊåáÂÆö‰∏ªÈîÆÔºåÊñπ‰æøÊü•Êâæ\n",
    "    \"text\": \"ËøôÊòØÁ¨¨‰∏ÄÊÆµËØù„ÄÇ\",\n",
    "    \"cluster_id\": 101\n",
    "}\n",
    "chunks.insert(doc, overwrite=True) # overwrite=True Á±ª‰ºº Upsert\n",
    "\n",
    "# Êü•ËØ¢Êï∞ÊçÆ (ÈÄöËøá Key)\n",
    "result = chunks.get(\"uuid_1\")\n",
    "print(result['text'])\n",
    "```\n",
    "\n",
    "### 3.3 ÂàõÂª∫ÂÖ≥Á≥ª (Edge Operation)\n",
    "\n",
    "ËøôÊòØÊàë‰ª¨ÊûÑÂª∫‚Äú‰∏ä‰∏ãÊñáÈìæÊù°‚ÄùÁöÑÂÖ≥ÈîÆ„ÄÇ\n",
    "\n",
    "```python\n",
    "# ÂàõÂª∫ËæπÈõÜÂêà (ÂøÖÈ°ªÊåáÂÆö edge=True)\n",
    "if not db.has_collection('rag_relations'):\n",
    "    edges = db.create_collection('rag_relations', edge=True)\n",
    "\n",
    "# ÊèíÂÖ•‰∏ÄÊù°ËæπÔºöË°®Á§∫ uuid_1 ÁöÑ‰∏ã‰∏ÄÊÆµÊòØ uuid_2\n",
    "edge_data = {\n",
    "    \"_from\": \"rag_chunks/uuid_1\",  # ÂøÖÈ°ªÂ∏¶ÈõÜÂêàÂâçÁºÄ\n",
    "    \"_to\":   \"rag_chunks/uuid_2\",\n",
    "    \"type\":  \"NEXT_TO\"\n",
    "}\n",
    "edges.insert(edge_data)\n",
    "```\n",
    "\n",
    "### 3.4 È´òÁ∫ßÊü•ËØ¢ÔºöAQL (ArangoDB Query Language)\n",
    "\n",
    "ËøôÊòØ ArangoDB ÊúÄÂº∫Â§ßÁöÑÂú∞Êñπ„ÄÇAQL ÁúãËµ∑Êù•ÂæàÂÉè SQL„ÄÇ\n",
    "\n",
    "**Âú∫ÊôØ 1ÔºöÊôÆÈÄöÊü•ËØ¢**\n",
    "*‚ÄúÁªôÊàëÊâæÂá∫Â±û‰∫éËÅöÁ±ª 101 ÁöÑÊâÄÊúâÊñáÊ°£„ÄÇ‚Äù*\n",
    "\n",
    "```python\n",
    "aql = \"\"\"\n",
    "FOR doc IN rag_chunks\n",
    "    FILTER doc.cluster_id == 101\n",
    "    RETURN { id: doc._key, content: doc.text }\n",
    "\"\"\"\n",
    "cursor = db.aql.execute(aql)\n",
    "for item in cursor:\n",
    "    print(item)\n",
    "```\n",
    "\n",
    "**Âú∫ÊôØ 2ÔºöÂõæÈÅçÂéÜÔºà‰∏ä‰∏ãÊñáÊâ©Â±ïÔºâ**\n",
    "*‚ÄúÊâæÂà∞ uuid_1 ËøôÊÆµËØùÔºåÂπ∂‰∏îÈ°∫ÁùÄ 'NEXT_TO' ÂÖ≥Á≥ªÔºåÊääÂÆÉÂêéÈù¢ÁöÑ‰∏ÄÊÆµËØù‰πüÊâæÂá∫Êù•„ÄÇ‚Äù*\n",
    "\n",
    "```python\n",
    "graph_aql = \"\"\"\n",
    "FOR v, e, p IN 1..1 OUTBOUND 'rag_chunks/uuid_1' rag_relations\n",
    "    FILTER e.type == 'NEXT_TO'\n",
    "    RETURN v.text\n",
    "\"\"\"\n",
    "# 1..1 OUTBOUND: ÂêëÂ§ñËµ∞ 1 Ê≠•\n",
    "# v: vertex (ËäÇÁÇπ/‰∏ã‰∏ÄÊÆµËØù)\n",
    "# e: edge (Ëæπ)\n",
    "# p: path (Ë∑ØÂæÑ)\n",
    "\n",
    "cursor = db.aql.execute(graph_aql)\n",
    "next_text = [doc for doc in cursor]\n",
    "print(f\"‰∏ãÊñáÊòØ: {next_text}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Âú®Êú¨È°πÁõÆ‰∏≠ÁöÑÊï∞ÊçÆÊµÅËΩ¨Âõæ\n",
    "\n",
    "‰∏∫‰∫ÜËÆ©‰Ω†ÂΩªÂ∫ïÁêÜËß£ÔºåËØ∑ÁúãËøôÂº†Êï∞ÊçÆÂú® ArangoDB ÂÜÖÈÉ®ÁöÑÊµÅËΩ¨ÂõæÔºö\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph ArangoDB\n",
    "        direction TB\n",
    "        \n",
    "        C1[Chunk A] -- NEXT_TO --> C2[Chunk B]\n",
    "        C2 -- NEXT_TO --> C3[Chunk C]\n",
    "        \n",
    "        Header[Ê†áÈ¢ò: Ë¥¢Âä°Êä•Ë°®] -- PARENT_OF --> C1\n",
    "        Header -- PARENT_OF --> C2\n",
    "        \n",
    "        style C1 fill:#e1f5fe,stroke:#01579b\n",
    "        style C2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px\n",
    "        style C3 fill:#e1f5fe,stroke:#01579b\n",
    "    end\n",
    "    \n",
    "    Milvus(Milvus Ë∑ØÁî±) -.->|Cluster ID: 101| C2\n",
    "    \n",
    "    classDef highlight fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;\n",
    "```\n",
    "\n",
    "1.  **Milvus** ÂëäËØâÊàë‰ª¨Ë¶ÅÊâæ **Chunk B**ÔºàÈªÑËâ≤È´ò‰∫ÆÔºâ„ÄÇ\n",
    "2.  Êàë‰ª¨Áõ¥Êé•Âéª ArangoDB ÊãøÂà∞ **Chunk B** ÁöÑÊ≠£Êñá„ÄÇ\n",
    "3.  ÈÄöËøá **AQL ÂõæÈÅçÂéÜ**ÔºåÊàë‰ª¨Áû¨Èó¥ÊäìÂèñÂà∞Ôºö\n",
    "    *   `OUTBOUND` -> **Chunk C** (‰∏ãÊñá)\n",
    "    *   `INBOUND` -> **Chunk A** (‰∏äÊñá)\n",
    "    *   `INBOUND` -> **Header** (Áà∂Ê†áÈ¢ò)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba281f",
   "metadata": {},
   "source": [
    "## ËÆæËÆ°ÊÄùÊÉ≥\n",
    "‰º†ÁªüÁöÑRAGÔºàÂàáÁâá+ÂêëÈáèÂ∫ìÔºâÂ≠òÂú®‰∏§‰∏™Ëá¥ÂëΩÈóÆÈ¢òÔºö\n",
    "1. Êñ≠Á´†Âèñ‰πâÔºöÊääÊñáÊ°£ÂàáÂâ≤‰∏∫500Â≠ó‰∏ÄÊÆµÔºå‰∏¢Â§±‰∫Ü‚ÄùËøôÊÆµËØùÂ±û‰∫éÂì™‰∏™Ê†áÈ¢ò‚ÄúÁöÑ‰∏ä‰∏ãÊñá„ÄÇ\n",
    "2. ÊòæÂ≠òÊòÇË¥µÔºöÂá†Áôæ‰∏áÊù°ÂêëÈáèÂÖ®ÈÉ®Â°ûÂÖ•MilvusÔºåÂÜÖÂ≠òÂç†Áî®Â∑®Â§ßÔºå‰∏îÂåÖÂê´Â§ßÈáèÊó†ÂÖ≥Âô™Èü≥„ÄÇ\n",
    "\n",
    "Êàë‰ª¨ËøôÈáåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆæËÆ°ÊñπÊ°àÔºöFusionGraph RAGÔºöÂü∫‰∫éËÅöÁ±ªË∑ØÁî±‰∏éÂõæË∞±Â¢ûÂº∫ÁöÑÊ£ÄÁ¥¢Á≥ªÁªü\n",
    "\n",
    "Êàë‰ª¨ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºö\n",
    "1. Meta-chunkingÔºöÂà©Áî®OCRÁâàÈù¢ÂàÜÊûêÔºåÊåâÁÖßÊ†áÈ¢òÂíåËØ≠‰πâÂàáÂàÜÔºåËÄå‰∏çÊòØÊåâÂ≠óÊï∞„ÄÇ(Êú¨ÊïôÁ®ã‰∏∫‰∫ÜÂ≠¶‰π†ËÄÖÊõ¥Â•ΩÁöÑË∑ëÈÄöÊµÅÁ®ãÔºåÂπ∂Êú™‰ΩøÁî®OCR)\n",
    "2. FusionANNSÔºö\n",
    "    * MilvusÂèòË∫´‰∏∫Ë∑ØÁî±Ë°®ÔºåÂè™Â≠òÂÇ®ËÅöÁ±ª‰∏≠ÂøÉÔºåÊûÅÂ∫¶ËäÇÁúÅÂÜÖÂ≠ò\n",
    "    * ArangoDBÂèò‰∏∫Ëóè‰π¶ÈòÅÔºåÂ≠òÂÇ®ÂéüÂßãÊñáÊú¨ÂíåÂõæÂÖ≥Á≥ª„ÄÇ\n",
    "3. Graph ExpansionÔºàÂõæË∞±Êâ©Â±ïÔºâÔºöÊ£ÄÁ¥¢Âà∞‰∏ÄÂè•ËØùÊó∂ÔºåÈ°∫ÁùÄÂõæÂÖ≥Á≥ªÊää‰ªñ‰ª¨ÁöÑÁà∂Ê†áÈ¢òÂíåÂâç‰∏ÄÊÆµ‰∏ÄËµ∑ÊçûÂá∫Êù•ÔºåÁªôLLMÁúãÂÆåÊï¥ÁöÑ‰∏ä‰∏ãÊñá„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcb7ea",
   "metadata": {},
   "source": [
    "\n",
    "| Èò∂ÊÆµ (Stage) | ËæìÂÖ• (Input) | Ê†∏ÂøÉÂä®‰Ωú (Action) | ËæìÂá∫ (Output) | ÊâøËΩΩÁªÑ‰ª∂ |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **1. ETL Êï∞ÊçÆÂ§ÑÁêÜÂ±Ç** | ÂéüÂßã PDF | **OCR ÁâàÈù¢ÂàÜÊûê** + **Meta-Chunking** (ËØ≠‰πâÂàáÂàÜ) | ÁªìÊûÑÂåñÁöÑ Chunks (Âê´ Header Ë∑ØÂæÑ) | PaddleOCR, Python |\n",
    "| **2. Á¥¢ÂºïÊûÑÂª∫Â±Ç** | Chunks | **BGE-M3 ÂêëÈáèÂåñ** + **FAISS ËÅöÁ±ª** | Ë¥®ÂøÉÂêëÈáè (Centroids) + ËÅöÁ±ª ID | FAISS (GPU), BGE-M3 |\n",
    "| **3. Â≠òÂÇ®‰∏éÊúçÂä°Â±Ç** | Ë¥®ÂøÉ & Chunks | **ÂèåÂÜôÂàÜÂèë (Dual Ingestion)** | Milvus (Ë∑ØÁî±) + ArangoDB (ÂÜÖÂÆπ) | Milvus, ArangoDB |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e8477",
   "metadata": {},
   "source": [
    "## ËØ¶ÁªÜË°®ÁªìÊûÑËÆæËÆ°\n",
    "‰∏•Ê†ºÈÅµÂæ™Â≠òÁÆóÂàÜÁ¶ªÂéüÂàôÔºöMilvusÂè™Â≠òÂÇ®Á¥¢ÂºïÂ§¥ÔºåArangoDBÂ≠òÂÇ®ÂÖ®ÈáèÊï∞ÊçÆ\n",
    "\n",
    "### 1.MilvusË°®ÁªìÊûÑËÆæËÆ°\n",
    "**ËÆæËÆ°ÂéüÂàô**\n",
    "ÊûÅÂ∫¶Áò¶Ë∫´ÔºåÂè™Â≠òÂÇ®ËØùÈ¢òÁ∞áÁöÑ‰∏≠ÂøÉÁÇπÔºåÁî®‰∫éÂø´ÈÄüÂÆö‰ΩçÁî®Êà∑ÈóÆÈ¢òÂ±û‰∫éÂì™‰∏™È¢ÜÂüüÔºà‰æãÂ¶ÇÔºöË¥¢Âä°„ÄÅÊäÄÊúØ„ÄÅËøêÂä®Ôºâ\n",
    "\n",
    "Collection NameÔºörag_cluster_centorids\n",
    "| Â≠óÊÆµÂêç | Êï∞ÊçÆÁ±ªÂûã | Â±ûÊÄß | Ëß£Èáä‰∏éËÆæËÆ°ÁêÜÁî± |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`cluster_id`** | **Int64** | **Primary Key** | **Ê†∏ÂøÉËøûÊé•ÁÇπ**„ÄÇËøôÊòØËøûÊé• Milvus Âíå ArangoDB ÁöÑÂîØ‰∏ÄÈí•Âåô„ÄÇ‰æãÂ¶ÇÔºö`101`„ÄÇ |\n",
    "| **`vector`** | **FloatVector** | Dim=1024 | **Ë¥®ÂøÉÂêëÈáè**„ÄÇËØ•ËÅöÁ±ª‰∏ãÊâÄÊúâ Chunk ÂêëÈáèÁöÑÂπ≥ÂùáÂÄº„ÄÇÊü•ËØ¢Êó∂Áî®ÂÆÉÂÅöÁõ∏‰ººÂ∫¶ÂåπÈÖç„ÄÇ |\n",
    "| `member_count` | Int32 | Scalar | ËØ•ËÅöÁ±ªÂåÖÂê´Â§öÂ∞ë‰∏™ Chunk„ÄÇÁî®‰∫éÂêéÁª≠ÂèØËÉΩÁöÑÊùÉÈáçË∞ÉÊï¥ÔºàÊØîÂ¶ÇÂ§ßËÅöÁ±ªÈôçÊùÉÔºâ„ÄÇ |\n",
    "* ‰∏∫‰ªÄ‰πàË¶ÅËøô‰πàËÆæËÆ°Ôºü\n",
    "    * ÁúÅÈí±ÔºöÂÅáËÆæÊúâ1000‰∏á‰∏™chunkÔºåËÅöÊàê‰∏Ä‰∏á‰∏™Á±ªÔºåMilvusÂè™Â≠òÂÇ®1‰∏áÊù°ÂêëÈáèÔºåÂÜÖÂ≠òÂç†Áî®Áõ¥Êé•Âèò‰∏∫ÂéüÊù•ÁöÑ‰∏ÄÂçÉÂàÜ‰πã‰∏Ä\n",
    "    * Âø´ÔºöÂú®‰∏Ä‰∏áÊù°Êï∞ÊçÆÈáåÊêúÁ¥¢Top-5ÔºåÊØîÂú®1000‰∏áÊù°ÈáåÊêúÔºåÈÄüÂ∫¶Âø´Âá†‰∏™Êï∞ÈáèÁ∫ß"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3765be9",
   "metadata": {},
   "source": [
    "### 2.ArangoDBË°®ÁªìÊûÑËÆæËÆ°\n",
    "**ËÆæËÆ°ÂéüÂàô**Â≠òÂÇ®ÂÖ®ÈáèÊï∞ÊçÆÔºåÂπ∂Âà©Áî®ÂõæÂÖ≥Á≥ªËß£ÂÜ≥‰∏ä‰∏ãÊñá‰∏¢Â§±ÈóÆÈ¢ò\n",
    "\n",
    "#### A. ËäÇÁÇπÈõÜÂêà (Document Collection): `rag_chunks`\n",
    "\n",
    "| Â≠óÊÆµÂêç | Á±ªÂûã | Á¥¢ÂºïÁ±ªÂûã | Ëß£Èáä‰∏éËÆæËÆ°ÁêÜÁî± |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`_key`** | String | Primary | **Chunk UUID**„ÄÇÂîØ‰∏ÄÊ†áËØÜ‰∏Ä‰∏™ÊñáÊú¨Âùó„ÄÇ |\n",
    "| **`cluster_id`** | Int64 | **Persistent Index** | **Â§ñÈîÆ**„ÄÇÂØπÂ∫î Milvus ÈáåÁöÑ ID„ÄÇ**Êü•ËØ¢Êó∂ÈÄöËøáÊ≠§Â≠óÊÆµÊØ´ÁßíÁ∫ßÊãâÂèñËØ•Á∞áÊâÄÊúâÊï∞ÊçÆ„ÄÇ** |\n",
    "| `text` | String | None | **Ê≠£Êñá**„ÄÇÊúÄÂç†Á©∫Èó¥ÁöÑÊï∞ÊçÆÔºåÂ≠òÂú®Á£ÅÁõò‰∏äÔºå‰∏çÂç†ÂÆùË¥µÁöÑÊòæÂ≠ò„ÄÇ |\n",
    "| `header_path`| List[Str]| None | **Â±ÇÁ∫ßË∑ØÂæÑ**„ÄÇ‰æãÂ¶Ç `['2023Âπ¥Êä•', 'Ë¥¢Âä°Êï∞ÊçÆ']`„ÄÇÁî®‰∫éÁîüÊàêÂºïÁî®Êù•Ê∫ê„ÄÇ |\n",
    "| `metadata` | JSON | None | Â≠ò Page, BBox Á≠âÂÖÉÊï∞ÊçÆÔºåÁî®‰∫éÂâçÁ´ØÈ´ò‰∫ÆÊòæÁ§∫„ÄÇ |\n",
    "\n",
    "#### B. ËæπÈõÜÂêà (Edge Collection): `rag_relations`\n",
    "\n",
    "| Â≠óÊÆµÂêç | Á±ªÂûã | Ëß£Èáä‰∏éËÆæËÆ°ÁêÜÁî± |\n",
    "| :--- | :--- | :--- |\n",
    "| **`_from`** | String | Ëµ∑ÂßãËäÇÁÇπ ID (‰æãÂ¶Ç `rag_chunks/uuid_A`) |\n",
    "| **`_to`** | String | ÁõÆÊ†áËäÇÁÇπ ID (‰æãÂ¶Ç `rag_chunks/uuid_B`) |\n",
    "| **`type`** | String | **ÂÖ≥Á≥ªÁ±ªÂûã**„ÄÇÊ†∏ÂøÉËÆæËÆ°ÁÇπÔºåÁî®‰∫é‰∏ä‰∏ãÊñáÊâ©Â±ï„ÄÇ |\n",
    "\n",
    "*   **ÂÖ≥Á≥ªÁ±ªÂûã (`type`) ËØ¶Ëß£**Ôºö\n",
    "    *   `NEXT_TO`: Ë°®Á§∫ÈòÖËØªÈ°∫Â∫è„ÄÇÊü•ËØ¢Âà∞ Chunk A Êó∂ÔºåÈ°∫ÁùÄËøôÊù°ËæπËÉΩÊâæÂà∞ Chunk BÔºà‰∏ã‰∏ÄÊÆµÔºâ„ÄÇ\n",
    "    *   `PARENT_OF`: Ë°®Á§∫Â±ÇÁ∫ß„ÄÇÊü•ËØ¢Âà∞ Chunk A Êó∂ÔºåÂèçÂêëÈÅçÂéÜËøôÊù°ËæπËÉΩÊâæÂà∞ÂÆÉÁöÑÁà∂Ê†áÈ¢òËäÇÁÇπÔºàÂ¶ÇÊûúÊúâÁã¨Á´ãÊ†áÈ¢òËäÇÁÇπËÆæËÆ°Ôºâ„ÄÇ\n",
    "\n",
    "*   **‰∏∫‰ªÄ‰πàË¶ÅËøô‰πàËÆæËÆ°Ôºü**\n",
    "    *   **‰∏ä‰∏ãÊñáÊïëÊòü**ÔºöÂΩì LLM ÁúãÂà∞‰∏ÄÂè•‚ÄúÂáÄÂà©Ê∂¶Â¢ûÈïø 10%‚ÄùÊó∂ÔºåÂÆÉ‰∏çÁü•ÈÅìÊòØË∞ÅÁöÑ„ÄÇÈÄöËøáÂõæÈÅçÂéÜ `PARENT_OF` ÊâæÂà∞Áà∂Ê†áÈ¢ò‚ÄúÂçé‰∏úÂàÜÂÖ¨Âè∏‚ÄùÔºåLLM Â∞±ËÉΩÁ≤æÂáÜÂõûÁ≠î„ÄÇ\n",
    "    *   **ÁÅµÊ¥ª**ÔºöMilvus ËøôÁßçÂêëÈáèÂ∫ìÂæàÈöæÂ≠òËøôÁßçÁΩëÁä∂ÂÖ≥Á≥ªÔºåÂõæÊï∞ÊçÆÂ∫ìÊòØÊúÄ‰Ω≥ÈÄâÊã©„ÄÇ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f40dfd",
   "metadata": {},
   "source": [
    "## Êï∞ÊçÆÊ†ºÂºè\n",
    "\n",
    "### ÂàÜÊÆµÂêéÁöÑÊï∞ÊçÆÊ†ºÂºè\n",
    "```json\n",
    "{\n",
    "  \"id\": \"a1b2c3d4-5678-90ef-...\",  // UUID v4 Êàñ MD5\n",
    "  \"text\": \"2023Âπ¥ÂÖ¨Âè∏ÂáÄÂà©Ê∂¶‰∏∫2.5‰∫øÂÖÉÔºåÂêåÊØîÂ¢ûÈïø15%...\",\n",
    "  \"metadata\": {\n",
    "    \"source_file\": \"2023_financial_report.pdf\",\n",
    "    \"page_number\": 5,\n",
    "    \"chunk_index\": 42,             // Âú®ÂÖ®Êñá‰∏≠ÁöÑÂ∫èÂè∑ÔºåÁî®‰∫éÊéíÂ∫è\n",
    "    \"bbox\": [100, 200, 500, 600],  // PaddleOCR Êèê‰æõÁöÑÂùêÊ†á [x1, y1, x2, y2]\n",
    "    \"type\": \"text\"                 // text / table\n",
    "  },\n",
    "  \"header_path\": [\"2023Âπ¥Â∫¶Êä•Âëä\", \"Á¨¨ÂõõÁ´† Ë¥¢Âä°Êï∞ÊçÆ\", \"‰∏ªË¶Å‰ºöËÆ°Êï∞ÊçÆ\"], // Ê†∏ÂøÉÔºöÂ±ÇÁ∫ßË∑ØÂæÑ\n",
    "  \n",
    "  // È¢ÑÁïôÂ≠óÊÆµÔºåÁ®çÂêéËÆ°ÁÆóÂ°´ÂÖÖ\n",
    "  \"vector\": null,                  // Á≠âÂæÖ BGE-M3 Â°´ÂÖÖ\n",
    "  \"sparse_vector\": null,           // Á≠âÂæÖ BGE-M3 Â°´ÂÖÖ\n",
    "  \"cluster_id\": -1,                // Á≠âÂæÖ FAISS Â°´ÂÖÖ\n",
    "  \"prev_chunk_id\": \"e5f6...\",      // ÊåáÂêëÁ¨¨ 41 Âè∑ Chunk ÁöÑ ID\n",
    "  \"next_chunk_id\": \"g7h8...\"       // ÊåáÂêëÁ¨¨ 43 Âè∑ Chunk ÁöÑ ID\n",
    "}\n",
    "```\n",
    "### ArangoDBÊï∞ÊçÆÊ†ºÂºè\n",
    "#### ËäÇÁÇπË°®-Â≠òÊ≠£Êñá\n",
    "```json\n",
    "// Document Example\n",
    "{\n",
    "  \"_key\": \"a1b2c3d4...\",           // Áõ¥Êé•Â§çÁî® Chunk ÁöÑ UUID\n",
    "  \"text\": \"2023Âπ¥ÂÖ¨Âè∏ÂáÄÂà©Ê∂¶...\",\n",
    "  \"source\": \"2023_financial_report.pdf\",\n",
    "  \"page\": 5,\n",
    "  \"cluster_id\": 105,               // ÂÖ≥ÈîÆÁ¥¢ÂºïÂ≠óÊÆµÔºöÁî®‰∫é‰ªé Milvus Ë∑ØÁî±ËøáÊù•\n",
    "  \"header_path_str\": \"2023Âπ¥Â∫¶Êä•Âëä > Ë¥¢Âä°Êï∞ÊçÆ\", // Êñπ‰æø‰∫∫Á±ªÈòÖËØªÁöÑÂ≠óÁ¨¶‰∏≤\n",
    "  \"embedding_status\": true\n",
    "}\n",
    "```\n",
    "#### ËäÇÁÇπË°®-Â≠òÊ†áÈ¢ò\n",
    "```json\n",
    "// Document Example\n",
    "{\n",
    "  \"_key\": \"md5_of_header_text\",    // Ê†áÈ¢òÂÜÖÂÆπÁöÑ Hash\n",
    "  \"text\": \"‰∏ªË¶Å‰ºöËÆ°Êï∞ÊçÆ\",\n",
    "  \"level\": 3                       // H3 Ê†áÈ¢ò\n",
    "}\n",
    "```\n",
    "#### ËæπË°®-Â≠òÂÖ≥Á≥ª\n",
    "```json\n",
    "{\n",
    "  \"_from\": \"rag_chunks/chunk_42\",\n",
    "  \"_to\": \"rag_chunks/chunk_43\",\n",
    "  \"type\": \"NEXT_TO\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42221f18",
   "metadata": {},
   "source": [
    "## Êï∞ÊçÆÂ§ÑÁêÜ\n",
    "\n",
    "[document.pdf](https://github.com/Anduin2017/HowToCook)ÊòØ‰∏Ä‰∏™ÂÅöÈ•≠ÁöÑÊñáÊ°£ÔºåÈáåÈù¢Êúâ800Â§öÈ°µÔºåÁé∞Âú®Êàë‰ª¨ÂØπÂÖ∂ËøõË°åÂ§ÑÁêÜÔºå‰øùÂ≠ò‰∏∫Êàë‰ª¨ÈúÄË¶ÅÁöÑÊï∞ÊçÆÊ†ºÂºèÔºåÊúâÊù°‰ª∂ÁöÑÂèØ‰ª•Áî®OCRÔºå‰ΩÜËøôÈáåÊàëÁî®ÁöÑPyMuPDF+RayÔºåÂ∞ΩÂèØËÉΩÊääCPUË∑ëÊª°ÔºåÊïàÁéáÊúÄÂ§ßÂåñ,PyMuPDFÊòØPythonÁïåÊúÄÂø´ÁöÑPDFËß£ÊûêÂ∫ìÔºåÂ∫ïÂ±ÇÂü∫‰∫éC++ÂÆûÁé∞ÔºåÂèØ‰ª•Áõ¥Êé•ÊèêÂèñÊñáÊú¨ÁöÑÂ≠ó‰ΩìÂ§ßÂ∞èÂíå‰ΩçÁΩÆ „ÄÇ\n",
    "\n",
    "ËøôÈáåÊàë‰ª¨Ë¶ÅÂØπ‰πãÂâçÁöÑÁ≠ñÁï•ËøõË°åË∞ÉÊï¥Ôºö‰ª•ÂâçÊàë‰ª¨Èù†OCRÂëäËØâÊàë‰ª¨Âì™ÈáåÊòØÊ†áÈ¢òÔºåÁé∞Âú®Êàë‰ª¨ÈúÄË¶ÅÈù†Â≠ó‰ΩìÂ§ßÂ∞èÁåúÊµãÊ†áÈ¢ò‰ΩçÁΩÆÔºàÊØîÂ¶ÇÔºöÂ≠óÂè∑ > Ê≠£ÊñáÂπ≥ÂùáÂ≠óÂè∑ÁöÑ 1.2 ÂÄç -> ËßÜ‰∏∫ H1/H2Ôºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f904aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ray pymupdf langchain langchain-community zhipuai faiss-cpu numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375b326",
   "metadata": {},
   "source": [
    "1.  **Ray + PyMuPDF**: Âπ∂Ë°åÊèêÂèñ 800 È°µ PDF ÁöÑÁ∫ØÊñáÊú¨„ÄÇ\n",
    "2.  **LangChain Splitter**: ‰ΩøÁî®Ê†áÂáÜÁöÑ `RecursiveCharacterTextSplitter` ËøõË°åÂàáÁâá„ÄÇ\n",
    "3.  **ZhipuAI API**: Ë∞ÉÁî®‰∫ëÁ´ØÊ®°ÂûãÁîüÊàêÂêëÈáè„ÄÇ\n",
    "4.  **FAISS (CPU)**: Êú¨Âú∞Âø´ÈÄüËÅöÁ±ª„ÄÇ\n",
    "\n",
    "\n",
    "1.  **Ray Ë¥üË¥£ËÑèÊ¥ªÁ¥ØÊ¥ª**Ôºö\n",
    "    *   PDF Ëß£ÊûêÊòØ CPU ÂØÜÈõÜÂûã‰ªªÂä°ÔºåRay Âà©Áî®Â§öÊ†∏Âπ∂Ë°åÂ§ÑÁêÜÔºåÈÄüÂ∫¶ÊûÅÂø´„ÄÇ\n",
    "    *   Âè™ÊèêÂèñÁ∫ØÊñáÊú¨ÔºåÂÜÖÂ≠òÂç†Áî®ÊûÅ‰Ωé„ÄÇ\n",
    "\n",
    "2.  **LangChain Ë¥üË¥£ËßÑËåÉÂåñ**Ôºö\n",
    "    *   ‰ΩøÁî®‰∫Ü `RecursiveCharacterTextSplitter`„ÄÇËøôÊòØÁõÆÂâçÊúÄÈÄöÁî®ÁöÑÂàÜÊÆµÊñπÂºèÔºåËôΩÁÑ∂ÂÆÉ‰∏çÂ¶Ç Meta-Chunking Êô∫ËÉΩÔºà‰ºö‰∏¢Â§± Header Â±ÇÁ∫ß‰ø°ÊÅØÔºâÔºå‰ΩÜ**ÂÖºÂÆπÊÄßÊúÄÂ•ΩÔºå‰∏äÊâãÊúÄÂø´**„ÄÇ\n",
    "    *   ÂÆÉ‰ºöËá™Âä®Â§ÑÁêÜÊ†áÁÇπÁ¨¶Âè∑ÂàáÂàÜÔºå‰øùËØÅÂè•Â≠êÂ∞ΩÈáèÂÆåÊï¥„ÄÇ\n",
    "\n",
    "3.  **Êô∫Ë∞± API Ë¥üË¥£Ê†∏ÂøÉÁÆóÂäõ**Ôºö\n",
    "    *   ‰ΩøÁî®‰∫Ü `ZhipuAIEmbeddings`„ÄÇ‰Ω†Âè™ÈúÄË¶ÅÂ°´ KeyÔºåÂâ©‰∏ãÁöÑ‰∫§Áªô‰∫ëÁ´Ø„ÄÇ\n",
    "    *   **Ê≥®ÊÑè**Ôºö800 È°µ PDF ÂèØËÉΩ‰ºöÁîüÊàê 5000-8000 ‰∏™ Chunk„ÄÇÊô∫Ë∞± API ÊòØÊî∂Ë¥πÁöÑÔºàËôΩÁÑ∂ embedding-2 Âæà‰æøÂÆúÔºâÔºåËØ∑ÂÖ≥Ê≥®‰Ω†ÁöÑ Token Áî®Èáè„ÄÇ\n",
    "\n",
    "4.  **Êï∞ÊçÆÁªìÊûÑ‰øùÊåÅÂÖºÂÆπ**Ôºö\n",
    "    *   ËôΩÁÑ∂Âõ†‰∏∫‰ΩøÁî®‰∫Ü LangChain SplitterÔºåÊàë‰ª¨‰∏¢Â§±‰∫Ü `header_path` ÁöÑËá™Âä®ÊèêÂèñÔºàÁé∞Âú®‰∏∫Á©∫ÂàóË°® `[]`ÔºâÔºå‰ΩÜ `cluster_id` ÂíåÂõæË∞±ÊâÄÈúÄÁöÑ `prev/next` ÈìæË°®ÂÖ≥Á≥ª‰æùÁÑ∂‰øùÁïô„ÄÇËøô‰∏çÂΩ±ÂìçÊàë‰ª¨ÂêéÁª≠ÊûÑÂª∫‚ÄúÂ≠òÁÆóÂàÜÁ¶ª‚ÄùÊû∂ÊûÑÔºåÂè™ÊòØÂõæË∞±ÈáåÂ∞ë‰∫Ü‰∏ÄÁßç‚ÄúÁà∂Â≠êÂÖ≥Á≥ª‚ÄùËæπËÄåÂ∑≤„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc90fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain ÁªÑ‰ª∂\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# ==================================================\n",
    "# ÈÖçÁΩÆÂå∫Âüü\n",
    "# ==================================================\n",
    "ZHIPU_API_KEY = \"\"  # ‰Ω†ÁöÑÊô∫Ë∞±APIKey\n",
    "PDF_PATH = \"document.pdf\"   # ‰Ω†ÁöÑPDFË∑ØÂæÑ\n",
    "\n",
    "# ==================================================\n",
    "# 1. Ray Actor: PDF ÊñáÊú¨ÊèêÂèñÂ∑•ÂÖµ\n",
    "# ==================================================\n",
    "@ray.remote\n",
    "class PDFTextExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_text(self, pdf_path, start_page, end_page):\n",
    "        \"\"\"\n",
    "        Âè™Ë¥üË¥£ÊèêÂèñÊñáÊú¨Ôºå‰∏çË¥üË¥£ÂàÜÊÆµ„ÄÇ\n",
    "        ËøîÂõû: List[Dict] -> [{'page': 1, 'text': '...'}, ...]\n",
    "        \"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        results = []\n",
    "        \n",
    "        # Èò≤Ê≠¢È°µÁ†ÅË∂äÁïå\n",
    "        total = len(doc)\n",
    "        \n",
    "        for p_num in range(start_page, end_page):\n",
    "            if p_num >= total: break\n",
    "            \n",
    "            page = doc[p_num]\n",
    "            text = page.get_text(\"text\") # Áõ¥Êé•ÊèêÂèñÁ∫ØÊñáÊú¨\n",
    "            \n",
    "            # ÁÆÄÂçïÁöÑÊ∏ÖÊ¥óÔºåÂéªÊéâËøáÂ§öÁöÑÁ©∫Ë°å\n",
    "            clean_text = \"\\n\".join([line.strip() for line in text.split('\\n') if line.strip()])\n",
    "            \n",
    "            if clean_text:\n",
    "                results.append({\n",
    "                    \"page\": p_num + 1,\n",
    "                    \"text\": clean_text\n",
    "                })\n",
    "                \n",
    "        doc.close()\n",
    "        return results\n",
    "\n",
    "# ==================================================\n",
    "# 2. LangChain ÂàÜÊÆµÈÄªËæë\n",
    "# ==================================================\n",
    "def split_text_with_langchain(raw_pages: List[Dict]):\n",
    "    print(\"--- Ê≠£Âú®‰ΩøÁî® LangChain RecursiveCharacterTextSplitter ÂàÜÊÆµ ---\")\n",
    "    \n",
    "    # ÂàùÂßãÂåñ LangChain ÂàÜÂâ≤Âô®\n",
    "    # chunk_size: ÊØè‰∏™ÂùóÁöÑÂ≠óÁ¨¶Êï∞\n",
    "    # chunk_overlap: ÈáçÂè†ÈÉ®ÂàÜÔºåÈò≤Ê≠¢‰∏ä‰∏ãÊñá‰∏¢Â§±\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    final_chunks = []\n",
    "    \n",
    "    # ÈÅçÂéÜÊØè‰∏ÄÈ°µËøõË°åÂàáÂàÜ\n",
    "    # Ê≥®ÊÑèÔºöLangChain ÈÄöÂ∏∏Â§ÑÁêÜÁ∫ØÊñáÊú¨ÔºåÊàë‰ª¨ÈúÄË¶ÅÊää page metadata Â∏¶ËøõÂéª\n",
    "    for page_data in raw_pages:\n",
    "        page_num = page_data['page']\n",
    "        content = page_data['text']\n",
    "        \n",
    "        # Ë∞ÉÁî® LangChain ÂàáÂàÜ\n",
    "        # create_documents ‰ºöËøîÂõû Document ÂØπË±°ÂàóË°®\n",
    "        docs = text_splitter.create_documents([content])\n",
    "        \n",
    "        for i, doc in enumerate(docs):\n",
    "            chunk_dict = {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"text\": doc.page_content,\n",
    "                \"header_path\": [], # Á∫ØÊñáÊú¨ÊèêÂèñ‰∏¢Â§±‰∫Ü Header ‰ø°ÊÅØÔºåËøôÈáåÁïôÁ©∫ÊàñÂêéÁª≠Ë°•\n",
    "                \"metadata\": {\n",
    "                    \"source\": \"doc.pdf\",\n",
    "                    \"page\": page_num,\n",
    "                    \"chunk_index_in_page\": i,\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                # È¢ÑÁïôÂ≠óÊÆµ\n",
    "                \"vector\": None,\n",
    "                \"cluster_id\": -1,\n",
    "                \"prev_chunk_id\": None,\n",
    "                \"next_chunk_id\": None\n",
    "            }\n",
    "            final_chunks.append(chunk_dict)\n",
    "            \n",
    "    # Âª∫Á´ãÈìæË°®ÂÖ≥Á≥ª (Next/Prev)\n",
    "    for i in range(len(final_chunks)):\n",
    "        if i > 0:\n",
    "            final_chunks[i]['prev_chunk_id'] = final_chunks[i-1]['id']\n",
    "        if i < len(final_chunks) - 1:\n",
    "            final_chunks[i]['next_chunk_id'] = final_chunks[i+1]['id']\n",
    "            \n",
    "    return final_chunks\n",
    "\n",
    "# ==================================================\n",
    "# 3. ZhipuAI ÂêëÈáèÂåñ & FAISS ËÅöÁ±ª\n",
    "# ==================================================\n",
    "def process_embeddings_and_clusters(chunks):\n",
    "    print(\"--- Ê≠£Âú®Ë∞ÉÁî®Êô∫Ë∞± API ÁîüÊàêÂêëÈáè ---\")\n",
    "    \n",
    "    if \"‰Ω†ÁöÑAPI_KEY\" in ZHIPU_API_KEY:\n",
    "        raise ValueError(\"ËØ∑ÂÖàÂú®‰ª£Á†ÅÈ°∂ÈÉ®Â°´ÂÖ•Ê≠£Á°ÆÁöÑ ZHIPU_API_KEY\")\n",
    "\n",
    "    # ÂàùÂßãÂåñ LangChain ÁöÑÊô∫Ë∞± Embeddings\n",
    "    embeddings_model = ZhipuAIEmbeddings(\n",
    "        model=\"embedding-2\", # Êô∫Ë∞±ÁõÆÂâçÁöÑÈÄöÁî® Embedding Ê®°Âûã\n",
    "        api_key=ZHIPU_API_KEY\n",
    "    )\n",
    "    \n",
    "    texts = [c['text'] for c in chunks]\n",
    "    vectors = []\n",
    "    \n",
    "    batch_size = 10 \n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding Progress\"):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        try:\n",
    "            batch_vecs = embeddings_model.embed_documents(batch)\n",
    "            vectors.extend(batch_vecs)\n",
    "        except Exception as e:\n",
    "            print(f\"Batch {i} failed: {e}\")\n",
    "            vectors.extend([ [0.0]*1024 for _ in range(len(batch)) ])\n",
    "\n",
    "    # ËΩ¨Êç¢‰∏∫ numpy Ê†ºÂºè\n",
    "    np_vectors = np.array(vectors).astype('float32')\n",
    "    \n",
    "    print(\"--- Ê≠£Âú®ËøõË°åÊú¨Âú∞ËÅöÁ±ª (FAISS) ---\")\n",
    "    # ËÅöÁ±ªÊï∞ÔºöÂÅáËÆæÊØè 30 ‰∏™ Chunk ÊòØ‰∏Ä‰∏™ËØùÈ¢òÁ∞á\n",
    "    num_clusters = max(int(len(chunks) / 30), 2)\n",
    "    d = np_vectors.shape[1] # 1024\n",
    "    \n",
    "    # ËÆ≠ÁªÉ K-Means\n",
    "    kmeans = faiss.Kmeans(d, num_clusters, niter=20, verbose=True)\n",
    "    kmeans.train(np_vectors)\n",
    "    \n",
    "    # ÂØªÊâæÂΩíÂ±û\n",
    "    D, I = kmeans.index.search(np_vectors, 1)\n",
    "    cluster_ids = I.flatten().tolist()\n",
    "    centroids = kmeans.centroids\n",
    "    \n",
    "    # ÂõûÂ°´\n",
    "    for k, chunk in enumerate(chunks):\n",
    "        chunk['vector'] = vectors[k]\n",
    "        chunk['cluster_id'] = int(cluster_ids[k])\n",
    "        \n",
    "    return chunks, centroids\n",
    "\n",
    "# ==================================================\n",
    "# 4. ‰∏ªÁ®ãÂ∫è\n",
    "# ==================================================\n",
    "def main():\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    \n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        print(f\"Êâæ‰∏çÂà∞Êñá‰ª∂: {PDF_PATH}\")\n",
    "        return\n",
    "\n",
    "    doc = fitz.open(PDF_PATH)\n",
    "    total_pages = len(doc)\n",
    "    doc.close()\n",
    "    \n",
    "    print(f\"ÊñáÊ°£ÂÖ± {total_pages} È°µÔºåÂêØÂä® Ray Âπ∂Ë°åËß£Êûê...\")\n",
    "    \n",
    "    # 1. Ray ÊèêÂèñÊñáÊú¨\n",
    "    num_actors = 8\n",
    "    chunk_size = 50 # ÊØèÊ¨°Â§ÑÁêÜ 50 È°µ\n",
    "    \n",
    "    actors = [PDFTextExtractor.remote() for _ in range(num_actors)]\n",
    "    futures = []\n",
    "    \n",
    "    for i in range(0, total_pages, chunk_size):\n",
    "        actor = actors[ (i // chunk_size) % num_actors ]\n",
    "        futures.append(\n",
    "            actor.extract_text.remote(PDF_PATH, i, min(i+chunk_size, total_pages))\n",
    "        )\n",
    "        \n",
    "    results_nested = ray.get(futures)\n",
    "    \n",
    "    # Â±ïÂπ≥Âπ∂ÊåâÈ°µÁ†ÅÊéíÂ∫è\n",
    "    all_pages = []\n",
    "    for res in results_nested:\n",
    "        all_pages.extend(res)\n",
    "    all_pages.sort(key=lambda x: x['page'])\n",
    "    \n",
    "    print(f\"ÊèêÂèñÂÆåÊàêÔºåÂÖ± {len(all_pages)} È°µÊúâÊïàÊñáÊú¨\")\n",
    "    \n",
    "    # 2. LangChain ÂàÜÊÆµ\n",
    "    final_chunks = split_text_with_langchain(all_pages)\n",
    "    print(f\"LangChain ÂàáÂàÜÂÆåÊàêÔºåÂÖ±ÁîüÊàê {len(final_chunks)} ‰∏™ Chunks\")\n",
    "    \n",
    "    # 3. Êô∫Ë∞± Embedding + ËÅöÁ±ª\n",
    "    processed_chunks, centroids = process_embeddings_and_clusters(final_chunks)\n",
    "    \n",
    "    # 4. ‰øùÂ≠òÁªìÊûú\n",
    "    output_file = \"ready_for_db_zhipu.json\"\n",
    "    data = {\n",
    "        \"centroids\": centroids.tolist(),\n",
    "        \"chunks\": processed_chunks\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    print(f\"‚úÖ ÂÖ®ÈÉ®ÂÆåÊàêÔºÅÊï∞ÊçÆÂ∑≤‰øùÂ≠òËá≥ {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e4c20",
   "metadata": {},
   "source": [
    "Áé∞Âú®Êàë‰ª¨Â∑≤ÁªèÊúâ‰∫ÜÊ†∏ÂøÉÊï∞ÊçÆ‰∫ÜÔºåÊé•‰∏ãÊù•ÁöÑ‰ªªÂä°Â∞±ÊòØÊääËøô‰∫õÊï∞ÊçÆÂêÑÂΩíÂÖ∂‰Ωç„ÄÇ\n",
    "\n",
    "## ÂèåÂÜôÂ≠òÂ∫ì\n",
    "Ëøô‰∏ÄÊ≠•ÈùûÂ∏∏ÂÖ≥ÈîÆÔºåÊàë‰ª¨Â∞ÜÂÆûÁé∞Ôºö\n",
    "1. MilvusÂª∫Ë°®ÂÜôÂÖ•ÔºöÊääË¥®ÂøÉÂêëÈáèÂ≠òËøõÂéªÔºåÁÆÄÂéÜHNSWÁ¥¢Âºï\n",
    "2. ArangoDBÂª∫Âõæ‰∏éÂÜôÂÖ•ÔºöÊääChunkÂ≠ò‰∏∫ËäÇÁÇπÔºåÊääNext_toÂÖ≥Á≥ªÂ≠ò‰∏∫Ëæπ\n",
    "\n",
    "ÈªòËÆ§‰Ω†Â∑≤ÁªèÂÆâË£ÖÂ•Ω‰∫ÜMilvusÂíåAttuÔºåËØ∑ÂÖàÂú®docker-desktopÂêØÂä®milvusÔºå\n",
    "Â¶ÇÊûú‰Ω†Ê≤°ÊúâÂÆâË£ÖArangoDBÔºåËØ∑ÊâßË°åÔºö\n",
    "```shell\n",
    "docker run -d -p 8529:8529 -e ARANGO_ROOT_PASSWORD=pass123 --name arango swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/arangodb/arangodb:3.11\n",
    "```\n",
    "\n",
    "ÁÑ∂ÂêéÊâßË°å‰ª•‰∏ãÊåá‰ª§ÔºåÂÆâË£ÖÊâÄÈúÄ‰æùËµñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pymilvus python-arango"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cadc75a",
   "metadata": {},
   "source": [
    "ÂÖ®ÈÉ®ÊâßË°åÊàêÂäüÂêéÔºåËØ∑ÊâßË°å‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÔºå‰∏ãÈù¢ÁöÑ‰ª£Á†Å‰øùËØÅ‰∫ÜÂπÇÁ≠âÊÄßÔºöÂ¶ÇÊûúË°®Â∑≤Â≠òÂú®‰ºöÂÖàÂà†Èô§ÂÜçÈáçÂª∫ÔºåÁ°Æ‰øùË∞ÉËØïÁöÑÊó∂ÂÄôÊï∞ÊçÆ‰∏ç‰ºöÈáçÂ§ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58473832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pymilvus import (\n",
    "    connections, FieldSchema, CollectionSchema, DataType, \n",
    "    Collection, utility\n",
    ")\n",
    "from arango import ArangoClient\n",
    "\n",
    "# ==========================================\n",
    "# ÈÖçÁΩÆÂå∫Âüü\n",
    "# ==========================================\n",
    "JSON_FILE = \"ready_for_db_zhipu.json\"\n",
    "\n",
    "# Milvus ÈÖçÁΩÆ\n",
    "MILVUS_HOST = \"127.0.0.1\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "MILVUS_COLLECTION = \"rag_cluster_centroids\"\n",
    "DIMENSION = 1024 \n",
    "\n",
    "# ArangoDB ÈÖçÁΩÆ\n",
    "ARANGO_URL = \"http://127.0.0.1:8529\"\n",
    "ARANGO_USER = \"root\"\n",
    "ARANGO_PASS = \"pass123\" \n",
    "ARANGO_DB_NAME = \"rag_db\"\n",
    "ARANGO_GRAPH_NAME = \"rag_knowledge_graph\" # ÂõæÂêçÁß∞\n",
    "\n",
    "def load_json_data():\n",
    "    print(f\"Ê≠£Âú®ËØªÂèñ {JSON_FILE} ...\")\n",
    "    try:\n",
    "        with open(JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ÈîôËØØ: Êâæ‰∏çÂà∞Êñá‰ª∂ {JSON_FILE}ÔºåËØ∑Ê£ÄÊü•Ë∑ØÂæÑ„ÄÇ\")\n",
    "        exit(1)\n",
    "\n",
    "# ==========================================\n",
    "# 1. Milvus ÂÖ•Â∫ìÈÄªËæë (Â≠òË∑ØÁî±)\n",
    "# ==========================================\n",
    "def ingest_to_milvus(centroids):\n",
    "    print(\"\\n=== ÂºÄÂßãÂÜôÂÖ• Milvus (Ë∑ØÁî±Â±Ç) ===\")\n",
    "    \n",
    "    # 1. ËøûÊé•\n",
    "    try:\n",
    "        connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    except Exception as e:\n",
    "        print(f\"Milvus ËøûÊé•Â§±Ë¥•: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Ê∏ÖÁêÜÊóßË°®\n",
    "    if utility.has_collection(MILVUS_COLLECTION):\n",
    "        utility.drop_collection(MILVUS_COLLECTION)\n",
    "        print(f\"Â∑≤Âà†Èô§ÊóßË°®: {MILVUS_COLLECTION}\")\n",
    "        \n",
    "    # 3. ÂÆö‰πâ Schema\n",
    "    fields = [\n",
    "        FieldSchema(name=\"cluster_id\", dtype=DataType.INT64, is_primary=True),\n",
    "        FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, description=\"RAG Cluster Centroids\")\n",
    "    \n",
    "    # 4. ÂàõÂª∫ÈõÜÂêà\n",
    "    collection = Collection(name=MILVUS_COLLECTION, schema=schema)\n",
    "    print(\"Ë°®ÁªìÊûÑÂàõÂª∫ÊàêÂäü\")\n",
    "    \n",
    "    # 5. ÊèíÂÖ•Êï∞ÊçÆ\n",
    "    ids = [i for i in range(len(centroids))]\n",
    "    vectors = centroids\n",
    "    \n",
    "    if len(vectors) > 0 and len(vectors[0]) != DIMENSION:\n",
    "        print(f\"ÈîôËØØ: ÂêëÈáèÁª¥Â∫¶ {len(vectors[0])} ‰∏éÈÖçÁΩÆ {DIMENSION} ‰∏çÁ¨¶ÔºÅ\")\n",
    "        return\n",
    "\n",
    "    mr = collection.insert([ids, vectors])\n",
    "    print(f\"ÊèíÂÖ•ËØ∑Ê±ÇÂ∑≤Êèê‰∫§ÔºåÂèóÂΩ±ÂìçË°åÊï∞: {mr.insert_count}\")\n",
    "    \n",
    "    print(\"Ê≠£Âú®Âà∑Áõò (Flush)...\")\n",
    "    collection.flush()\n",
    "    \n",
    "    # 7. ÂàõÂª∫Á¥¢Âºï\n",
    "    index_params = {\n",
    "        \"metric_type\": \"COSINE\", \n",
    "        \"index_type\": \"HNSW\",\n",
    "        \"params\": {\"M\": 16, \"efConstruction\": 200}\n",
    "    }\n",
    "    print(\"Ê≠£Âú®ÊûÑÂª∫Á¥¢Âºï...\")\n",
    "    collection.create_index(field_name=\"vector\", index_params=index_params)\n",
    "    utility.index_building_progress(MILVUS_COLLECTION)\n",
    "    \n",
    "    # 8. Load\n",
    "    collection.load()\n",
    "    print(\"Milvus Collection Loaded ‚úÖ\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. ArangoDB ÂÖ•Â∫ìÈÄªËæë (Â≠òÂÜÖÂÆπÂõæË∞±)\n",
    "# ==========================================\n",
    "def ingest_to_arango(chunks):\n",
    "    print(\"\\n=== ÂºÄÂßãÂÜôÂÖ• ArangoDB (ÂÜÖÂÆπÂ±Ç) ===\")\n",
    "    \n",
    "    # 1. ËøûÊé•\n",
    "    try:\n",
    "        sys_client = ArangoClient(hosts=ARANGO_URL)\n",
    "        sys_db = sys_client.db('_system', username=ARANGO_USER, password=ARANGO_PASS)\n",
    "        \n",
    "        # ÂàõÂª∫Êï∞ÊçÆÂ∫ì\n",
    "        if not sys_db.has_database(ARANGO_DB_NAME):\n",
    "            sys_db.create_database(ARANGO_DB_NAME)\n",
    "        \n",
    "        db = sys_client.db(ARANGO_DB_NAME, username=ARANGO_USER, password=ARANGO_PASS)\n",
    "    except Exception as e:\n",
    "        print(f\"ArangoDB ËøûÊé•Â§±Ë¥•: {e}\")\n",
    "        return\n",
    "    \n",
    "    COL_CHUNKS = \"rag_chunks\"\n",
    "    COL_RELATIONS = \"rag_relations\"\n",
    "    \n",
    "    # 2. Ê∏ÖÁêÜÁéØÂ¢É (Âà†Èô§ÊóßÁöÑ Graph ÂÆö‰πâÂíå Collections)\n",
    "    # ÂÖàÂà† GraphÔºåÂÜçÂà† CollectionÔºåÂê¶Âàô‰ºöÊä•Èîô\n",
    "    if db.has_graph(ARANGO_GRAPH_NAME):\n",
    "        db.delete_graph(ARANGO_GRAPH_NAME)\n",
    "    if db.has_collection(COL_RELATIONS): db.delete_collection(COL_RELATIONS)\n",
    "    if db.has_collection(COL_CHUNKS): db.delete_collection(COL_CHUNKS)\n",
    "    \n",
    "    # 3. ÂàõÂª∫ Collection\n",
    "    chunks_col = db.create_collection(COL_CHUNKS)\n",
    "    relations_col = db.create_collection(COL_RELATIONS, edge=True)\n",
    "    \n",
    "    # 4. ÂáÜÂ§áÊï∞ÊçÆ\n",
    "    batch_docs = []\n",
    "    batch_edges = []\n",
    "    \n",
    "    print(\"Ê≠£Âú®È¢ÑÂ§ÑÁêÜÊï∞ÊçÆ...\")\n",
    "    for chunk in chunks:\n",
    "        # Á°Æ‰øù cluster_id Â≠òÂú®ÔºåÂê¶ÂàôÁªôÈªòËÆ§ÂÄº -1\n",
    "        cid = chunk.get(\"cluster_id\", -1)\n",
    "        if cid is None: cid = -1\n",
    "\n",
    "        doc = {\n",
    "            \"_key\": chunk[\"id\"], \n",
    "            \"text\": chunk[\"text\"],\n",
    "            \"cluster_id\": int(cid),\n",
    "            \"page\": chunk[\"metadata\"].get(\"page\", -1),\n",
    "            \"source\": chunk[\"metadata\"].get(\"source\", \"unknown\")\n",
    "        }\n",
    "        batch_docs.append(doc)\n",
    "        \n",
    "        if chunk.get(\"next_chunk_id\"):\n",
    "            edge = {\n",
    "                \"_from\": f\"{COL_CHUNKS}/{chunk['id']}\",\n",
    "                \"_to\": f\"{COL_CHUNKS}/{chunk['next_chunk_id']}\",\n",
    "                \"type\": \"NEXT_TO\"\n",
    "            }\n",
    "            batch_edges.append(edge)\n",
    "\n",
    "    # 5. ÊâπÈáèÂÜôÂÖ• \n",
    "    print(f\"Ê≠£Âú®ÊâπÈáèÂÜôÂÖ• {len(batch_docs)} ‰∏™ Chunk...\")\n",
    "    # on_duplicate=\"replace\" ‰øùËØÅÈáçÂ§çËøêË°å‰∏çÊä•Èîô\n",
    "    res_docs = chunks_col.import_bulk(batch_docs, on_duplicate=\"replace\") \n",
    "    if res_docs['errors'] > 0:\n",
    "        print(f\"Ë≠¶Âëä: Chunk ÂÜôÂÖ•Âá∫Áé∞ {res_docs['errors']} ‰∏™ÈîôËØØ! ËØ¶ÊÉÖ: {res_docs['details']}\")\n",
    "    \n",
    "    print(f\"Ê≠£Âú®ÊâπÈáèÂÜôÂÖ• {len(batch_edges)} Êù°ÂÖ≥Á≥ª...\")\n",
    "    res_edges = relations_col.import_bulk(batch_edges, on_duplicate=\"replace\")\n",
    "    if res_edges['errors'] > 0:\n",
    "        print(f\"Ë≠¶Âëä: ÂÖ≥Á≥ªÂÜôÂÖ•Âá∫Áé∞ {res_edges['errors']} ‰∏™ÈîôËØØ! (ÂèØËÉΩÊòØ next_id ‰∏çÂ≠òÂú®)\")\n",
    "\n",
    "    # 6. ÂàõÂª∫ Graph ÂÆö‰πâ (Êñπ‰æøÂú® Web UI Êü•Áúã)\n",
    "    print(\"Ê≠£Âú®ÂàõÂª∫ÂõæË∞±ÂÆö‰πâ (Graph Definition)...\")\n",
    "    edge_definitions = [\n",
    "        {\n",
    "            \"edge_collection\": COL_RELATIONS,\n",
    "            \"from_vertex_collections\": [COL_CHUNKS],\n",
    "            \"to_vertex_collections\": [COL_CHUNKS]\n",
    "        }\n",
    "    ]\n",
    "    db.create_graph(ARANGO_GRAPH_NAME, edge_definitions=edge_definitions)\n",
    "    \n",
    "    # 7. ÂàõÂª∫Á¥¢Âºï\n",
    "    chunks_col.add_persistent_index(fields=[\"cluster_id\"])\n",
    "    print(\"ArangoDB ÂÖ•Â∫ì & ÂõæË∞±ÊûÑÂª∫ÂÆåÊàê ‚úÖ\")\n",
    "\n",
    "# ==========================================\n",
    "# ‰∏ªÁ®ãÂ∫è\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_json_data()\n",
    "    centroids = data.get(\"centroids\", [])\n",
    "    chunks = data.get(\"chunks\", [])\n",
    "    \n",
    "    if not centroids or not chunks:\n",
    "        print(\"ÈîôËØØ: JSON Êï∞ÊçÆ‰∏∫Á©∫ÊàñÊ†ºÂºè‰∏çÊ≠£Á°Æ\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"Êï∞ÊçÆÂä†ËΩΩÂÆåÊØï: {len(centroids)} ‰∏™Ë¥®ÂøÉ, {len(chunks)} ‰∏™ÊñáÊú¨Âùó\")\n",
    "    \n",
    "    # ÊâßË°åÂÜôÂÖ•\n",
    "    ingest_to_milvus(centroids)\n",
    "    ingest_to_arango(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046a750",
   "metadata": {},
   "source": [
    "\n",
    "### ÊÄé‰πàÈ™åËØÅÊàêÂäü‰∫ÜÔºü\n",
    "\n",
    "1.  **ArangoDB**: ÊµèËßàÂô®ÊâìÂºÄ `http://127.0.0.1:8529`ÔºåÁôªÂΩï root/pass123„ÄÇ\n",
    "    *   ÈÄâÊã©Êï∞ÊçÆÂ∫ì `rag_db`„ÄÇ\n",
    "\n",
    "        ![alt](https://github.com/datawhalechina/easy-vectordb/blob/main/docs/public/images/arango_login.png)\n",
    "        \n",
    "    *   ÁÇπÂ∑¶‰æß **COLLECTIONS** -> `rag_chunks`„ÄÇ‰Ω†Â∫îËØ•ËÉΩÁúãÂà∞ÊâÄÊúâÁöÑÊñáÊú¨Âùó„ÄÇ\n",
    "\n",
    "        ![alt](https://github.com/datawhalechina/easy-vectordb/blob/main/docs/public/images/arango_rag_chunks.png)\n",
    "\n",
    "    *   ÁÇπ **GRAPHS** -> Create Graph -> ÈÄâÂàöÊâçÈÇ£‰∏§‰∏™Ë°® -> Èöè‰æøÁÇπ‰∏Ä‰∏™ËäÇÁÇπÔºåÁúãÁúãËÉΩ‰∏çËÉΩÁúãÂà∞ËøûÁ∫øÔºàNEXT_TOÔºâ„ÄÇ\n",
    "2.  **Milvus**: ‰ΩøÁî® `Attu` (Milvus ÁöÑÂèØËßÜÂåñÂ∑•ÂÖ∑) ÊàñËÄÖÂçïÁ∫ØÁúã Python ËÑöÊú¨ÊúÄÂêéÊúâÊ≤°ÊúâÊâìÂç∞ `Milvus Collection Loaded ‚úÖ`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cce20d",
   "metadata": {},
   "source": [
    "## Ê¢≥ÁêÜ\n",
    "\n",
    "‰∏ãÈù¢Â∞±ÂèØ‰ª•ËøõË°åÊêúÁ¥¢‰∫ÜÔºå‰ΩÜÂú®ËøõË°åÊêúÁ¥¢‰πãÂâçÔºåÊàë‰ª¨ÈúÄË¶ÅÊ¢≥ÁêÜ‰∏Ä‰∏ãÔºå‰∏∫‰ªÄ‰πàË¶ÅËøô‰πàËÆæËÆ°ÔºåËÆæËÆ°ÁöÑÁªÜËäÇ‰Ω†ÊòØÂê¶ÁêÜËß£ÔºåÂ≠òÂÇ®ÁöÑÊó∂ÂÄôÔºåmilvusÂíåarangodbÊòØÂ¶Ç‰ΩïÂÖ≥ËÅî‰∏äÁöÑÔºåÂ≠òÂÇ®ÁöÑÊµÅÁ®ãÊòØÊÄé‰πàÊ†∑ÁöÑÔºåÂÖàmilvusÂÜçarangodbÂêóÔºåÁÑ∂ÂêéÂ≠òÂÇ®ÁöÑÊó∂ÂÄôÔºåËøôÊ†∑ËÆæËÆ°ÊòØ‰∏∫‰∫ÜÊñπ‰æøÊêúÁ¥¢ÂêóÔºåËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢ò\n",
    "\n",
    "\n",
    "Êàë‰ª¨Áé∞Âú®ÂÅú‰∏ãÊù•Ôºå‰∏çÂÜô‰ª£Á†ÅÔºå‰∏ìÈó®Êää ‚Äú‰∏∫‰ªÄ‰πàË¶ÅËøô‰πàËÆæËÆ°‚Äù Âíå ‚ÄúÂÆÉ‰ª¨ÊòØÊÄé‰πàÂÖ≥ËÅîÁöÑ‚Äù Ëøô‰∏§‰∏™ÈóÆÈ¢òÂΩªÂ∫ïÊãÜËß£Ê∏ÖÊ•ö„ÄÇ\n",
    "\n",
    "**1. Ê†∏ÂøÉÁñëÈóÆÔºöMilvusÂíåArangoDBÊòØÂ¶Ç‰ΩïÂÖ≥ËÅî‰∏äÁöÑÔºü**\n",
    "Á≠îÊ°àÔºöÈÄöËøácluster_idÂÖ≥ËÅîÁöÑ„ÄÇ\n",
    "\n",
    "\n",
    "Âú®Â≠òÂÇ®ÁöÑÊó∂ÂÄôÔºö\n",
    "* Êàë‰ª¨ÁÆóÂá∫Ëøô‰∏™N‰∏™ChunkÂèØ‰ª•ÂàÜ‰∏∫k‰∏™Á±ªÔºàËÅöÁ±ªÔºâ„ÄÇ\n",
    "* Êàë‰ª¨ÁªôÂÖ∂‰∏≠ÁöÑÊüê‰∏™Á±ªÂàÜÈÖç‰∫Ü‰∏Ä‰∏™Âè∑Á†ÅÁâåÊØîÂ¶ÇÔºöcluster_id = 101\n",
    "* Êàë‰ª¨ÂëäËØâ**Milvus**ÔºöÂè∑Á†ÅÁâå101ÁöÑÁâπÂæÅÊòØ[0.1,0.9,0.3...]ÔºàËÅöÁ±ª‰∏≠ÂøÉÁöÑË¥®ÂøÉÂêëÈáèÔºâ\n",
    "* Êàë‰ª¨ÂëäËØâ**ArangoDB**ÔºöËøô‰∫õchunkÈÉΩÊòØÂ±û‰∫é101Âè∑ÁöÑÔºåÁªô‰ªñ‰ª¨Ë¥¥‰∏äcluster_id:101ÁöÑÊ†áÁ≠æ\n",
    "\n",
    "Âú®Êü•ËØ¢ÁöÑÊó∂ÂÄôÔºö\n",
    "* Áî®Êà∑ÈóÆÊüê‰∏™Á±ªÁöÑÂêçÁß∞\n",
    "* MilvusÁÆó‰∫Ü‰∏Ä‰∏ãÔºåËØ¥ÔºöÂéª101Âè∑Êâæ\n",
    "* ‰Ω†Ëµ∞Âà∞‰∫ÜArangoDBÔºåÂñä‰∫Ü‰∏ÄÂ£∞ÔºöÈÇ£‰∫õchunkÊòØÂ±û‰∫é101Âè∑ÁöÑÔºåÁ´ôÂá∫Êù•ÔºÅ\n",
    "\n",
    "ÁªìËÆ∫ÔºöMilvusÂíåArangoDBÈÄöËøáËÅöÁ±ªidÁõ∏ÂÖ≥ËÅîÔºåMilvusÂ≠òID->ÂêëÈáèÔºåArangoDBÂ≠òID->Ê≠£Êñá\n",
    "\n",
    "**2. Â≠òÂÇ®ÊµÅÁ®ãÔºö‰∏∫‰ªÄ‰πàÊòØÂÖàÁÆóÂêéÂ≠òÔºü**\n",
    "\n",
    "ÂèØËÉΩÊúâÁßçÁñëÊÉëÔºöÂÖàMilvusÂÜçArangoDBÂêóÔºü\n",
    "\n",
    "ÂÖ∂ÂÆûÊõ¥ÂáÜÁ°ÆÁöÑËØ¥Ê≥ïÊòØÔºöÂÖàÂÜÖÂ≠òËÆ°ÁÆóÔºåÂÜçÂèåÂÜôÂàÜÂèë„ÄÇ\n",
    "\n",
    "ÊµÅÁ®ãÂ§çÁõòÔºö\n",
    "* Âú®PythonÂÜÖÂ≠òÈáåÔºöËøôÊòØÊúÄÂÖ≥ÈîÆÁöÑ‰∏ÄÊ≠•ÔºåÊï∞ÊçÆËøòÊ≤°ÊúâËøõÂÖ•Êï∞ÊçÆÂ∫ì‰πãÂâçÔºåÂ∑≤ÁªèÂú®Python‰∏≠ËøõË°å‰∫ÜÂêëÈáèÂåñÂíåËÅöÁ±ª\n",
    "    * Ê≠§Êó∂ÔºåÊØè‰∏™chunkÂØπË±°Âú®ÂÜÖÂ≠ò‰∏≠Â∑≤ÁªèÊúâ‰∫Ücluster_idËøô‰∏™Â±ûÊÄß\n",
    "* ÂàÜÂèëÔºö\n",
    "    * Âè™Ë¶ÅÂÜÖÂ≠òÈáåÁöÑÊï∞ÊçÆÊúâ‰∫Ücluster_idÔºåÈÇ£ÂÖàÂÜôÂÖ•Ë∞ÅÂπ∂‰∏çÈáçË¶Å„ÄÇ\n",
    "    * ‰∏∫‰∫Ü‰ª£Á†ÅÈÄªËæëÊ∏ÖÊô∞ÔºåÈÄöÂ∏∏Âπ∂Ë°åÂÜôÂÖ•ÔºåÊàñËÄÖÂÖàÂÜôÂÖ•MilvusÂª∫Á´ãÁõÆÂΩïÔºåÁÑ∂ÂêéÂÜçÂÜôÂÖ•ArangoDB„ÄÇ\n",
    "\n",
    "**3. Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÈóÆÈ¢òÔºü**\n",
    "\n",
    "Êó¢ÁÑ∂ÊúâMilvus‰∫ÜÔºå‰∏∫‰ªÄ‰πà‰∏çÊäätextÁõ¥Êé•Â≠òÂú®MilvusÁöÑpayload‰∏≠Ôºå‰∏∫‰ªÄ‰πàË¶ÅÊêû‰ø©Êï∞ÊçÆÂ∫ìÔºü\n",
    "\n",
    "ÈóÆÈ¢ò1ÔºöÊòæÂ≠òÂ§™Ë¥µÔºåHNSWÁ¥¢ÂºïÂ§™Âç†Áî®ÂÜÖÂ≠ò\n",
    "* ‰º†ÁªüÊñπÊ≥ïÔºöÊää1000‰∏áÊù°ChunkÁöÑÂêëÈáèÂÖ®ÈÉ®Âª∫Á´ãHNSWÁ¥¢Âºï„ÄÇ\n",
    "    * ÂêéÊûúÔºöHNSWÁ¥¢ÂºïÈúÄË¶ÅÊääÂêëÈáèÂä†ËΩΩÂà∞ÂÜÖÂ≠òÔºå1000‰∏áÊù° x 1024Áª¥ x 4Â≠óËäÇ ‚âà 40GBÂÜÖÂ≠ò„ÄÇ\n",
    "* Êàë‰ª¨ÁöÑÂÅöÊ≥ïÔºöMilvusÂè™Â≠ò1‰∏á‰∏™ËÅöÁ±ªË¥®ÂøÉ\n",
    "    * ÂêéÊûúÔºö1‰∏áÊù°ÂêëÈáè ‚âà 40MBÂÜÖÂ≠ò\n",
    "    * ‰ºòÂäøÔºöÂÜÖÂ≠òÂç†Áî®Èôç‰Ωé1000ÂÄç\n",
    "\n",
    "ÈóÆÈ¢ò2ÔºöËØ≠‰πâÊ£ÄÁ¥¢ÁöÑËøëËßÜÁúºÈóÆÈ¢ò\n",
    "* ‰º†ÁªüÂÅöÊ≥ïÔºöMilvusÊêúÂá∫Êù•‰∏ÄÂè•ËØùÔºöÂáÄÂà©Ê∂¶Â¢ûÈïø‰∫Ü10%\n",
    "    * ÂêéÊûúÔºöLLMÊãøÂà∞ËøôÂè•ËØùÊòØÊáµÁöÑ„ÄÇË∞ÅÁöÑÂáÄÂà©Ê∂¶ÔºüÊòØÈõÜÂõ¢ÁöÑËøòÊòØÂ≠êÂÖ¨Âè∏ÁöÑÔºüÊòØ2023Âπ¥ÁöÑËøòÊòØ2022Âπ¥ÁöÑÔºü\n",
    "* Êàë‰ª¨ÁöÑÂÅöÊ≥ïÔºöArangoDBÂ≠òÂÇ®‰∫ÜNEXT_TOÂíåPARENT_OFÂÖ≥Á≥ª„ÄÇ\n",
    "    * ‰ºòÂäøÔºöÂú®Êü•ËØ¢Âà∞ËøôÂè•ËØùÁöÑÂêåÊó∂ÔºåÈ°∫ÁùÄÂõæÂÖ≥Á≥ªÔºåÊääÂÆÉÁöÑÁà∂Ê†áÈ¢òÔºàÂçé‰∏úÂàÜÂÖ¨Âè∏ÔºâÂíåÂâç‰∏ÄÊÆµÔºà2023Âπ¥Ë¥¢Êä•ÊëòË¶ÅÔºâ‰∏ÄËµ∑ÊäìÂá∫Êù•„ÄÇLLMÁû¨Èó¥ÁúãÊáÇ‰∫ÜÔºåMilvusÂÅö‰∏çÂà∞ËøôÁßçÂõæÈÅçÂéÜÊü•ËØ¢\n",
    "\n",
    "ÈóÆÈ¢ò3ÔºöÊêúÁ¥¢ÁöÑÊºèÊñóÊïàÂ∫î\n",
    "* ‰º†ÁªüÂÅöÊ≥ïÔºöTop-KÊêúÁ¥¢ÔºàÊØîÂ¶ÇK= 5Ôºâ\n",
    "    * ÂêéÊûúÔºöÂ¶ÇÊûúÁõ∏ÂÖ≥ÂÜÖÂÆπÊúâ20ÊÆµÔºå‰Ω†Âè™Âè¨Âõû‰∫Ü5ÊÆµÔºåÂâ©‰∏ãÁöÑ15ÊÆµ‰∏¢‰∫ÜÔºåËøôÂè´‰ΩéÂè¨ÂõûÁéá„ÄÇ\n",
    "* Êàë‰ª¨ÁöÑÂÅöÊ≥ïÔºöÂÖàÊâæÁõ∏ÂÖ≥ÁöÑËØùÈ¢òÁ∞á\n",
    "    * ‰ºòÂäøÔºöÊØîÂ¶ÇÂëΩ‰∏≠Ë¥¢Âä°Á∞áÔºåÊàë‰ª¨ÊääËøô‰∏™Á∞áÈáåÁöÑ50‰∏™ChunkÂÖ®ÈÉ®ÊãøÂá∫Êù•\n",
    "    * ÁªìÊûúÔºöÂè¨ÂõûËåÉÂõ¥Êâ©Â§ß‰∫ÜÔºàRecallÊèêÂçáÔºâÔºåÁÑ∂ÂêéÂÜçÁî®RerankÊ®°ÂûãÁ≤æÊåëÁªÜÈÄâÔºåËøôÊòØ‰∏ÄÁßçÂπøÊííÁΩëÔºåÁ≤æÊçïÊçûÁöÑÁ≠ñÁï•ÔºåÈÄÇÂêàÈïøÊñáÊ°£ÈóÆÁ≠î„ÄÇ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2bde1c",
   "metadata": {},
   "source": [
    "## Ê£ÄÁ¥¢\n",
    "\n",
    "ËøôÈáåÊàë‰ª¨‰ºöÁî®Âà∞BGE-Reranker-BaseÈáçÊéíÂ∫èÊ®°Âûã\n",
    "* Â§ßÂ∞èÔºöÊ®°ÂûãÊñá‰ª∂Âè™Êúâ1GB\n",
    "* ÊïàÊûúÔºöÊòØÁõÆÂâçÂºÄÊ∫êÁïåÊÄß‰ª∑ÊØîÊúÄÈ´òÁöÑÈáçÊéíÂ∫èÊ®°Âûã‰πã‰∏Ä\n",
    "* Êù•Ê∫êÔºöÊàë‰ª¨ËøôÈáåÁî®modelscopeÊãâÂèñ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b92cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from modelscope import snapshot_download\n",
    "import numpy as np\n",
    "from pymilvus import connections, Collection\n",
    "from arango import ArangoClient\n",
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "# ==========================================\n",
    "# ÈÖçÁΩÆÂå∫Âüü\n",
    "# ==========================================\n",
    "ZHIPU_API_KEY = \"\"\n",
    "\n",
    "# Milvus & ArangoDB (‰øùÊåÅ‰∏çÂèò)\n",
    "MILVUS_HOST = \"127.0.0.1\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "MILVUS_COLLECTION = \"rag_cluster_centroids\"\n",
    "ARANGO_URL = \"http://127.0.0.1:8529\"\n",
    "ARANGO_USER = \"root\"\n",
    "ARANGO_PASS = \"pass123\"\n",
    "ARANGO_DB_NAME = \"rag_db\"\n",
    "COL_CHUNKS = \"rag_chunks\"\n",
    "\n",
    "# ÊêúÁ¥¢ÂèÇÊï∞\n",
    "TOP_K_CLUSTERS = 3\n",
    "TOP_K_FINAL = 5\n",
    "\n",
    "# ==========================================\n",
    "# 1. ÂàùÂßãÂåñËµÑÊ∫ê (Êñ∞Â¢û Reranker Âä†ËΩΩ)\n",
    "# ==========================================\n",
    "class RerankerEngine:\n",
    "    def __init__(self):\n",
    "        print(\"Ê≠£Âú®‰∏ãËΩΩ/Âä†ËΩΩ BGE-Reranker Ê®°Âûã (Á∫¶1GB)...\")\n",
    "        # ‰ªé ModelScope ‰∏ãËΩΩÊ®°ÂûãÂà∞Êú¨Âú∞ÁºìÂ≠ò\n",
    "        model_dir = snapshot_download('Xorbits/bge-reranker-base', revision='master')\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        self.model.eval() # ËØÑ‰º∞Ê®°Âºè\n",
    "        \n",
    "        # Â¶ÇÊûúÊúâNÂç°Â∞±Áî®cudaÔºåÊ≤°ÊúâÂ∞±cpu\n",
    "        self.device = 'cpu' \n",
    "        self.model.to(self.device)\n",
    "        print(\"Reranker Ê®°ÂûãÂä†ËΩΩÂÆåÊàê ‚úÖ\")\n",
    "\n",
    "    def compute_score(self, query, candidates):\n",
    "        \"\"\"\n",
    "        ËÆ°ÁÆó (query, text) ÂØπÁöÑÁõ∏ÂÖ≥ÊÄßÂàÜÊï∞\n",
    "        \"\"\"\n",
    "        pairs = [[query, doc['text'][:512]] for doc in candidates] # Êà™Êñ≠‰∏Ä‰∏ãÈò≤Ê≠¢ÁàÜÊòæÂ≠ò\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            scores = self.model(**inputs, return_dict=True).logits.view(-1,).float()\n",
    "            \n",
    "        return scores.cpu().numpy()\n",
    "\n",
    "def init_resources():\n",
    "    # 1. Milvus & Arango \n",
    "    connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    milvus_col = Collection(MILVUS_COLLECTION)\n",
    "    milvus_col.load()\n",
    "    \n",
    "    client = ArangoClient(hosts=ARANGO_URL)\n",
    "    db = client.db(ARANGO_DB_NAME, username=ARANGO_USER, password=ARANGO_PASS)\n",
    "    \n",
    "    # 2. Embedding\n",
    "    embed_model = ZhipuAIEmbeddings(model=\"embedding-2\", api_key=ZHIPU_API_KEY)\n",
    "    \n",
    "    # 3. Reranker\n",
    "    reranker = RerankerEngine()\n",
    "    \n",
    "    return milvus_col, db, embed_model, reranker\n",
    "\n",
    "# ==========================================\n",
    "# 2. ËûçÂêàÊ£ÄÁ¥¢ (Â∏¶ Rerank)\n",
    "# ==========================================\n",
    "def fusion_search(query: str, milvus_col, db, embed_model, reranker):\n",
    "    print(f\"\\nüîé Áî®Êà∑ÊèêÈóÆ: „Äê{query}„Äë\")\n",
    "    \n",
    "    # --- Step 1 & 2: Á≤óÊéí (Routing) ---\n",
    "    print(\">>> 1. Milvus Ë∑ØÁî±...\")\n",
    "    q_vec = embed_model.embed_query(query)\n",
    "    \n",
    "    res = milvus_col.search(\n",
    "        data=[q_vec], anns_field=\"vector\", \n",
    "        param={\"metric_type\": \"COSINE\", \"params\": {\"ef\": 64}}, \n",
    "        limit=TOP_K_CLUSTERS, output_fields=[\"cluster_id\"]\n",
    "    )\n",
    "    target_ids = [hit.id for hit in res[0]]\n",
    "    \n",
    "    # --- Step 3: Âè¨Âõû (Retrieval) ---\n",
    "    print(f\">>> 2. ArangoDB Âè¨Âõû (Cluster IDs: {target_ids})...\")\n",
    "    aql = f\"\"\"\n",
    "    FOR c IN {COL_CHUNKS}\n",
    "        FILTER c.cluster_id IN @ids\n",
    "        RETURN {{ id: c._key, text: c.text, cluster_id: c.cluster_id }}\n",
    "    \"\"\"\n",
    "    candidates = list(db.aql.execute(aql, bind_vars={\"ids\": target_ids}))\n",
    "    print(f\"ÂÄôÈÄâÈõÜÊï∞Èáè: {len(candidates)}\")\n",
    "    \n",
    "    if not candidates: return []\n",
    "\n",
    "    # --- Step 4: Á≤æÊéí (Rerank) ---\n",
    "    print(f\">>> 3. BGE-Reranker Á≤æÊéí‰∏≠...\")\n",
    "    scores = reranker.compute_score(query, candidates)\n",
    "    \n",
    "    # ÊääÂàÜÊï∞ÂÜôÂõûÂéª\n",
    "    for i, doc in enumerate(candidates):\n",
    "        doc['score'] = float(scores[i])\n",
    "        \n",
    "    # ÊåâÂàÜÊï∞ÈôçÂ∫è\n",
    "    candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    final_results = candidates[:TOP_K_FINAL]\n",
    "\n",
    "    # --- Step 5: ‰∏ä‰∏ãÊñáÊâ©Â±ï ---\n",
    "    print(f\">>> 4. ÂõæË∞±‰∏ä‰∏ãÊñáÊâ©Â±ï...\")\n",
    "    top_doc = final_results[0]\n",
    "    context_aql = \"\"\"\n",
    "    FOR v, e, p IN 1..1 OUTBOUND @start_node rag_relations\n",
    "        FILTER e.type == 'NEXT_TO'\n",
    "        RETURN v.text\n",
    "    \"\"\"\n",
    "    start_node_id = f\"{COL_CHUNKS}/{top_doc['id']}\" \n",
    "    \n",
    "    next_texts = list(db.aql.execute(context_aql, bind_vars={\"start_node\": start_node_id}))\n",
    "    top_doc['next_text'] = next_texts[0] if next_texts else \"(Êó†ÂêéÊñá)\"\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# ==========================================\n",
    "# ‰∏ªÁ®ãÂ∫è\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    col, db, embed, rerank = init_resources()\n",
    "    \n",
    "    while True:\n",
    "        q = input(\"\\nËØ∑ËæìÂÖ•ÈóÆÈ¢ò (qÈÄÄÂá∫): \").strip()\n",
    "        if q == 'q': break\n",
    "        if not q: continue\n",
    "        \n",
    "        try:\n",
    "            results = fusion_search(q, col, db, embed, rerank)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            for i, res in enumerate(results):\n",
    "                print(f\"Rank {i+1} | Score: {res['score']:.4f} | Cluster: {res['cluster_id']}\")\n",
    "                print(f\"Content: {res['text'][:100]}...\") \n",
    "                if i == 0:\n",
    "                    print(f\"Context+: {res.get('next_text', '')[:100]}...\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "# ==========================================\n",
    "# ÈÖçÁΩÆ\n",
    "# ==========================================\n",
    "ZHIPU_API_KEY = \"\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "‰Ω†ÊòØ‰∏Ä‰∏™Âü∫‰∫éÂõæË∞±Â¢ûÂº∫ÁöÑÊô∫ËÉΩÂä©Êâã„ÄÇ‰Ω†ÁöÑ‰ªªÂä°ÊòØÊ†πÊçÆÊèê‰æõÁöÑ„ÄêÂèÇËÄÉ‰∏ä‰∏ãÊñá„ÄëÂõûÁ≠îÁî®Êà∑ÁöÑÈóÆÈ¢ò„ÄÇ\n",
    "ËØ∑Ê≥®ÊÑèÔºö\n",
    "1. ‰ªÖÊ†πÊçÆÂèÇËÄÉ‰ø°ÊÅØÂõûÁ≠îÔºå‰∏çË¶ÅÁºñÈÄ†„ÄÇ\n",
    "2. Â¶ÇÊûúÂèÇËÄÉ‰ø°ÊÅØÈáåÊ≤°ÊúâÁ≠îÊ°àÔºåËØ∑Áõ¥Êé•ËØ¥‚ÄúÊ†πÊçÆÁé∞ÊúâÊñáÊ°£Êó†Ê≥ïÂõûÁ≠î‚Äù„ÄÇ\n",
    "3. ÂõûÁ≠îË¶ÅÊù°ÁêÜÊ∏ÖÊô∞ÔºåÂ¶ÇÊûúÂèÇËÄÉ‰∫Ü‰∏ä‰∏ãÊñáÁöÑÊâ©Â±ïÂÜÖÂÆπÔºåËØ∑Ëá™ÁÑ∂Âú∞ËûçÂêàÂú®ÂõûÁ≠î‰∏≠„ÄÇ\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# RAG ÁîüÊàêÈÄªËæë\n",
    "# ==========================================\n",
    "def generate_answer(client, query, search_results):\n",
    "    \"\"\"\n",
    "    ÁªÑË£Ö Prompt Âπ∂Ë∞ÉÁî® GLM-4 ÁîüÊàêÂõûÁ≠î\n",
    "    \"\"\"\n",
    "    if not search_results:\n",
    "        return \"Êä±Ê≠âÔºåÁü•ËØÜÂ∫ì‰∏≠Ê≤°ÊúâÊâæÂà∞Áõ∏ÂÖ≥ÂÜÖÂÆπ„ÄÇ\"\n",
    "\n",
    "    # 1. ÊûÑÂª∫‰∏ä‰∏ãÊñá (Context)\n",
    "    # Êàë‰ª¨Âèñ Top-3 ÁöÑÁªìÊûúÔºåÂπ∂Êää Top-1 ÁöÑÊâ©Â±ï‰∏ä‰∏ãÊñá‰πüÊãºËøõÂéª\n",
    "    context_str = \"\"\n",
    "    \n",
    "    for i, res in enumerate(search_results[:3]):\n",
    "        context_str += f\"--- ÂèÇËÄÉÁâáÊÆµ {i+1} ---\\n\"\n",
    "        context_str += f\"{res['text']}\\n\"\n",
    "        \n",
    "        # Â¶ÇÊûúÊòØ Top-1 ‰∏îÊúâÊâ©Â±ïÂÜÖÂÆπÔºåÂä†‰∏äÂéª\n",
    "        if i == 0 and res.get('next_text') and res['next_text'] != \"(Êó†ÂêéÊñá)\":\n",
    "            context_str += f\"[ÂêéÁª≠ÂÜÖÂÆπ]: {res['next_text']}\\n\"\n",
    "    \n",
    "    # 2. ÁªÑË£ÖÊúÄÁªà Prompt\n",
    "    user_prompt = f\"\"\"\n",
    "    Áî®Êà∑ÈóÆÈ¢ò: {query}\n",
    "    \n",
    "    „ÄêÂèÇËÄÉ‰∏ä‰∏ãÊñá„Äë:\n",
    "    {context_str}\n",
    "    \n",
    "    ËØ∑Ê†πÊçÆ‰ª•‰∏ä‰ø°ÊÅØÂõûÁ≠îÁî®Êà∑ÈóÆÈ¢ò„ÄÇ\n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Ë∞ÉÁî®Â§ßÊ®°Âûã (GLM-4)\n",
    "    print(\">>> 5. Ê≠£Âú®ÊÄùËÄÉÂπ∂ÁîüÊàêÂõûÁ≠î (GLM-4)...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",  # ÊàñËÄÖ glm-4-flash (Êõ¥‰æøÂÆú)\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        stream=True, #‰ª•Ê≠§Ëé∑ÂæóÊâìÂ≠óÊú∫ÊïàÊûú\n",
    "    )\n",
    "    \n",
    "    # 4. ÊµÅÂºèËæìÂá∫\n",
    "    print(\"\\n\" + \"=\"*20 + \" AI ÂõûÁ≠î \" + \"=\"*20)\n",
    "    full_answer = \"\"\n",
    "    for chunk in response:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full_answer += content\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    return full_answer\n",
    "\n",
    "# ==========================================\n",
    "# ‰∏ªÁ®ãÂ∫è\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. ÂàùÂßãÂåñÊ£ÄÁ¥¢Âô® (Milvus, Arango, Reranker)\n",
    "    # Ê≥®ÊÑèÔºöËøôÊ≠•ÊØîËæÉÊÖ¢ÔºåÂõ†‰∏∫Ë¶ÅÂä†ËΩΩ Reranker Ê®°ÂûãÔºåËÄêÂøÉÁ≠âÂæÖ\n",
    "    print(\"Ê≠£Âú®ÂêØÂä® RAG Á≥ªÁªüÔºåÂä†ËΩΩÊ®°Âûã‰∏≠...\")\n",
    "    milvus_col, db, embed_model, reranker = init_resources()\n",
    "    \n",
    "    # 2. ÂàùÂßãÂåñ LLM ÂÆ¢Êà∑Á´Ø\n",
    "    llm_client = ZhipuAI(api_key=ZHIPU_API_KEY)\n",
    "    \n",
    "    print(\"\\n RAG Á≥ªÁªüÂ∑≤Â∞±Áª™ÔºÅ(ËæìÂÖ• 'q' ÈÄÄÂá∫)\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\n ËØ∑ËæìÂÖ•ÈóÆÈ¢ò: \").strip()\n",
    "        if query.lower() == 'q': break\n",
    "        if not query: continue\n",
    "        \n",
    "        try:\n",
    "            # Step A: Ê£ÄÁ¥¢ (Retrieval)\n",
    "            # Â§çÁî®‰πãÂâçÂÜôÂ•ΩÁöÑËûçÂêàÊ£ÄÁ¥¢ÂáΩÊï∞\n",
    "            results = fusion_search(query, milvus_col, db, embed_model, reranker)\n",
    "            \n",
    "            # Step B: ÁîüÊàê (Generation)\n",
    "            generate_answer(llm_client, query, results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" ÂèëÁîüÈîôËØØ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e11e8",
   "metadata": {},
   "source": [
    "## ÂèØËßÜÂåñÁïåÈù¢\n",
    "\n",
    "ËøêË°ågradioÁ®ãÂ∫èÔºåÂá∫Áé∞‰∫ÜÈóÆÈ¢òÔºåËØ∑Á¨¨‰∏ÄÊó∂Èó¥ÊÄÄÁñëgradioÁöÑÁâàÊú¨ÂíåpythonÁâàÊú¨ÂÜ≤Á™Å\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gradio==4.44.1\n",
    "! pip install --upgrade gradio gradio_client pydantic fastapi uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62238ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "# 1. ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè (Èò≤Ê≠¢‰ª£ÁêÜÊã¶Êà™ localhost)\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1,::1\"\n",
    "\n",
    "# 2. Â∞ùËØïÂØºÂÖ•Ê£ÄÁ¥¢Ê®°Âùó\n",
    "try:\n",
    "    print(\">>> Ê≠£Âú®Âä†ËΩΩÊ®°ÂûãÂíåÊï∞ÊçÆÂ∫ì...\")\n",
    "    milvus_col, db, embed_model, reranker = init_resources()\n",
    "    print(\" ËµÑÊ∫êÂä†ËΩΩÊàêÂäü\")\n",
    "except ImportError:\n",
    "    milvus_col = db = embed_model = reranker = None\n",
    "except Exception as e:\n",
    "    print(f\" ËµÑÊ∫êÂä†ËΩΩÂá∫Èîô: {e}\")\n",
    "    milvus_col = db = embed_model = reranker = None\n",
    "\n",
    "# 3. ÈÖçÁΩÆ API\n",
    "ZHIPU_API_KEY = \"\"\n",
    "llm_client = ZhipuAI(api_key=ZHIPU_API_KEY)\n",
    "\n",
    "# 4. Ê†∏ÂøÉÈÄªËæëÂáΩÊï∞ \n",
    "def chat_function(message, history):\n",
    "    \"\"\"\n",
    "    history Áé∞Âú®ÁöÑÊ†ºÂºèÊòØ: \n",
    "    [{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': 'hello'}]\n",
    "    \"\"\"\n",
    "    if not message:\n",
    "        yield history, \"\"\n",
    "        return\n",
    "    \n",
    "    # ËøΩÂä†Áî®Êà∑Ê∂àÊÅØ (Â≠óÂÖ∏Ê†ºÂºè)\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    yield history, \"Ê≠£Âú®ÊÄùËÄÉ...\"\n",
    "    \n",
    "    log_content = f\"üîé Áî®Êà∑ÊèêÈóÆ: {message}\\n\" + \"-\"*30 + \"\\n\"\n",
    "    \n",
    "    try:\n",
    "        # A. Ê£ÄÁ¥¢\n",
    "        results = []\n",
    "        if milvus_col:\n",
    "            log_content += \">>> Fusion Search...\\n\"\n",
    "            results = fusion_search(message, milvus_col, db, embed_model, reranker)\n",
    "            \n",
    "        if not results:\n",
    "            log_content += \"‚ö†Ô∏è Êú™Âè¨ÂõûÁõ∏ÂÖ≥ÂÜÖÂÆπ„ÄÇ\\n\"\n",
    "            context_str = \"\"\n",
    "        else:\n",
    "            # ËÆ∞ÂΩï Top-1\n",
    "            top1 = results[0]\n",
    "            log_content += f\"‚úÖ Top-1 Score: {top1.get('score', 0):.4f} | Cluster: {top1['cluster_id']}\\n\"\n",
    "            log_content += f\"üìÑ ÁâáÊÆµ: {top1['text'][:50]}...\\n\"\n",
    "            if top1.get('next_text'):\n",
    "                log_content += f\"üîó Êâ©Â±ï: {top1['next_text'][:30]}...\\n\"\n",
    "            \n",
    "            # ÊûÑÂª∫ Context\n",
    "            context_str = \"\"\n",
    "            for i, res in enumerate(results[:3]):\n",
    "                context_str += f\"ÂèÇËÄÉ {i+1}: {res['text']}\\n\"\n",
    "                if i == 0 and res.get('next_text'):\n",
    "                    context_str += f\"[‰∏ãÊñá]: {res['next_text']}\\n\"\n",
    "\n",
    "        # B. ÁîüÊàê\n",
    "        if context_str:\n",
    "            user_prompt = f\"Áî®Êà∑ÈóÆÈ¢ò: {message}\\n\\n„ÄêÂèÇËÄÉËµÑÊñô„Äë:\\n{context_str}\\n\\nËØ∑Ê†πÊçÆËµÑÊñôÂõûÁ≠î„ÄÇ\"\n",
    "        else:\n",
    "            user_prompt = message # Ê≤°Êü•Âà∞Â∞±Áõ¥Êé•ÈóÆ\n",
    "\n",
    "        log_content += \">>> GLM-4 Generating...\\n\"\n",
    "        \n",
    "        # ËøΩÂä†‰∏Ä‰∏™Á©∫ÁöÑ AI Ê∂àÊÅØÂç†‰ΩçÁ¨¶\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "        yield history, log_content\n",
    "        \n",
    "        response = llm_client.chat.completions.create(\n",
    "            model=\"glm-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"‰Ω†ÊòØ‰∏Ä‰∏™RAGÂä©Êâã„ÄÇ\"},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        partial_answer = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                partial_answer += chunk.choices[0].delta.content\n",
    "                # Êõ¥Êñ∞ÊúÄÂêé‰∏ÄÊù°Ê∂àÊÅØÁöÑÂÜÖÂÆπ\n",
    "                history[-1][\"content\"] = partial_answer\n",
    "                yield history, log_content\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error: {str(e)}\"\n",
    "        # Âá∫Èîô‰πüÊõ¥Êñ∞ËøõÁïåÈù¢\n",
    "        if history[-1][\"role\"] == \"assistant\":\n",
    "            history[-1][\"content\"] = error_msg\n",
    "        else:\n",
    "            history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "        yield history, log_content + \"\\n\" + error_msg\n",
    "\n",
    "# 5. ÊûÑÂª∫ÁïåÈù¢ÂØπË±°\n",
    "with gr.Blocks(title=\"FusionGraph RAG\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## üß† FusionGraph RAG\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(height=600, label=\"ÂØπËØù\")\n",
    "            msg = gr.Textbox(label=\"ÈóÆÈ¢ò\", placeholder=\"ËØ∑ËæìÂÖ•...\")\n",
    "            clear = gr.Button(\"Ê∏ÖÁ©∫\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            log_box = gr.TextArea(label=\"ÊÄùÁª¥ÈìæÊó•Âøó\", lines=25, interactive=False)\n",
    "\n",
    "    # ÁªëÂÆö‰∫ã‰ª∂\n",
    "    msg.submit(chat_function, [msg, chatbot], [chatbot, log_box])\n",
    "    # Ê∏ÖÁ©∫Êó∂ËøîÂõûÁ©∫ÂàóË°®\n",
    "    clear.click(lambda: [], None, chatbot, queue=False)\n",
    "\n",
    "# 6. ÂêØÂä®\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ÂêØÂä®‰∏≠...\")\n",
    "    demo.queue().launch(\n",
    "        server_name=\"127.0.0.1\", \n",
    "        server_port=9200, \n",
    "        share=False,\n",
    "        inbrowser=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
