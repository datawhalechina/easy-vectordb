# 精确检索算法：FLAT


FLAT算法代表了向量搜索中最直接了当的方法——暴力搜索。这种方式**不对向量进行压缩，采用暴力搜索（即穷举搜索）策略**，在每次查询时将目标向量与数据集中的所有向量进行逐一比较。这一特性使得FLAT能够保证100%的召回率，也是目前唯一能实现这一结果的算法。因此，FLAT算法在小规模数据集（通常为百万量级）或对检索精度有极高要求的场景中具有不可替代性，尤其适用于需要完美精确度的任务。

想象一下在一个巨大的图书馆里寻找特定书籍的场景。如果使用FLAT算法，就相当于不借助任何目录系统，从第一个书架开始，逐本检查每本书是否符合你的需求。这种方法虽然极其耗时，但却能保证你不会错过任何一本可能相关的书籍。

FLAT算法的核心价值在于它提供了绝对精确的结果。在某些对精度要求极高的场景中，这种保证是无可替代的。例如在医疗影像分析中，即使只有一个可疑病灶被漏检，都可能造成严重的临床后果。又如在法律文档检索中，任何相关判例的遗漏都可能影响案件的结果。

然而，这种精确性是以巨大的计算开销为代价的。当数据量增长时，FLAT算法的搜索时间呈线性增长，很快就会达到无法接受的程度。这就好比在一个拥有千万册藏书的图书馆中逐本翻阅——理论上可行，但实际上完全不现实。

FLAT算法最适合的是那些数据量不大但对精度要求极高的场景，不适合处理海量数据。一般来说，当向量数量在十万条以内时，FLAT算法还能提供相对可接受的性能。超过这个规模，就需要考虑其他更高效的算法了。在参数设置方面，FLAT算法较为简单，仅需指定距离计算方式（如L2欧氏距离或IP内积），且无需额外的训练过程。此外，FLAT算法的检索结果常被用作其他近似算法的性能比较基准，为评估不同算法的召回率提供参考标准。

总结一下上面的描述，可以总结为：FLAT算法是最简单直观的算法类型，它的工作原理就像在图书馆里一本一本地翻书：
```python
# 创建FLAT算法
index_params = {
    "index_type": "FLAT",
    "metric_type": "L2"
}
collection.create_index(field_name="embedding", index_params=index_params)
```
1. **穷举比较**：将查询向量与数据集中每一个向量进行逐一比较
2. **精确计算**：使用完整的向量数据计算距离，不做任何近似
3. **100%召回**：保证返回的结果是真正的最相似向量

| 特性      | 详细说明                              |
| ------- | --------------------------------- |
| 算法类型    | 精确检索算法，采用暴力搜索（穷举搜索）策略             |
| 核心原理    | 不对向量进行压缩，每次查询时将目标向量与数据集中所有向量逐一比较  |
| 召回率     | 100%，是目前唯一能实现这一结果的算法类型            |
| 适用场景    | 小规模数据集（通常为百万量级）或对检索精度有极高要求的场景     |
| 搜索速度    | 较慢（由于线性扫描本质）                      |
| 海量数据适用性 | 不适合处理海量数据                         |
| 参数设置    | 仅需指定距离计算方式（如L2欧氏距离或IP内积），无需额外训练过程 |
| 主要用途    | 精确检索任务、作为其他近似算法的性能比较基准            |

那什么时候用这个FLAT算法呢？

首先，当我们的数据量很小的时候，比如几千或者几万条数据，并且对精度要求极高的情况下，比如你客户突然发给你了他们的产品的一堆文档，让你做一个关于这个产品的客服，当然，如果你真的有这个需求的话，简单的加上这个FLAT算法是一个非常合适的选择，但不是你唯一需要做的，你需要考虑的主要是模型的回复限制，以及前期用户的输入问题的拦截，为什么要这样考虑呢，主要有这些顾虑：首先如果客户文档是政务类型的文档数据，你对其进行向量化后，选择FLAT算法进行暴力搜索，尽管我们可以获取到最精确的搜索结果，但你不能保证模型对于用户问题的理解是否完全正确，除此之外，你也不能完全的保证模型的输出是否存在部分幻觉内容，关于这个部分，最近的一篇论文中写道:
> **为什么模型会出现幻觉**
> 模型的幻觉就是在生成内容时产生与事实不符、缺乏依据或逻辑矛盾的输出。这可能是由于训练数据存在偏差、噪声，模型对复杂语义理解不充分，或者在推理过程中对不确定性的处理不当等原因导致的。例如在语言模型中，可能会编造出不存在的事件、人物或信息。
> 上面的这一行内容就是我让模型生成的，他说的对吗？在[为什么模型会出现幻觉](https://www.alphaxiv.org/abs/2509.04664)论文中，作者提到了模型出现幻觉的原因：模型之所以会产生幻觉，是因为**现有的训练和评估流程在奖励“猜测”，而不是“承认不确定性”。
> **具体来说：**
    1. 训练压力：模型的训练目标是预测最有可能的下一个词，这使得它们在面对不确定性时，倾向于生成一个看似合理但不一定正确的答案，**而不是直接表示“我不知道”**。
    2. 评估偏差：绝大多数的评测基准都采用二元评分，即答案“非对即错”。在这种模式下，模型给出正确答案得1分，而回答“我不知道”或给出错误答案都得0分。这就像一个学生在参加考试，当不确定答案时，猜测一个选项至少有蒙对的可能，而交白卷则肯定没分。为了在这些评测中获得高分，模型被优化得更倾向于“冒险猜测”。
> 因此，即使像我们设想的那样，使用FLAT算法提供了最精确的上下文信息，语言模型在综合这些信息并生成最终答案时，仍然可能因为其“考生心态”而产生幻觉。它可能会过度解读信息，或者在信息不足以回答问题时，选择编造一个看似连贯的答案，而不是坦诚地表示信息不足。

综上所述，我们不能盲目的选择FLAT算法，选择FLAT算法是确保RAG系统中“检索”环节准确无误的有效手段，但这只是构建可信赖AI系统的一部分。我们还必须关注“生成”环节的模型行为，一个完整的解决方案需要结合精确的检索工具和经过专门优化、鼓励表达不确定性的语言模型，以及在系统层面设计好多轮对话管理和**事实核验的机制**。
