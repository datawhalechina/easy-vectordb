# æ„å»ºä½ çš„å‘é‡æ•°æ®åº“ï¼ˆä»é›¶æ‰‹å†™å®ç° Mini Vector DBï¼‰

åœ¨å‰äº”ç« ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿå­¦ä¹ äº†å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒæ¦‚å¿µã€åº•å±‚ç®—æ³•å’ŒæŠ€æœ¯åŸç†ã€‚æœ¬ç« å°†èšç„¦â€œå®è·µâ€ï¼Œå¸¦é¢†å¤§å®¶ç”¨Pythonæ‰‹åŠ¨å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆå‘é‡æ•°æ®åº“ï¼Œç†è§£å…¶æ ¸å¿ƒå·¥ä½œæµç¨‹ã€‚é€šè¿‡æœ¬æ¬¡å®è·µï¼Œä½ å°†æŒæ¡å‘é‡å­˜å‚¨ã€ç´¢å¼•æ„å»ºã€ç›¸ä¼¼åº¦æ£€ç´¢ç­‰æ ¸å¿ƒåŠŸèƒ½çš„å®ç°é€»è¾‘ï¼Œä¸ºåç»­ä½¿ç”¨å·¥ä¸šçº§å‘é‡æ•°æ®åº“æ‰“ä¸‹åšå®åŸºç¡€ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
> æœ¬ç« ç›®æ ‡ï¼šæŒæ¡å‘é‡æ•°æ®åº“çš„åŸºæœ¬æ¶æ„ï¼Œå¹¶é€šè¿‡ Python æ‰‹å†™å®ç°ä¸€ä¸ªå¯æ‰§è¡Œçš„è¿·ä½ ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ï¼šå‘é‡å­˜å‚¨ã€ç›¸ä¼¼åº¦æœç´¢ã€IVF ä¸ HNSW ç®€æ˜“æ¨¡æ‹Ÿç»“æ„ã€‚

## 1.æ ¸å¿ƒç›®æ ‡ä¸æŠ€æœ¯é€‰å‹
### 1.1 å®ç°ç›®æ ‡

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½çš„å‘é‡æ•°æ®åº“åŸå‹ï¼š

- å‘é‡æ•°æ®çš„å¢åˆ æ”¹æŸ¥ï¼ˆåŸºç¡€æ•°æ®æ“ä½œï¼‰
- åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„ç²¾ç¡®æ£€ç´¢
- åŸºäºIVFï¼ˆå€’æ’æ–‡ä»¶ï¼‰çš„è¿‘ä¼¼æ£€ç´¢ï¼ˆæå‡æ£€ç´¢æ•ˆç‡ï¼‰
- å‘é‡æ•°æ®çš„æŒä¹…åŒ–å­˜å‚¨ï¼ˆé¿å…ç¨‹åºé€€å‡ºåæ•°æ®ä¸¢å¤±ï¼‰

### 1.2 æŠ€æœ¯é€‰å‹

ä¸ºç®€åŒ–å®ç°å¹¶èšç„¦æ ¸å¿ƒé€»è¾‘ï¼Œæˆ‘ä»¬é€‰ç”¨ä»¥ä¸‹è½»é‡çº§å·¥å…·åº“ï¼š

- numpyï¼šæ ¸å¿ƒæ•°å€¼è®¡ç®—åº“ï¼Œç”¨äºå‘é‡çš„å­˜å‚¨ã€è¿ç®—å’Œç›¸ä¼¼åº¦è®¡ç®—
- scikit-learnï¼šæä¾›KMeansèšç±»ç®—æ³•ï¼Œç”¨äºIVFç´¢å¼•çš„æ„å»ºï¼ˆå¯¹åº”Chapter 5çš„IVFç®—æ³•åŸç†ï¼‰
- pickleï¼šPythonå†…ç½®åºåˆ—åŒ–å·¥å…·ï¼Œç”¨äºå‘é‡æ•°æ®å’Œç´¢å¼•çš„æŒä¹…åŒ–å­˜å‚¨
- uuidï¼šç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºå‘é‡æ•°æ®çš„ä¸»é”®æ ‡è¯†
å®‰è£…å‘½ä»¤ï¼š
```bash
pip install numpy scikit-learn sentence-transformers modelscope
```

å‘é‡æ•°æ®åº“ä¸æ˜¯ä¸€ä¸ªâ€œç®—æ³•é›†åˆâ€ï¼Œè€Œæ˜¯ä¸€æ•´å¥—ç³»ç»Ÿï¼š

ğŸ§± å‘é‡æ•°æ®åº“æ ¸å¿ƒæ¨¡å—

| æ¨¡å— | ä½œç”¨ |
|------|------|
| **1. å‘é‡å­˜å‚¨ï¼ˆVector Storeï¼‰** | å­˜å‚¨æ‰€æœ‰å‘é‡ embeddingï¼ˆé€šå¸¸ä½¿ç”¨ NumPy / mmap / GPU bufferï¼‰ |
| **2. ç´¢å¼•ï¼ˆIndexï¼‰** | åŠ é€Ÿç›¸ä¼¼åº¦æœç´¢ï¼Œå¦‚ IVFã€HNSWã€PQã€LSH |
| **3. ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆMetricï¼‰** | è®¡ç®—å‘é‡ä¹‹é—´çš„è·ç¦»ï¼šCosine / L2 / Inner Product |
| **4. æŸ¥è¯¢å¼•æ“ï¼ˆQuery Engineï¼‰** | è´Ÿè´£æŸ¥è¯¢è·¯ç”±ï¼šé€‰æ‹©ç´¢å¼•ã€æ§åˆ¶ nprobeã€èšåˆå½©ç»“æœ |
| **5. å…ƒæ•°æ®ç®¡ç†ï¼ˆMetadataï¼‰** | ä¿å­˜å‘é‡å¯¹åº”çš„ IDã€æ–‡æœ¬å†…å®¹ã€æ ‡ç­¾ã€ä¸šåŠ¡å­—æ®µ |
| **6. æŒä¹…åŒ–ï¼ˆPersistenceï¼‰** | å°†å‘é‡ã€ç´¢å¼•ã€å…ƒæ•°æ®ä¿å­˜åˆ°ç£ç›˜ï¼Œæ”¯æŒ load/rebuild |

## 2.åˆ†æ­¥å®ç°å‘é‡æ•°æ®åº“
### 2.1åˆå§‹åŒ–æ•°æ®åº“ä¸æ•°æ®å­˜å‚¨å±‚
æ•°æ®å­˜å‚¨å±‚çš„æ ¸å¿ƒä½œç”¨æ˜¯ç®¡ç†åŸå§‹å‘é‡æ•°æ®åŠå¯¹åº”çš„å…ƒæ•°æ®ï¼ˆå¦‚å”¯ä¸€IDã€æè¿°ä¿¡æ¯ç­‰ï¼‰ã€‚æˆ‘ä»¬å°†ç”¨numpyæ•°ç»„å­˜å‚¨å‘é‡é›†åˆï¼Œç”¨å­—å…¸å»ºç«‹IDä¸å‘é‡ä¸‹æ ‡çš„æ˜ å°„ï¼ˆä¾¿äºå¿«é€Ÿå®šä½ï¼‰ã€‚
**ä»¥ä¸‹ä»£ç éœ€æ•´åˆåˆ°åŒä¸€ä¸ªç±»ä¸­**
```python
import numpy as np
import pickle
import uuid
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

class SimpleVectorDB:
    def __init__(self, vector_dim, db_path="vector_db.pkl"):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        :param vector_dim: å‘é‡ç»´åº¦ï¼ˆæ‰€æœ‰å…¥åº“å‘é‡å¿…é¡»ä¿æŒä¸€è‡´ï¼‰
        :param db_path: æ•°æ®æŒä¹…åŒ–å­˜å‚¨è·¯å¾„
        """
        self.vector_dim = vector_dim  # å‘é‡ç»´åº¦
        self.db_path = db_path        # æŒä¹…åŒ–è·¯å¾„
        
        # æ•°æ®å­˜å‚¨æ ¸å¿ƒç»“æ„
        self.vectors = np.array([])               # å­˜å‚¨æ‰€æœ‰å‘é‡ï¼ˆnumpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º[N, vector_dim]ï¼‰
        self.vector_ids = []                      # å­˜å‚¨å‘é‡å”¯ä¸€IDï¼Œä¸vectorsä¸‹æ ‡ä¸€ä¸€å¯¹åº”
        self.id_to_index = dict()                 # æ˜ å°„ï¼šå‘é‡ID â†’ å‘é‡åœ¨vectorsä¸­çš„ä¸‹æ ‡
        self.metadata = dict()                    # å­˜å‚¨å‘é‡å…ƒæ•°æ®ï¼ˆkey: å‘é‡ID, value: å…ƒæ•°æ®å­—å…¸ï¼‰
        
        # ç´¢å¼•ç›¸å…³ï¼ˆåç»­åˆå§‹åŒ–ï¼‰
        self.ivf_index = None                     # IVFç´¢å¼•ç»“æ„
        self.ivf_kmeans = None
        
    def _check_vector_dim(self, vector):
        """æ ¡éªŒå‘é‡ç»´åº¦æ˜¯å¦ç¬¦åˆè¦æ±‚"""
        if len(vector) != self.vector_dim:
            raise ValueError(f"å‘é‡ç»´åº¦é”™è¯¯ï¼Œéœ€ä¸º{self.vector_dim}ç»´ï¼Œå½“å‰ä¸º{len(vector)}ç»´")
    
```
### 2.2 å®ç°å¢åˆ æ”¹æŸ¥ API
è¿™éƒ¨åˆ†å°†å®ç°å‘é‡çš„æ’å…¥ã€æŸ¥è¯¢ã€æ›´æ–°å’Œåˆ é™¤åŠŸèƒ½ï¼Œæ˜¯æ•°æ®åº“æœ€åŸºç¡€çš„äº¤äº’èƒ½åŠ›ã€‚

```python
    def insert(self, vector, metadata=None):
        """
        æ’å…¥å‘é‡æ•°æ®
        :param vector: å‘é‡æ•°æ®ï¼ˆlistæˆ–numpy.ndarrayï¼‰
        :param metadata: å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼Œå¦‚æ–‡æœ¬æè¿°ã€æ¥æºç­‰ï¼‰
        :return: å‘é‡å”¯ä¸€IDï¼ˆç”¨äºåç»­æŸ¥è¯¢/æ›´æ–°ï¼‰
        """
        # ç±»å‹è½¬æ¢ä¸ç»´åº¦æ ¡éªŒ
        vector = np.array(vector, dtype=np.float32).flatten()
        self._check_vector_dim(vector)
        
        # ç”Ÿæˆå”¯ä¸€ID
        vector_id = str(uuid.uuid4())
        
        # æ’å…¥æ•°æ®
        if len(self.vectors) == 0:
            self.vectors = np.expand_dims(vector, axis=0)
        else:
            self.vectors = np.vstack([self.vectors, vector])
        self.vector_ids.append(vector_id)
        self.id_to_index[vector_id] = len(self.vector_ids) - 1
        if metadata:
            self.metadata[vector_id] = metadata
        
        return vector_id

    def get_by_id(self, vector_id):
        """é€šè¿‡IDæŸ¥è¯¢å‘é‡åŠå…ƒæ•°æ®"""
        if vector_id not in self.id_to_index:
            raise KeyError(f"æœªæ‰¾åˆ°IDä¸º{vector_id}çš„å‘é‡")
        index = self.id_to_index[vector_id]
        return {
            "vector_id": vector_id,
            "vector": self.vectors[index].tolist(),
            "metadata": self.metadata.get(vector_id, {})
        }

    def update(self, vector_id, new_vector=None, new_metadata=None):
        """æ›´æ–°å‘é‡æ•°æ®æˆ–å…ƒæ•°æ®"""
        if vector_id not in self.id_to_index:
            raise KeyError(f"æœªæ‰¾åˆ°IDä¸º{vector_id}çš„å‘é‡")
        index = self.id_to_index[vector_id]
        
        # æ›´æ–°å‘é‡ï¼ˆè‹¥æä¾›æ–°å‘é‡ï¼‰
        if new_vector is not None:
            new_vector = np.array(new_vector, dtype=np.float32).flatten()
            self._check_vector_dim(new_vector)
            self.vectors[index] = new_vector
        
        # æ›´æ–°å…ƒæ•°æ®ï¼ˆè‹¥æä¾›æ–°å…ƒæ•°æ®ï¼‰
        if new_metadata is not None:
            self.metadata[vector_id] = new_metadata

    def delete(self, vector_id):
        """åˆ é™¤å‘é‡æ•°æ®"""
        if vector_id not in self.id_to_index:
            raise KeyError(f"æœªæ‰¾åˆ°IDä¸º{vector_id}çš„å‘é‡")
        index = self.id_to_index[vector_id]
        
        # åˆ é™¤æ ¸å¿ƒæ•°æ®
        self.vectors = np.delete(self.vectors, index, axis=0)
        self.vector_ids.pop(index)
        del self.id_to_index[vector_id]
        if vector_id in self.metadata:
            del self.metadata[vector_id]
        
        # é‡æ–°æ„å»ºIDä¸ä¸‹æ ‡çš„æ˜ å°„ï¼ˆå› åˆ é™¤åä¸‹æ ‡å‘ç”Ÿå˜åŒ–ï¼‰
        self.id_to_index = {vid: idx for idx, vid in enumerate(self.vector_ids)}
    
```
### 2.3 å®ç°ç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆæš´åŠ› + IVFï¼‰
æ£€ç´¢æ˜¯å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒä»·å€¼ï¼Œæˆ‘ä»¬å°†å…ˆå®ç°ç²¾ç¡®æ£€ç´¢ï¼ˆæš´åŠ›æ£€ç´¢ï¼‰ï¼Œå†åŸºäºIVFç®—æ³•å®ç°è¿‘ä¼¼æ£€ç´¢ï¼Œå¯¹æ¯”ä¸¤è€…çš„æ•ˆç‡å·®å¼‚ã€‚

```python
def brute_force_search(self, query_vector, top_k=5):
        """
        æš´åŠ›æ£€ç´¢ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰ï¼šè®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        :param query_vector: æŸ¥è¯¢å‘é‡
        :param top_k: è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„å‰kä¸ªç»“æœ
        :return: æ£€ç´¢ç»“æœï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—ï¼‰
        """
        if len(self.vectors) == 0:
            return []
        
        # é¢„å¤„ç†æŸ¥è¯¢å‘é‡
        query_vector = np.array(query_vector, dtype=np.float32).flatten()
        self._check_vector_dim(query_vector)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆåˆ©ç”¨sklearnç®€åŒ–å®ç°ï¼Œä¹Ÿå¯æ‰‹åŠ¨å®ç°ï¼š(aÂ·b)/(||a||Â·||b||)ï¼‰
        similarities = cosine_similarity([query_vector], self.vectors)[0]
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åºï¼Œå–å‰top_k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # ç»„è£…ç»“æœ
        results = []
        for idx in top_indices:
            vector_id = self.vector_ids[idx]
            results.append({
                "vector_id": vector_id,
                "similarity": float(similarities[idx]),
                "vector": self.vectors[idx].tolist(),
                "metadata": self.metadata.get(vector_id, {})
            })
        return results

    def build_ivf_index(self, n_clusters=8):
        """
        æ„å»ºIVFç´¢å¼•ï¼ˆåŸºäºKMeansèšç±»ï¼‰
        æ ¸å¿ƒé€»è¾‘ï¼šå°†å‘é‡èšç±»åˆ°n_clustersä¸ªæ¡¶ä¸­ï¼Œæ£€ç´¢æ—¶å…ˆæ‰¾æŸ¥è¯¢å‘é‡æ‰€å±çš„æ¡¶ï¼Œå†åœ¨æ¡¶å†…æš´åŠ›æ£€ç´¢
        """
        if len(self.vectors) == 0:
            raise ValueError("æ•°æ®åº“ä¸­æ— å‘é‡æ•°æ®ï¼Œæ— æ³•æ„å»ºç´¢å¼•")
        
        # è½¬æ¢ä¸º float64
        vectors_for_kmeans = self.vectors.astype(np.float64)
        
        # KMeans èšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(vectors_for_kmeans)
        
        # æ„å»ºIVFç´¢å¼•
        self.ivf_index = {i: [] for i in range(n_clusters)}
        for idx, label in enumerate(cluster_labels):
            self.ivf_index[label].append(idx)
        
        self.ivf_kmeans = kmeans

    def ivf_search(self, query_vector, top_k=5):
        """åŸºäºIVFç´¢å¼•çš„è¿‘ä¼¼æ£€ç´¢"""
        if self.ivf_index is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨build_ivf_index()æ„å»ºIVFç´¢å¼•")
        if len(self.vectors) == 0:
            return []
        
        # 1. é¢„å¤„ç†æŸ¥è¯¢å‘é‡ï¼Œç¡®å®šå…¶æ‰€å±çš„èšç±»ï¼ˆæ¡¶ï¼‰
        query_vector = np.array(query_vector, dtype=np.float64).flatten()
        self._check_vector_dim(query_vector)
        cluster_id = self.ivf_kmeans.predict([query_vector])[0]
        
        # 2. è·å–è¯¥èšç±»ä¸‹çš„æ‰€æœ‰å‘é‡ä¸‹æ ‡ï¼Œæå–å¯¹åº”å‘é‡
        cluster_indices = self.ivf_index[cluster_id]
        if not cluster_indices:
            return []
        cluster_vectors = self.vectors[cluster_indices]
        
        # 3. åœ¨èšç±»å†…è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
        similarities = cosine_similarity([query_vector], cluster_vectors)[0]
        top_cluster_indices = np.argsort(similarities)[::-1][:top_k]
        
        # 4. ç»„è£…ç»“æœï¼ˆæ˜ å°„å›åŸæ•°æ®åº“çš„å‘é‡IDï¼‰
        results = []
        for idx in top_cluster_indices:
            original_idx = cluster_indices[idx]
            vector_id = self.vector_ids[original_idx]
            results.append({
                "vector_id": vector_id,
                "similarity": float(similarities[idx]),
                "vector": self.vectors[original_idx].tolist(),
                "metadata": self.metadata.get(vector_id, {}),
                "cluster_id": int(cluster_id)  # æ ‡æ³¨æ‰€å±èšç±»ï¼Œä¾¿äºç†è§£
            })
        return results
    
```

### 2.4 æ•°æ®æŒä¹…åŒ–
ä¸ºé¿å…ç¨‹åºé€€å‡ºåæ•°æ®ä¸¢å¤±ï¼Œæˆ‘ä»¬éœ€è¦å°†æ ¸å¿ƒæ•°æ®ç»“æ„åºåˆ—åŒ–åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œä¸‹æ¬¡å¯åŠ¨æ—¶å†åŠ è½½ã€‚

```python 
def save(self):
    """å°†æ•°æ®åº“æ•°æ®ä¸ç´¢å¼•æŒä¹…åŒ–åˆ°æœ¬åœ°æ–‡ä»¶"""
    data = {
        "vector_dim": self.vector_dim,
        "vectors": self.vectors,
        "vector_ids": self.vector_ids,
        "id_to_index": self.id_to_index,
        "metadata": self.metadata,
        "ivf_index": self.ivf_index,
        "ivf_kmeans": self.ivf_kmeans  # å­˜å‚¨èšç±»æ¨¡å‹
    }
    with open(self.db_path, "wb") as f:
        pickle.dump(data, f)
    print(f"æ•°æ®åº“å·²ä¿å­˜è‡³{self.db_path}")

@classmethod
def load(cls, db_path="vector_db.pkl"):
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®åº“"""
    with open(db_path, "rb") as f:
        data = pickle.load(f)
    
    # é‡å»ºæ•°æ®åº“å®ä¾‹
    db = cls(vector_dim=data["vector_dim"], db_path=db_path)
    db.vectors = data["vectors"]
    db.vector_ids = data["vector_ids"]
    db.id_to_index = data["id_to_index"]
    db.metadata = data["metadata"]
    db.ivf_index = data["ivf_index"]
    db.ivf_kmeans = data.get("ivf_kmeans")
    
    print(f"å·²ä»{db_path}åŠ è½½æ•°æ®åº“ï¼Œå…±åŒ…å«{len(db.vectors)}ä¸ªå‘é‡")
    return db
    
```

## 3.å®æˆ˜ï¼šä½¿ç”¨è‡ªå®šä¹‰å‘é‡æ•°æ®åº“å®ä¹ æ–‡æœ¬å‘é‡æ£€ç´¢
æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªâ€œæ–‡æœ¬å‘é‡æ£€ç´¢â€çš„å®ä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¸Šé¢å®ç°çš„å‘é‡æ•°æ®åº“ã€‚

æ–‡æœ¬å‘é‡ä½¿ç”¨GTEæ–‡æœ¬å‘é‡-ä¸­æ–‡-é€šç”¨é¢†åŸŸæ¨¡å‹ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ã€‚
éœ€è¦å®‰è£…çš„ç¬¬ä¸‰æ–¹åŒ…
```
pip install sentence-transformers modelscope
```
### 3.1 ä¸‹è½½æ¨¡å‹
é¦–å…ˆæ˜¯ä½¿ç”¨modelscope ä¸‹è½½å¯¹åº”çš„æ¨¡å‹
```python
#æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('iic/nlp_gte_sentence-embedding_chinese-base',cache_dir='./model')
```

### 3.2 åŠ è½½æ¨¡å‹
```python
#åŠ è½½æ¨¡å‹
import os
import jieba
from sentence_transformers import SentenceTransformer

def read_txt_and_split(file_path):
    """è¯»å–txtæ–‡ä»¶å†…å®¹å¹¶åˆ‡åˆ†ä¸ºå¥å­åˆ—è¡¨"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æŒ‡å®šçš„txtæ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
    # è¯»å–æ–‡ä»¶ï¼ˆé»˜è®¤UTF-8ç¼–ç ï¼Œè‹¥æœ‰ä¹±ç å¯å°è¯•gbkï¼‰
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    # å¥å­åˆ‡åˆ†
    sentences = content.split("ã€‚")
    print(f"æˆåŠŸè¯»å–txtæ–‡ä»¶ï¼Œå…±åˆ‡åˆ†å‡º {len(sentences)} ä¸ªå¥å­")
    return sentences

def text_to_vector(text, model):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼ˆåŸºäºSentenceTransformeræ¨¡å‹ï¼‰"""
    # encodeæ–¹æ³•ç›´æ¥è¿”å›å‘é‡ï¼Œconvert_to_numpy=Trueç¡®ä¿è¾“å‡ºä¸ºnumpyæ•°ç»„
    return model.encode(text, convert_to_numpy=True)

if __name__ == "__main__":
    # 1. é…ç½®å‚æ•°ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹txtæ–‡ä»¶è·¯å¾„å’Œæ¨¡å‹è·¯å¾„ï¼‰
    txt_file = "./data/Datawhaleç¤¾åŒºä»‹ç»å¤§æ¨¡å‹æ”¹å†™ç‰ˆ.txt"  # ä½ çš„txtæ–‡ä»¶è·¯å¾„
    model_dir = "./model/iic/nlp_gte_sentence-embedding_chinese-base"  # é¢„è®­ç»ƒæ¨¡å‹æœ¬åœ°å­˜å‚¨è·¯å¾„
    VECTOR_DIM = 768  # ä¸»æµä¸­æ–‡è¯­ä¹‰æ¨¡å‹è¾“å‡ºç»´åº¦å¤šä¸º768ï¼Œå¯æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´
    
    # 2. åŠ è½½ä¸­æ–‡è¯­ä¹‰å‘é‡æ¨¡å‹ï¼ˆä¸¤ç§åŠ è½½æ–¹å¼å¯é€‰ï¼‰
    print("æ­£åœ¨åŠ è½½ä¸­æ–‡æ–‡æœ¬åµŒå…¥æ¨¡å‹...")
    model = SentenceTransformer(model_dir)
    print(f"æˆåŠŸåŠ è½½æœ¬åœ°æ¨¡å‹ï¼š{model_dir}")

    # 3. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    db = SimpleVectorDB(vector_dim=VECTOR_DIM)
    
    # 4. è¯»å–txtæ–‡ä»¶å¹¶åˆ‡åˆ†å¥å­
    print(f"\næ­£åœ¨è¯»å–å¹¶å¤„ç†txtæ–‡ä»¶ï¼š{txt_file}")
    sentences = read_txt_and_split(txt_file)
    if not sentences:
        raise ValueError("æœªä»txtæ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆå¥å­")
    
    # 5. å¥å­è½¬å‘é‡å¹¶æ’å…¥æ•°æ®åº“ï¼ˆé™„å¸¦å…ƒæ•°æ®ï¼šåŸå§‹å¥å­ï¼‰
    print("\næ­£åœ¨å°†å¥å­è½¬å‘é‡å¹¶æ’å…¥æ•°æ®åº“...")
    embeddings = model.encode(sentences)  # æ‰¹é‡ç”Ÿæˆå‘é‡ï¼Œæ•ˆç‡æ›´é«˜
    for idx, (sentence, embedding) in enumerate(zip(sentences, embeddings), 1):
        # å…ƒæ•°æ®åŒ…å«å¥å­å†…å®¹å’Œåºå·ï¼Œä¾¿äºåç»­æŸ¥çœ‹
        metadata = {"sentence": sentence, "sequence": idx}
        db.insert(embedding, metadata=metadata)
    
    # 6. æŒä¹…åŒ–æ•°æ®åº“
    db.save()
    
    # 7. æµ‹è¯•æš´åŠ›æ£€ç´¢ï¼ˆæŸ¥è¯¢ä¸å‘é‡æ•°æ®åº“ç›¸å…³çš„å†…å®¹ï¼‰
    print("\n=== æš´åŠ›æ£€ç´¢ç»“æœï¼ˆæŸ¥è¯¢ï¼š'Datawhaleæœ‰å¤šä¸ªå­¦ä¹ è€…å‚ä¸æ´»åŠ¨'ï¼‰===")
    query_text = "Datawhaleæœ‰å¤šä¸ªå­¦ä¹ è€…å‚ä¸æ´»åŠ¨"
    query_vector = text_to_vector(query_text, model)
    brute_results = db.brute_force_search(query_vector, top_k=3)
    for res in brute_results:
        print(f"ç›¸ä¼¼åº¦ï¼š{res['similarity']:.4f} | å¥å­ï¼š{res['metadata']['sentence']}")
    
    # 8. æ„å»ºIVFç´¢å¼•å¹¶æµ‹è¯•è¿‘ä¼¼æ£€ç´¢
    print("\n=== IVFè¿‘ä¼¼æ£€ç´¢ç»“æœï¼ˆæŸ¥è¯¢ï¼š'Datawhaleæœ‰å¤šä¸ªå­¦ä¹ è€…å‚ä¸æ´»åŠ¨'ï¼‰===")
    # æ ¹æ®å¥å­æ•°é‡è°ƒæ•´èšç±»æ•°ï¼ˆä¸€èˆ¬ä¸ºæ•°æ®é‡çš„å¹³æ–¹æ ¹å·¦å³ï¼‰
    n_clusters = max(2, int(len(sentences)**0.5))
    db.build_ivf_index(n_clusters=n_clusters)
    ivf_results = db.ivf_search(query_vector, top_k=3)
    for res in ivf_results:
        print(f"ç›¸ä¼¼åº¦ï¼š{res['similarity']:.4f} | èšç±»IDï¼š{res['cluster_id']} | å¥å­ï¼š{res['metadata']['sentence']}")
    
    # 9. æµ‹è¯•æ•°æ®æ›´æ–°ä¸æŸ¥è¯¢
    print("\n=== æ•°æ®æ›´æ–°ä¸æŸ¥è¯¢æµ‹è¯• ===")
    # è·å–ç¬¬ä¸€ä¸ªå‘é‡çš„IDï¼ˆå³ç¬¬ä¸€ä¸ªå¥å­å¯¹åº”çš„å‘é‡ï¼‰
    first_vector_id = db.vector_ids[0]
    first_sentence = db.get_by_id(first_vector_id)['metadata']['sentence']
    print(f"å¾…æ›´æ–°çš„åŸå§‹å¥å­ï¼š{first_sentence}")
    # æ›´æ–°å…¶å…ƒæ•°æ®ï¼ˆæ¨¡æ‹Ÿå¥å­ä¿®æ­£ï¼‰
    new_metadata = {"sentence": f"ã€ä¿®æ­£ã€‘{first_sentence}", "sequence": 1, "updated": True}
    db.update(first_vector_id, new_metadata=new_metadata)
    # æŒ‰IDæŸ¥è¯¢æ›´æ–°ç»“æœ
    updated_res = db.get_by_id(first_vector_id)
    print(f"æ›´æ–°åçš„æ•°æ®ï¼š{updated_res['metadata']['sentence']}")
    
    # 10. æµ‹è¯•æ•°æ®åˆ é™¤
    db.delete(first_vector_id)
    print(f"\nåˆ é™¤åæ•°æ®åº“å‘é‡æ€»æ•°ï¼š{len(db.vectors)}")
    
    # 11. ä»æœ¬åœ°åŠ è½½æ•°æ®åº“éªŒè¯æŒä¹…åŒ–åŠŸèƒ½
    print("\n=== ä»æœ¬åœ°åŠ è½½æ•°æ®åº“ ===")
    loaded_db = SimpleVectorDB.load()
    print(f"åŠ è½½çš„æ•°æ®åº“å‘é‡æ€»æ•°ï¼š{len(loaded_db.vectors)}")
    # éªŒè¯åŠ è½½çš„æ•°æ®
    if loaded_db.vector_ids:
        sample_id = loaded_db.vector_ids[0]
        sample_data = loaded_db.get_by_id(sample_id)
        print(f"åŠ è½½æ•°æ®ç¤ºä¾‹ï¼š{sample_data['metadata']['sentence']}")
```
è¿è¡Œç»“æœï¼š
```
æ­£åœ¨åŠ è½½ä¸­æ–‡æ–‡æœ¬åµŒå…¥æ¨¡å‹...
No sentence-transformers model found with name ./model/iic/nlp_gte_sentence-embedding_chinese-base. Creating a new one with mean pooling.
æˆåŠŸåŠ è½½æœ¬åœ°æ¨¡å‹ï¼š./model/iic/nlp_gte_sentence-embedding_chinese-base

æ­£åœ¨è¯»å–å¹¶å¤„ç†txtæ–‡ä»¶ï¼š./data/Datawhaleç¤¾åŒºä»‹ç»å¤§æ¨¡å‹æ”¹å†™ç‰ˆ.txt
æˆåŠŸè¯»å–txtæ–‡ä»¶ï¼Œå…±åˆ‡åˆ†å‡º 68 ä¸ªå¥å­

æ­£åœ¨å°†å¥å­è½¬å‘é‡å¹¶æ’å…¥æ•°æ®åº“...
æ•°æ®åº“å·²ä¿å­˜è‡³vector_db.pkl

=== æš´åŠ›æ£€ç´¢ç»“æœï¼ˆæŸ¥è¯¢ï¼š'Datawhaleæœ‰å¤šä¸ªå­¦ä¹ è€…å‚ä¸æ´»åŠ¨'ï¼‰===
ç›¸ä¼¼åº¦ï¼š0.9302 | å¥å­ï¼š
Datawhale ç›®å‰æœ‰è¶…è¿‡å…­ä¸‡åæ ¸å¿ƒå­¦ä¹ è€…å’Œå¿—æ„¿è€…å‚ä¸ç¤¾åŒºæ´»åŠ¨
ç›¸ä¼¼åº¦ï¼š0.9168 | å¥å­ï¼š
Datawhale å°†å­¦ä¹ è€…ç»„ç»‡æˆä¸åŒå…´è¶£æ–¹å‘çš„å°ç»„
ç›¸ä¼¼åº¦ï¼š0.9065 | å¥å­ï¼š
Datawhale é€šè¿‡åä½œæœºåˆ¶å¸å¼•å¤§é‡å¿—æ„¿è€…å‚ä¸å­¦ä¹ å†…å®¹çš„å…±å»º

=== IVFè¿‘ä¼¼æ£€ç´¢ç»“æœï¼ˆæŸ¥è¯¢ï¼š'Datawhaleæœ‰å¤šä¸ªå­¦ä¹ è€…å‚ä¸æ´»åŠ¨'ï¼‰===
ç›¸ä¼¼åº¦ï¼š0.9302 | èšç±»IDï¼š7 | å¥å­ï¼š
Datawhale ç›®å‰æœ‰è¶…è¿‡å…­ä¸‡åæ ¸å¿ƒå­¦ä¹ è€…å’Œå¿—æ„¿è€…å‚ä¸ç¤¾åŒºæ´»åŠ¨
ç›¸ä¼¼åº¦ï¼š0.9017 | èšç±»IDï¼š7 | å¥å­ï¼š
Datawhale æ¯å¹´ä¸¾è¡Œè¶…è¿‡äºŒååœºæŠ€æœ¯ä¸»é¢˜åˆ†äº«
ç›¸ä¼¼åº¦ï¼š0.8981 | èšç±»IDï¼š7 | å¥å­ï¼š
Datawhale æ¯å¹´ä¸¾åŠåæ¬¡ä»¥ä¸Š AI ç¤¾åŒºæ´»åŠ¨

=== æ•°æ®æ›´æ–°ä¸æŸ¥è¯¢æµ‹è¯• ===
å¾…æ›´æ–°çš„åŸå§‹å¥å­ï¼šDatawhale æ˜¯ä¸€ä¸ªå¼€æºã€å¼€æ”¾ã€å…¬ç›Šçš„å­¦ä¹ å‹ç»„ç»‡
æ›´æ–°åçš„æ•°æ®ï¼šã€ä¿®æ­£ã€‘Datawhale æ˜¯ä¸€ä¸ªå¼€æºã€å¼€æ”¾ã€å…¬ç›Šçš„å­¦ä¹ å‹ç»„ç»‡

åˆ é™¤åæ•°æ®åº“å‘é‡æ€»æ•°ï¼š67

=== ä»æœ¬åœ°åŠ è½½æ•°æ®åº“ ===
å·²ä»vector_db.pklåŠ è½½æ•°æ®åº“ï¼Œå…±åŒ…å«68ä¸ªå‘é‡
åŠ è½½çš„æ•°æ®åº“å‘é‡æ€»æ•°ï¼š68
åŠ è½½æ•°æ®ç¤ºä¾‹ï¼šDatawhale æ˜¯ä¸€ä¸ªå¼€æºã€å¼€æ”¾ã€å…¬ç›Šçš„å­¦ä¹ å‹ç»„ç»‡
```

## 4.ç»“æœåˆ†æä¸æ‰©å±•æ–¹å‘
### 4.1 æ ¸å¿ƒåŠŸèƒ½éªŒè¯

è¿è¡Œä¸Šè¿°ä»£ç åï¼Œä½ å°†è§‚å¯Ÿåˆ°ï¼š

1. æš´åŠ›æ£€ç´¢èƒ½ç²¾å‡†è¿”å›ä¸æŸ¥è¯¢æ–‡æœ¬è¯­ä¹‰æœ€ç›¸å…³çš„ç»“æœï¼ˆç›¸ä¼¼åº¦æœ€é«˜ï¼‰ï¼›
2. IVFæ£€ç´¢ç»“æœä¸æš´åŠ›æ£€ç´¢æ¥è¿‘ï¼Œä½†é€Ÿåº¦æ›´å¿«ï¼ˆå°¤å…¶å½“å‘é‡æ•°é‡åºå¤§æ—¶ï¼Œä¼˜åŠ¿æ›´æ˜æ˜¾ï¼‰ï¼›
3. å¢åˆ æ”¹æŸ¥åŠæŒä¹…åŒ–åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼Œæ•°æ®å¯è·¨ç¨‹åºä¼šè¯ä¿ç•™ã€‚

### 4.2 ä¸å·¥ä¸šçº§å‘é‡æ•°æ®åº“çš„å·®è·

æˆ‘ä»¬çš„ç®€åŒ–ç‰ˆå®ç°ä»…åŒ…å«æ ¸å¿ƒé€»è¾‘ï¼Œå·¥ä¸šçº§å‘é‡æ•°æ®åº“ï¼ˆå¦‚Milvusã€Pineconeï¼‰è¿˜å…·å¤‡ä»¥ä¸‹é«˜çº§ç‰¹æ€§ï¼š

- åˆ†å¸ƒå¼å­˜å‚¨ä¸å¹¶è¡Œè®¡ç®—ï¼ˆæ”¯æŒäº¿çº§å‘é‡ï¼‰ï¼›
- æ›´å¤šç´¢å¼•ç®—æ³•ï¼ˆå¦‚HNSWã€PQçš„ä¼˜åŒ–å®ç°ï¼‰ï¼›
- å‘é‡ä¸ç»“æ„åŒ–æ•°æ®çš„æ··åˆæŸ¥è¯¢ï¼›
- é«˜å¯ç”¨ä¸å®¹é”™æœºåˆ¶ï¼ˆå‰¯æœ¬ã€å¤‡ä»½ï¼‰ï¼›
- RESTful APIæˆ–SDKå°è£…ï¼ˆä¾¿äºå¤šè¯­è¨€è°ƒç”¨ï¼‰ã€‚

### 4.3 æ‰©å±•æ–¹å‘ï¼ˆè¯¾åç»ƒä¹ ï¼‰

åŸºäºæœ¬å®ç°ï¼Œä½ å¯ä»¥å°è¯•æ‰©å±•ä»¥ä¸‹åŠŸèƒ½ï¼Œæ·±åŒ–ç†è§£ï¼š

1. å®ç°HNSWç´¢å¼•ï¼ˆå‚è€ƒChapter 5çš„HNSWç®—æ³•åŸç†ï¼Œç”¨å›¾ç»“æ„å­˜å‚¨å‘é‡é‚»å±…å…³ç³»ï¼‰ï¼›
2. æ·»åŠ å‘é‡ç»´åº¦å‹ç¼©åŠŸèƒ½ï¼ˆåŸºäºPQç®—æ³•ï¼Œå‡å°‘å­˜å‚¨å¼€é”€ï¼‰ï¼›
3. å®ç°æ‰¹é‡æ’å…¥/æ‰¹é‡æ£€ç´¢æ¥å£ï¼ˆæå‡æ•°æ®æ“ä½œæ•ˆç‡ï¼‰ï¼›
4. å¢åŠ å‘é‡è¿‡æ»¤åŠŸèƒ½ï¼ˆå¦‚æŒ‰å…ƒæ•°æ®ä¸­çš„å…³é”®è¯ç­›é€‰åå†æ£€ç´¢ï¼‰ã€‚

## 5.æ€»ç»“

æœ¬ç« é€šè¿‡Pythonæ‰‹åŠ¨å®ç°å‘é‡æ•°æ®åº“ï¼Œå°†å‰åºç« èŠ‚çš„ç†è®ºçŸ¥è¯†ï¼ˆå¦‚å‘é‡ç›¸ä¼¼åº¦ã€IVFç´¢å¼•ï¼‰è½åœ°ä¸ºå¯è¿è¡Œçš„ä»£ç ã€‚æ ¸å¿ƒæ”¶è·åŒ…æ‹¬ï¼š

- ç†è§£å‘é‡æ•°æ®åº“â€œå­˜å‚¨-ç´¢å¼•-æ£€ç´¢â€çš„æ ¸å¿ƒæµç¨‹ï¼›
- æŒæ¡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ã€IVFç´¢å¼•æ„å»ºçš„å®é™…ä»£ç å®ç°ï¼›
- æ˜ç¡®ç®€åŒ–ç‰ˆä¸å·¥ä¸šçº§å®ç°çš„å·®å¼‚ï¼Œä¸ºåç»­å­¦ä¹ å¥ å®šåŸºç¡€ã€‚

æ­å–œï¼ä½ å·²ç»æˆåŠŸå®Œæˆæœ¬ç« çš„å®è·µï¼Œå¹¶æˆåŠŸå®ç°äº†ä¸€ä¸ªå‘é‡æ•°æ®åº“ã€‚

