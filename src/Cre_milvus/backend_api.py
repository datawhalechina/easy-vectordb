from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import os
import yaml
import logging
from typing import List, Dict, Any, Optional
import time
from datetime import datetime
from pydantic import BaseModel
from contextlib import asynccontextmanager

from dataBuilder.chunking.meta_chunking import DependencyChecker
from dataBuilder.chunking.chunk_strategies import ChunkingManager

_app_initialized = False
_progress_tracker = None
_collection_manager = None
CHUNKING_AVAILABLE = True
chunking_manager = None
dependency_checker = None

# æ·»åŠ å¤„ç†é”ï¼Œé˜²æ­¢é‡å¤å¤„ç†
_processing_lock = {}
import threading
_lock_mutex = threading.Lock()

logger = logging.getLogger(__name__)
#å…¨å±€çŠ¶æ€
_progress_tracker = None
_collection_manager = None
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('back.log', encoding='utf-8')
    ]
)
def initialize_chunking_services():
    """åˆå§‹åŒ–åˆ†å—æœåŠ¡"""
    global chunking_manager, dependency_checker
    
    if not CHUNKING_AVAILABLE:
        logger.warning("åˆ†å—æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
        return False
    
    try:
        # åˆå§‹åŒ–ä¾èµ–æ£€æŸ¥å™¨
        from dataBuilder.chunking.meta_chunking import DependencyChecker
        dependency_checker = DependencyChecker()
        
        # åˆå§‹åŒ–åˆ†å—ç®¡ç†å™¨ï¼Œä¼ å…¥é…ç½®
        from dataBuilder.chunking.chunk_strategies import ChunkingManager
        chunking_manager = ChunkingManager(config=config)
        
        logger.info("åˆ†å—æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"PPLåˆ†å—å¯ç”¨æ€§: {dependency_checker.is_ppl_chunking_available()}")
        
        return True
        
    except Exception as e:
        logger.error(f"åˆ†å—æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ– - ä½¿ç”¨ç®€åŒ–ç»„ä»¶"""
    global _collection_manager, _progress_tracker, _app_initialized
    
    try:
        logger.info("=" * 50)
        logger.info("ğŸš€ LIFESPAN å‡½æ•°å·²è¢«è°ƒç”¨ - å¼€å§‹ç®€åŒ–åˆå§‹åŒ–")
        logger.info("=" * 50)
        
        # å¿«é€Ÿåˆå§‹åŒ–åŸºç¡€ç»„ä»¶
        _progress_tracker = InsertProgressTracker()
        _collection_manager = CollectionStateManager()
        
        # åˆå§‹åŒ–åˆ†å—æœåŠ¡
        if CHUNKING_AVAILABLE:
            initialize_chunking_services()
            logger.info("âœ… åˆ†å—æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        
        # ä½¿ç”¨ç®€åŒ–çš„é…ç½®åŠ è½½å™¨
        success = load_config()
        logger.info(f"ğŸ“ é…ç½®åŠ è½½: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
        
        # æ ‡è®°ä¸ºå·²åˆå§‹åŒ–ï¼Œå…è®¸APIå“åº”
        _app_initialized = True
        logger.info("âœ… åŸºç¡€ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼ŒAPIç°åœ¨å¯ä»¥å“åº”è¯·æ±‚")
        
        # åœ¨åå°å¼‚æ­¥åˆå§‹åŒ–è¿æ¥ï¼ˆä¸é˜»å¡APIå¯åŠ¨ï¼‰
        import asyncio
        # asyncio.create_task(background_initialize())
        
        logger.info("=" * 50)
        logger.info("âœ… ç³»ç»Ÿå¿«é€Ÿå¯åŠ¨å®Œæˆï¼è¿æ¥åˆå§‹åŒ–åœ¨åå°è¿›è¡Œ")
        logger.info("=" * 50)
        yield
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        _app_initialized = False
        yield

async def background_initialize():
    """åå°åˆå§‹åŒ–è¿æ¥ - ä½¿ç”¨ç®€åŒ–ç»„ä»¶"""
    try:
        logger.info("ğŸ”„ å¼€å§‹åå°è¿æ¥åˆå§‹åŒ–ï¼ˆä½¿ç”¨ç®€åŒ–ç»„ä»¶ï¼‰...")
        
        # ä½¿ç”¨ç®€åŒ–çš„è¿æ¥åˆå§‹åŒ–
        from config_loader import load_config
        from start_simple import connect_milvus
        
        # åŠ è½½é…ç½®
        config_data = load_config()
        logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
        # åˆå§‹åŒ–Milvusè¿æ¥ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        logger.info("ğŸ”— å¼€å§‹åˆå§‹åŒ–Milvusè¿æ¥ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰...")
        milvus_config = config_data.get("milvus", {})
        host = milvus_config.get("host", "localhost")
        port = int(milvus_config.get("port", 19530))
        success = connect_milvus(host, port)
        
        if success:
            logger.info("âœ… Milvusè¿æ¥åˆå§‹åŒ–æˆåŠŸï¼Œæ•°æ®æ’å…¥åŠŸèƒ½å·²å°±ç»ª")
        else:
            logger.warning("âš ï¸ Milvusè¿æ¥åˆå§‹åŒ–å¤±è´¥ï¼Œæ•°æ®æ’å…¥åŠŸèƒ½å¯èƒ½å—å½±å“")
        
        logger.info("âœ… åå°è¿æ¥åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ åå°è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


app = FastAPI(lifespan=lifespan)


app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

config = {}

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    global config
    try:
        with open('config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        config = {}
        return False

def save_config():
    """ä¿å­˜é…ç½®æ–‡ä»¶"""
    try:
        with open('config.yaml', 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        logger.info("é…ç½®æ–‡ä»¶ä¿å­˜æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
        return False
class CollectionStateManager:
    """é›†åˆçŠ¶æ€ç®¡ç†å™¨ - ä½¿ç”¨ç°æœ‰milvusBuilderç»„ä»¶"""
    
    def __init__(self):
        self._collection_states = {}
        self._state_lock = {}
        
    def _get_connection_alias(self) -> Optional[str]:
        """è·å–å½“å‰Milvusè¿æ¥åˆ«å"""
        try:
            # ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯é»˜è®¤è¿æ¥è€Œä¸æ˜¯åˆ«åï¼Œè¿™é‡Œè¿”å›é»˜è®¤è¿æ¥æ ‡è¯†
            from start_simple import is_milvus_connected
            if is_milvus_connected():
                return "default"  # ä½¿ç”¨é»˜è®¤è¿æ¥
            return None
        except Exception as e:
            logger.error(f"æ£€æŸ¥è¿æ¥çŠ¶æ€å¤±è´¥: {e}")
            print(f"æ£€æŸ¥è¿æ¥çŠ¶æ€å¤±è´¥: {e}")
            return None

    def _collection_exists(self, collection_name: str) -> bool:
        """æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨"""
        try:
            from pymilvus import utility
            connection_alias = self._get_connection_alias()
            if not connection_alias:
                return False
                
            return utility.has_collection(collection_name, using=connection_alias)
        except Exception as e:
            logger.error(f"æ£€æŸ¥é›†åˆ '{collection_name}' æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False

    def _is_collection_loaded(self, collection_name: str) -> bool:
        """æ£€æŸ¥é›†åˆæ˜¯å¦å·²åŠ è½½"""
        try:
            from pymilvus import utility
            connection_alias = self._get_connection_alias()
            if not connection_alias:
                return False
                
            load_state = utility.load_state(collection_name, using=connection_alias)
            return load_state.name == "Loaded"
        except Exception as e:
            logger.error(f"æ£€æŸ¥é›†åˆ '{collection_name}' åŠ è½½çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return False

    def _create_collection_if_needed(self, collection_name: str) -> bool:
        """æŒ‰éœ€åˆ›å»ºé›†åˆï¼ˆå ä½æ–¹æ³•ï¼Œå®é™…åˆ›å»ºåœ¨æ•°æ®æ’å…¥æ—¶å¤„ç†ï¼‰"""
        logger.info(f"é›†åˆ {collection_name} ä¸å­˜åœ¨ï¼Œå°†åœ¨æ•°æ®æ’å…¥æ—¶åˆ›å»º")
        return True

    def load_collection_with_retry(self, collection_name: str, max_retries: int = 3) -> bool:
        """é‡è¯•åŠ è½½é›†åˆ"""
        try:
            from pymilvus import Collection
            connection_alias = self._get_connection_alias()
            logger.info(f"load_collection_with_retryä½¿ç”¨çš„è¿æ¥åˆ«å: {connection_alias}")
            print(f"load_collection_with_retryä½¿ç”¨çš„è¿æ¥åˆ«å: {connection_alias}")
            if not connection_alias:
                logger.error("load_collection_with_retryæ— æ³•è·å–è¿æ¥åˆ«å")
                print("load_collection_with_retryæ— æ³•è·å–è¿æ¥åˆ«å")
                return False
                
            logger.info(f"åˆ›å»ºCollectionå¯¹è±¡: name={collection_name}, using={connection_alias}")
            print(f"åˆ›å»ºCollectionå¯¹è±¡: name={collection_name}, using={connection_alias}")
            
            # éªŒè¯è¿æ¥åˆ«åæ˜¯å¦æœ‰æ•ˆ
            try:
                utility.list_collections(using=connection_alias)
                logger.info(f"âœ… è¿æ¥åˆ«åéªŒè¯é€šè¿‡: {connection_alias}")
                print(f"âœ… è¿æ¥åˆ«åéªŒè¯é€šè¿‡: {connection_alias}")
            except Exception as e:
                logger.error(f"âŒ è¿æ¥åˆ«åéªŒè¯å¤±è´¥: {connection_alias}, é”™è¯¯: {e}")
                print(f"âŒ è¿æ¥åˆ«åéªŒè¯å¤±è´¥: {connection_alias}, é”™è¯¯: {e}")
            
            collection = Collection(name=collection_name, using=connection_alias)
            
            for attempt in range(max_retries):
                try:
                    logger.info(f"å°è¯•åŠ è½½é›†åˆ {collection_name} (ç¬¬{attempt + 1}æ¬¡)")
                    collection.load()
                    
                    # å…³é”®ä¿®å¤ï¼šç­‰å¾…é›†åˆåŠ è½½å®Œæˆ
                    utility.wait_for_loading_complete(collection_name, using=connection_alias, timeout=300)
                    logger.info(f"é›†åˆ {collection_name} åŠ è½½å®Œæˆç¡®è®¤")
                    
                    # éªŒè¯é›†åˆçŠ¶æ€
                    load_state = utility.load_state(collection_name, using=connection_alias)
                    if load_state != "Loaded":
                        raise Exception(f"é›†åˆåŠ è½½å¤±è´¥ï¼Œå½“å‰çŠ¶æ€: {load_state}")
                    
                    logger.info(f"é›†åˆçŠ¶æ€ç¡®è®¤: {load_state}")
                    logger.info(f"é›†åˆ {collection_name} åŠ è½½æˆåŠŸ")
                    return True
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = min((attempt + 1) * 2, 5)
                        logger.warning(f"åŠ è½½å¤±è´¥ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•: {e}")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"åŠ è½½é›†åˆ {collection_name} å¤±è´¥: {e}")
                        return False
                        
        except Exception as e:
            logger.error(f"åŠ è½½é›†åˆ {collection_name} æ—¶å‡ºé”™: {e}")
            return False

    def ensure_collection_loaded(self, collection_name: str) -> bool:
        """ç¡®ä¿é›†åˆå·²åŠ è½½"""
        try:
            # æ£€æŸ¥è¿æ¥
            connection_alias = self._get_connection_alias()
            if not connection_alias:
                logger.error("æ— æœ‰æ•ˆMilvusè¿æ¥")
                return False
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self._collection_exists(collection_name):
                logger.info(f"é›†åˆ {collection_name} ä¸å­˜åœ¨ï¼Œæ— éœ€åŠ è½½")
                return True  # ä¸å­˜åœ¨çš„é›†åˆä¸éœ€è¦åŠ è½½
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å·²åŠ è½½
            if not self._is_collection_loaded(collection_name):
                logger.info(f"é›†åˆ {collection_name} æœªåŠ è½½ï¼Œå¼€å§‹åŠ è½½")
                return self.load_collection_with_retry(collection_name)
            
            logger.info(f"é›†åˆ {collection_name} å·²åŠ è½½")
            return True
            
        except Exception as e:
            logger.error(f"ç¡®ä¿é›†åˆåŠ è½½å¤±è´¥: {e}")
            return False

    def get_collection_status(self, collection_name: str) -> Dict[str, Any]:
        """è·å–é›†åˆçŠ¶æ€ä¿¡æ¯"""
        try:
            from pymilvus import Collection, utility
            connection_alias = self._get_connection_alias()
            if not connection_alias:
                return {"status": "error", "msg": "æ— æœ‰æ•ˆè¿æ¥"}
            
            if not utility.has_collection(collection_name, using=connection_alias):
                return {
                    "status": "success",
                    "exists": False,
                    "loaded": False
                }
            
            collection = Collection(name=collection_name, using=connection_alias)
            is_loaded = self._is_collection_loaded(collection_name)
            
            return {
                "status": "success",
                "exists": True,
                "loaded": is_loaded,
                "num_entities": collection.num_entities
            }
            
        except Exception as e:
            logger.error(f"è·å–é›†åˆçŠ¶æ€å¤±è´¥: {e}")
            return {"status": "error", "msg": str(e)}

    def list_all_collections(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é›†åˆ"""
        try:
            from pymilvus import utility
            connection_alias = self._get_connection_alias()
            if not connection_alias:
                return []
                
            return utility.list_collections(using=connection_alias)
        except Exception as e:
            logger.error(f"åˆ—å‡ºé›†åˆå¤±è´¥: {e}")
            return []

    def release_collection(self, collection_name: str) -> bool:
        """é‡Šæ”¾é›†åˆ"""
        try:
            from pymilvus import Collection
            connection_alias = self._get_connection_alias()
            if not connection_alias:
                return False
                
            collection = Collection(name=collection_name, using=connection_alias)
            collection.release()
            logger.info(f"é›†åˆ {collection_name} å·²é‡Šæ”¾")
            return True
            
        except Exception as e:
            logger.error(f"é‡Šæ”¾é›†åˆ {collection_name} å¤±è´¥: {e}")
            return False

    def drop_collection(self, collection_name: str) -> bool:
        """åˆ é™¤é›†åˆ"""
        try:
            from pymilvus import utility
            connection_alias = self._get_connection_alias()
            if not connection_alias:
                return False
                
            if utility.has_collection(collection_name, using=connection_alias):
                utility.drop_collection(collection_name, using=connection_alias)
                logger.info(f"é›†åˆ {collection_name} å·²åˆ é™¤")
                return True
            else:
                logger.warning(f"é›†åˆ {collection_name} ä¸å­˜åœ¨")
                return True
                
        except Exception as e:
            logger.error(f"åˆ é™¤é›†åˆ {collection_name} å¤±è´¥: {e}")
            return False
# è¯·æ±‚æ¨¡å‹
class MilvusConfig(BaseModel):
    host: str
    port: str
    collection_name: str
    vector_name: str = "default"
    index_name: str = "IVF_FLAT"
    replica_num: int = 1
    index_device: str = "cpu"

class SystemConfig(BaseModel):
    url_split: bool = False

class ChunkingConfig(BaseModel):
    strategy: str = "fixed"
    chunk_size: int = 500
    chunk_overlap: int = 50

class SearchRequest(BaseModel):
    question: str  # æ”¹ä¸ºquestionåŒ¹é…å‰ç«¯
    col_choice: str = "hdbscan"
    collection_name: str = "Test_one"
    enable_visualization: bool = True
    top_k: int = 10
class InsertProgressTracker:
    """æ’å…¥è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self._progress_data = {}
        self._tracking_counter = 0
    
    def start_tracking(self, total_items: int) -> str:
        """å¼€å§‹è·Ÿè¸ªæ’å…¥è¿›åº¦"""
        self._tracking_counter += 1
        tracking_id = f"insert_{self._tracking_counter}_{int(time.time())}"
        
        self._progress_data[tracking_id] = {
            "tracking_id": tracking_id,
            "total_items": total_items,
            "processed_items": 0,
            "failed_items": 0,
            "start_time": datetime.now(),
            "estimated_completion": None,
            "current_status": "preparing",
            "error_details": [],
            "last_update": datetime.now()
        }
        
        logger.info(f"å¼€å§‹è·Ÿè¸ªæ’å…¥è¿›åº¦: {tracking_id}, æ€»é¡¹ç›®æ•°: {total_items}")
        return tracking_id
    
    def update_progress(self, tracking_id: str, processed: int, failed: int = 0, status: str = "inserting") -> None:
        """æ›´æ–°æ’å…¥è¿›åº¦"""
        if tracking_id not in self._progress_data:
            logger.warning(f"è·Ÿè¸ªID {tracking_id} ä¸å­˜åœ¨")
            return
        
        progress = self._progress_data[tracking_id]
        progress["processed_items"] = processed
        progress["failed_items"] = failed
        progress["current_status"] = status
        progress["last_update"] = datetime.now()
        
        # è®¡ç®—é¢„ä¼°å®Œæˆæ—¶é—´
        if processed > 0:
            elapsed_time = (datetime.now() - progress["start_time"]).total_seconds()
            items_per_second = processed / elapsed_time
            remaining_items = progress["total_items"] - processed
            
            if items_per_second > 0:
                from datetime import timedelta
                estimated_seconds = remaining_items / items_per_second
                progress["estimated_completion"] = datetime.now() + timedelta(seconds=estimated_seconds)
        
        logger.debug(f"æ›´æ–°è¿›åº¦ {tracking_id}: {processed}/{progress['total_items']} é¡¹å·²å¤„ç†")
    
    def get_progress_status(self, tracking_id: str) -> Dict[str, Any]:
        """è·å–æ’å…¥è¿›åº¦çŠ¶æ€"""
        if tracking_id not in self._progress_data:
            return {
                "error": "è·Ÿè¸ªIDä¸å­˜åœ¨",
                "status": "not_found"
            }
        
        progress = self._progress_data[tracking_id]
        
        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
        progress_percentage = 0
        if progress["total_items"] > 0:
            progress_percentage = (progress["processed_items"] / progress["total_items"]) * 100
        
        # è®¡ç®—å¤„ç†é€Ÿåº¦
        elapsed_time = (datetime.now() - progress["start_time"]).total_seconds()
        items_per_second = progress["processed_items"] / elapsed_time if elapsed_time > 0 else 0
        
        return {
            "tracking_id": tracking_id,
            "total_items": progress["total_items"],
            "processed_items": progress["processed_items"],
            "failed_items": progress["failed_items"],
            "progress_percentage": round(progress_percentage, 2),
            "current_status": progress["current_status"],
            "start_time": progress["start_time"].isoformat(),
            "last_update": progress["last_update"].isoformat(),
            "estimated_completion": progress["estimated_completion"].isoformat() if progress["estimated_completion"] else None,
            "items_per_second": round(items_per_second, 2),
            "error_details": progress["error_details"],
            "status": "active"
        }
    
    def finish_tracking(self, tracking_id: str, success: bool, final_message: str = "") -> Dict[str, Any]:
        """å®Œæˆæ’å…¥è¿›åº¦è·Ÿè¸ª"""
        if tracking_id not in self._progress_data:
            return {
                "error": "è·Ÿè¸ªIDä¸å­˜åœ¨",
                "status": "not_found"
            }
        
        progress = self._progress_data[tracking_id]
        progress["current_status"] = "completed" if success else "failed"
        progress["last_update"] = datetime.now()
        
        if final_message:
            progress["final_message"] = final_message
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        elapsed_time = (datetime.now() - progress["start_time"]).total_seconds()
        from datetime import timedelta
        final_stats = {
            "tracking_id": tracking_id,
            "success": success,
            "total_items": progress["total_items"],
            "processed_items": progress["processed_items"],
            "failed_items": progress["failed_items"],
            "success_rate": (progress["processed_items"] / progress["total_items"]) * 100 if progress["total_items"] > 0 else 0,
            "total_time_seconds": round(elapsed_time, 2),
            "average_items_per_second": round(progress["processed_items"] / elapsed_time, 2) if elapsed_time > 0 else 0,
            "final_message": final_message,
            "completed_at": datetime.now().isoformat()
        }
        
        logger.info(f"æ’å…¥è·Ÿè¸ªå®Œæˆ {tracking_id}: æˆåŠŸ={success}, å¤„ç†={progress['processed_items']}/{progress['total_items']}")
        
        return final_stats
    
    def add_error(self, tracking_id: str, error_message: str) -> None:
        """æ·»åŠ é”™è¯¯ä¿¡æ¯"""
        if tracking_id in self._progress_data:
            self._progress_data[tracking_id]["error_details"].append({
                "timestamp": datetime.now().isoformat(),
                "message": error_message
            })
    
    def cleanup_old_tracking(self, max_age_hours: int = 24) -> None:
        """æ¸…ç†æ—§çš„è·Ÿè¸ªæ•°æ®"""
        from datetime import timedelta
        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        
        to_remove = []
        for tracking_id, progress in self._progress_data.items():
            if progress["last_update"] < cutoff_time:
                to_remove.append(tracking_id)
        
        for tracking_id in to_remove:
            del self._progress_data[tracking_id]
            logger.info(f"æ¸…ç†æ—§çš„è·Ÿè¸ªæ•°æ®: {tracking_id}")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "Cre_milvus API", "status": "running", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "config_loaded": bool(config)
    }

@app.get("/config")
async def get_config():
    """è·å–å½“å‰é…ç½®"""
    return {"config": config}

@app.post("/config/milvus")
async def update_milvus_config(milvus_config: MilvusConfig):
    """æ›´æ–°Milvusé…ç½®"""
    try:
        if 'milvus' not in config:
            config['milvus'] = {}
        
        config['milvus'].update(milvus_config.dict())
        
        if save_config():
            logger.info("Milvusé…ç½®æ›´æ–°æˆåŠŸ")
            return {"success": True, "message": "Milvusé…ç½®æ›´æ–°æˆåŠŸ"}
        else:
            raise HTTPException(status_code=500, detail="é…ç½®ä¿å­˜å¤±è´¥")
            
    except Exception as e:
        logger.error(f"æ›´æ–°Milvusé…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/config/system")
async def update_system_config(system_config: SystemConfig):
    """æ›´æ–°ç³»ç»Ÿé…ç½®"""
    try:
        if 'system' not in config:
            config['system'] = {}
        
        config['system'].update(system_config.dict())
        
        if save_config():
            logger.info("ç³»ç»Ÿé…ç½®æ›´æ–°æˆåŠŸ")
            return {"success": True, "message": "ç³»ç»Ÿé…ç½®æ›´æ–°æˆåŠŸ"}
        else:
            raise HTTPException(status_code=500, detail="é…ç½®ä¿å­˜å¤±è´¥")
            
    except Exception as e:
        logger.error(f"æ›´æ–°ç³»ç»Ÿé…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/progress/{tracking_id}")
async def get_progress(tracking_id: str):
    """è·å–å¤„ç†è¿›åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    try:
        return {
            "tracking_id": tracking_id,
            "status": "completed",
            "progress_percentage": 100,
            "current_status": "completed",
            "processed_items": 1,
            "total_items": 1,
            "processing_time": 1.0,
            "message": "å¤„ç†å®Œæˆ"
        }
    except Exception as e:
        logger.error(f"è·å–è¿›åº¦å¤±è´¥: {e}")
        return {
            "status": "not_found",
            "error": str(e)
        }

@app.post("/update_config")
async def update_config(config_update: dict):
    """æ›´æ–°é…ç½®"""
    try:
        config.update(config_update)
        
        if save_config():
            logger.info("é…ç½®æ›´æ–°æˆåŠŸ")
            return {"success": True, "message": "é…ç½®æ›´æ–°æˆåŠŸ"}
        else:
            raise HTTPException(status_code=500, detail="é…ç½®ä¿å­˜å¤±è´¥")
            
    except Exception as e:
        logger.error(f"æ›´æ–°é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/config/chunking")
async def update_chunking_config(chunking_config: ChunkingConfig):
    """æ›´æ–°åˆ†å—é…ç½®"""
    try:
        if 'chunking' not in config:
            config['chunking'] = {}
        
        config['chunking'].update(chunking_config.dict())
        
        if save_config():
            logger.info("åˆ†å—é…ç½®æ›´æ–°æˆåŠŸ")
            return {"success": True, "message": "åˆ†å—é…ç½®æ›´æ–°æˆåŠŸ"}
        else:
            raise HTTPException(status_code=500, detail="é…ç½®ä¿å­˜å¤±è´¥")
            
    except Exception as e:
        logger.error(f"æ›´æ–°åˆ†å—é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload")
async def upload_file(file: UploadFile = File(...), folder_name: str = Form(None)):
    """æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†"""
    # ç®€å•çš„é‡å¤å¤„ç†æ£€æŸ¥
    folder_key = folder_name.strip() if folder_name and folder_name.strip() else "default"
    
    with _lock_mutex:
        if folder_key in _processing_lock:
            logger.warning(f"æ–‡ä»¶å¤¹ {folder_key} æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡é‡å¤è¯·æ±‚")
            return {
                "success": False,
                "message": f"æ–‡ä»¶å¤¹ {folder_key} æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆ",
                "status": "processing"
            }
        _processing_lock[folder_key] = True
    
    try:
        if folder_name and folder_name.strip():
            upload_dir = f"./data/upload/{folder_name.strip()}"
            logger.info(f"ä½¿ç”¨æŒ‡å®šæ–‡ä»¶å¤¹: {upload_dir}")
        else:
            upload_dir = "./data/upload"
            logger.info(f"ä½¿ç”¨é»˜è®¤æ–‡ä»¶å¤¹: {upload_dir}")
        os.makedirs(upload_dir, exist_ok=True)
        
        uploaded_files = []
        file_types = {}
        
        if file.filename:
            file_path = os.path.join(upload_dir, file.filename)
            
            content = await file.read()
            with open(file_path, "wb") as buffer:
                buffer.write(content)
            
            uploaded_files.append(file.filename)
            file_extension = os.path.splitext(file.filename)[1].lower()
            file_types[file_extension] = file_types.get(file_extension, 0) + 1
            
            logger.info(f"æ–‡ä»¶å·²ä¿å­˜: {file.filename}")
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ å®Œæˆ: {len(uploaded_files)} ä¸ªæ–‡ä»¶")
        
        try:
            logger.info("å¼€å§‹å‘é‡åŒ–å­˜å‚¨...")
            
            with open("config.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            
            insert_mode = config.get("system", {}).get("insert_mode", "overwrite")
            collection_name = config.get("milvus", {}).get("collection_name", "Test_one")
            
            if insert_mode == "append":
                logger.info(f"ä½¿ç”¨appendæ¨¡å¼ï¼Œæ£€æŸ¥é›†åˆ {collection_name} çŠ¶æ€")
                
                if _collection_manager:
                    collection_ready = _collection_manager.ensure_collection_loaded(collection_name)
                    if not collection_ready:
                        logger.error(f"é›†åˆ {collection_name} åŠ è½½å¤±è´¥")
                        return {
                            "message": f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œä½†é›†åˆåŠ è½½å¤±è´¥",
                            "files": uploaded_files,
                            "upload_dir": upload_dir,
                            "file_types": file_types,
                            "vectorized": False,
                            "error": f"é›†åˆ {collection_name} åŠ è½½å¤±è´¥",
                            "status": "partial_success"
                        }
                    else:
                        logger.info(f"é›†åˆ {collection_name} å·²å‡†å¤‡å°±ç»ª")
                else:
                    logger.warning("é›†åˆçŠ¶æ€ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            else:
                logger.info(f"ä½¿ç”¨overwriteæ¨¡å¼ï¼Œå°†é‡æ–°åˆ›å»ºé›†åˆ {collection_name}")
            
            if folder_name:
                if "data" not in config:
                    config["data"] = {}
                config["data"]["data_location"] = upload_dir
                logger.info(f"æ›´æ–°æ•°æ®è·¯å¾„ä¸º: {upload_dir}")
                
                with open("config.yaml", "w", encoding="utf-8") as f:
                    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            
            tracking_id = None
            if _progress_tracker:
                tracking_id = _progress_tracker.start_tracking(len(uploaded_files))
                logger.info(f"å¼€å§‹è·Ÿè¸ªå‘é‡åŒ–è¿›åº¦: {tracking_id}")
            
            # from System.new_start import fast_vector_database_build_from_config
            from System.start import Cre_VectorDataBaseStart_from_config
            start_time = time.time()
            
            if tracking_id:
                _progress_tracker.update_progress(tracking_id, 0, 0, "å¼€å§‹å‘é‡åŒ–å­˜å‚¨")
            
            try:
                # result = fast_vector_database_build_from_config(config)
                result = Cre_VectorDataBaseStart_from_config(config)
                end_time = time.time()
                
                logger.info(f"å‘é‡åŒ–å­˜å‚¨å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
                
                if tracking_id:
                    success = result.get("status") == "success"
                    final_message = f"å‘é‡åŒ–å­˜å‚¨{'æˆåŠŸ' if success else 'å¤±è´¥'}ï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’"
                    _progress_tracker.finish_tracking(tracking_id, success, final_message)
                    
            except Exception as build_error:
                end_time = time.time()
                logger.error(f"å‘é‡åŒ–æ„å»ºè¿‡ç¨‹å¤±è´¥: {build_error}")
                
                if tracking_id:
                    _progress_tracker.add_error(tracking_id, str(build_error))
                    _progress_tracker.finish_tracking(tracking_id, False, f"å‘é‡åŒ–æ„å»ºå¤±è´¥: {str(build_error)}")
                
                raise build_error
            
            if result.get("status") == "success":
                return {
                    "success": True,
                    "message": f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶å¹¶å®Œæˆå‘é‡åŒ–å­˜å‚¨",
                    "filename": file.filename,
                    "size": len(content),
                    "path": file_path,
                    "folder_name": folder_name,
                    "files": uploaded_files,
                    "upload_dir": upload_dir,
                    "file_types": file_types,
                    "vectorized": True,
                    "vectorization_result": result,
                    "processing_time": end_time - start_time,
                    "tracking_id": tracking_id,
                    "status": "success"
                }
            else:
                return {
                    "success": True,
                    "message": f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œä½†å‘é‡åŒ–å­˜å‚¨å¤±è´¥",
                    "filename": file.filename,
                    "size": len(content),
                    "path": file_path,
                    "folder_name": folder_name,
                    "files": uploaded_files,
                    "upload_dir": upload_dir,
                    "file_types": file_types,
                    "vectorized": False,
                    "error": result.get("msg", "æœªçŸ¥é”™è¯¯"),
                    "tracking_id": tracking_id,
                    "status": "partial_success"
                }
                
        except Exception as vector_error:
            logger.error(f"å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {vector_error}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            return {
                "success": True,
                "message": f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œä½†å‘é‡åŒ–å­˜å‚¨å¤±è´¥",
                "filename": file.filename,
                "size": len(content) if 'content' in locals() else 0,
                "path": file_path if 'file_path' in locals() else "",
                "folder_name": folder_name,
                "files": uploaded_files,
                "upload_dir": upload_dir,
                "file_types": file_types,
                "vectorized": False,
                "error": str(vector_error),
                "tracking_id": tracking_id if 'tracking_id' in locals() else None,
                "status": "partial_success"
            }
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
        )
    finally:
        # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½è¦é‡Šæ”¾é”
        with _lock_mutex:
            if folder_key in _processing_lock:
                del _processing_lock[folder_key]
                logger.info(f"é‡Šæ”¾æ–‡ä»¶å¤¹ {folder_key} çš„å¤„ç†é”")

@app.post("/search")
async def search(request: SearchRequest):
    """æœç´¢åŠŸèƒ½"""

    if not _app_initialized:
        raise HTTPException(
            status_code=503, 
            detail="æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·ç­‰å¾…åˆå§‹åŒ–å®Œæˆ"
        )
    
    try:
        # ç›´æ¥ä½¿ç”¨requestå¯¹è±¡ï¼Œä¸éœ€è¦å†æ¬¡è§£æJSON
        question = request.question
        col_choice = request.col_choice
        collection_name = request.collection_name
        enable_visualization = request.enable_visualization
        
        if not question:
            raise HTTPException(status_code=400, detail="é—®é¢˜ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"æ”¶åˆ°æœç´¢è¯·æ±‚: {question}, èšç±»æ–¹æ³•: {col_choice}")
        
        
        with open("config.yaml", "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        
        from System.start import Cre_Search
        
        start_time = time.time()
        result = Cre_Search(config, question)
        search_time = time.time() - start_time
        
        logger.info(f"åŸºç¡€æœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time:.2f}ç§’")
        
        if enable_visualization and "clusters" in result and result["clusters"]:
            try:
                from Search.clustering import create_clustering_service
                clustering_service = create_clustering_service()
                
                
                clusters = []
                for cluster_data in result["clusters"]:
                    from Search.clustering import Cluster, SearchResult
                    
                    documents = []
                    for doc in cluster_data.get("documents", []):
                        search_result = SearchResult(
                            id=str(doc.get("id", "")),
                            content=doc.get("content", ""),
                            url=doc.get("url"),
                            distance=float(doc.get("distance", 0.0)),
                            embedding=doc.get("embedding", []),
                            metadata=doc.get("metadata", {})
                        )
                        documents.append(search_result)
                    
                    cluster = Cluster(
                        cluster_id=cluster_data.get("cluster_id", 0),
                        documents=documents,
                        centroid=cluster_data.get("centroid"),
                        size=len(documents),
                        avg_distance=cluster_data.get("avg_distance", 0.0),
                        keywords=cluster_data.get("keywords", [])
                    )
                    clusters.append(cluster)
                
                viz_start_time = time.time()
                
                scatter_plot_data = clustering_service.create_cluster_scatter_plot(clusters)
                size_chart_data = clustering_service.create_cluster_size_chart(clusters)
                heatmap_data = clustering_service.create_cluster_heatmap(clusters)
                cluster_summary = clustering_service.generate_cluster_summary(clusters)
                cluster_metrics = clustering_service.calculate_cluster_metrics(clusters)
                
                viz_time = time.time() - viz_start_time
                logger.info(f"å¯è§†åŒ–æ•°æ®ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {viz_time:.2f}ç§’")
                
                # æ·»åŠ å¯è§†åŒ–æ•°æ®åˆ°ç»“æœä¸­
                result["visualization_data"] = {
                    "scatter_plot": scatter_plot_data,
                    "size_chart": size_chart_data,
                    "heatmap": heatmap_data,
                    "cluster_summary": cluster_summary,
                    "cluster_metrics": cluster_metrics
                }
                
                # æ›´æ–°æ‰§è¡Œæ—¶é—´
                result["execution_time"] = search_time + viz_time
                result["search_time"] = search_time
                result["visualization_time"] = viz_time
                
                logger.info(f"å¢å¼ºæœç´¢å®Œæˆï¼Œæ€»è€—æ—¶: {result['execution_time']:.2f}ç§’")
                
            except Exception as viz_error:
                logger.error(f"ç”Ÿæˆå¯è§†åŒ–æ•°æ®å¤±è´¥: {viz_error}")
                # å¯è§†åŒ–å¤±è´¥ä¸å½±å“åŸºç¡€æœç´¢ç»“æœ
                result["visualization_error"] = str(viz_error)
        
        # æ·»åŠ è´¨é‡æŒ‡æ ‡ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if "quality_metrics" not in result and "clusters" in result:
            try:
                result["quality_metrics"] = _calculate_search_quality_metrics(result)
            except Exception as e:
                logger.warning(f"è®¡ç®—è´¨é‡æŒ‡æ ‡å¤±è´¥: {e}")
        
        return result
        
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"æœç´¢å¤±è´¥: {str(e)}"
        )
        
def _calculate_search_quality_metrics(search_result: Dict[str, Any]) -> Dict[str, float]:
    """è®¡ç®—æœç´¢è´¨é‡æŒ‡æ ‡"""
    try:
        clusters = search_result.get("clusters", [])
        if not clusters:
            return {"relevance_score": 0.0, "diversity_score": 0.0, "coverage_score": 0.0}
        
        total_docs = sum(len(cluster.get("documents", [])) for cluster in clusters)
        if total_docs == 0:
            return {"relevance_score": 0.0, "diversity_score": 0.0, "coverage_score": 0.0}
        
        total_distance = 0
        for cluster in clusters:
            for doc in cluster.get("documents", []):
                total_distance += doc.get("distance", 1.0)
        
        avg_distance = total_distance / total_docs
        relevance_score = max(0, 1 - avg_distance)  
        
        
        num_clusters = len(clusters)
        if num_clusters <= 1:
            diversity_score = 0.0
        else:
            
            cluster_sizes = [len(cluster.get("documents", [])) for cluster in clusters]
            mean_size = sum(cluster_sizes) / len(cluster_sizes)
            variance = sum((size - mean_size) ** 2 for size in cluster_sizes) / len(cluster_sizes)
            std_dev = variance ** 0.5
            
            
            max_possible_std = mean_size * 0.5  
            diversity_score = max(0, 1 - (std_dev / max_possible_std)) if max_possible_std > 0 else 0
        
        
        coverage_ratio = num_clusters / total_docs if total_docs > 0 else 0
        coverage_score = min(1.0, coverage_ratio * 5)  
        
        return {
            "relevance_score": round(relevance_score, 3),
            "diversity_score": round(diversity_score, 3),
            "coverage_score": round(coverage_score, 3)
        }
        
    except Exception as e:
        logger.error(f"è®¡ç®—è´¨é‡æŒ‡æ ‡å¤±è´¥: {e}")
        return {"relevance_score": 0.0, "diversity_score": 0.0, "coverage_score": 0.0}
@app.get("/load-test/list")
async def list_load_tests():
    """åˆ—å‡ºæ‰€æœ‰å‹åŠ›æµ‹è¯•"""
    try:
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        tests = manager.list_active_tests()
        
        return {
            "status": "success",
            "tests": tests,
            "count": len(tests)
        }
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºæµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºæµ‹è¯•å¤±è´¥: {str(e)}")


# å‹æµ‹ç®¡ç†ç«¯ç‚¹
@app.post("/load-test/start")
async def start_load_test(request: Request):
    """å¯åŠ¨å‹åŠ›æµ‹è¯•"""
    try:
        data = await request.json()
        
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = manager.create_test_config(data)
        
        # å¯åŠ¨æµ‹è¯•
        test_id = manager.start_load_test(config)
        
        # è·å–Webç•Œé¢URL
        web_url = manager.get_locust_web_url(test_id)
        
        return {
            "status": "success",
            "test_id": test_id,
            "web_url": web_url,
            "message": "å‹åŠ›æµ‹è¯•å·²å¯åŠ¨"
        }
        
    except Exception as e:
        logger.error(f"å¯åŠ¨å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨å‹åŠ›æµ‹è¯•å¤±è´¥: {str(e)}")


@app.get("/load-test/status/{test_id}")
async def get_load_test_status(test_id: str):
    """è·å–å‹åŠ›æµ‹è¯•çŠ¶æ€"""
    try:
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        status = manager.get_test_status(test_id)
        
        if status:
            return {
                "status": "success",
                "test_status": status
            }
        else:
            raise HTTPException(status_code=404, detail="æµ‹è¯•ä¸å­˜åœ¨")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æµ‹è¯•çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æµ‹è¯•çŠ¶æ€å¤±è´¥: {str(e)}")


@app.post("/load-test/stop/{test_id}")
async def stop_load_test(test_id: str):
    """åœæ­¢å‹åŠ›æµ‹è¯•"""
    try:
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        success = manager.stop_test(test_id)
        
        if success:
            return {
                "status": "success",
                "message": "å‹åŠ›æµ‹è¯•å·²åœæ­¢"
            }
        else:
            raise HTTPException(status_code=404, detail="æµ‹è¯•ä¸å­˜åœ¨æˆ–å·²åœæ­¢")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åœæ­¢æµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢æµ‹è¯•å¤±è´¥: {str(e)}")


@app.get("/load-test/web-url/{test_id}")
async def get_load_test_web_url(test_id: str):
    """è·å–Locust Webç•Œé¢URL"""
    try:
        from testing.locust_manager import create_locust_test_manager
        manager = create_locust_test_manager()
        
        web_url = manager.get_locust_web_url(test_id)
        
        if web_url:
            return {
                "status": "success",
                "web_url": web_url
            }
        else:
            raise HTTPException(status_code=404, detail="æµ‹è¯•ä¸å­˜åœ¨æˆ–Webç•Œé¢ä¸å¯ç”¨")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–Webç•Œé¢URLå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–Webç•Œé¢URLå¤±è´¥: {str(e)}")


@app.get("/load-test/history")
async def get_load_test_history():
    """è·å–æµ‹è¯•å†å²è®°å½•ï¼ˆåŒ…æ‹¬å·²å®Œæˆçš„æµ‹è¯•ï¼‰"""
    try:
        import os
        import json
        from datetime import datetime
        
        history_tests = []
        results_dir = "test_results"
        
        if os.path.exists(results_dir):
            # è¯»å–æ‰€æœ‰æµ‹è¯•ç»“æœæ–‡ä»¶
            for file in os.listdir(results_dir):
                if file.startswith("test_") and file.endswith(".json"):
                    try:
                        file_path = os.path.join(results_dir, file)
                        if os.path.getsize(file_path) > 0:  # ç¡®ä¿æ–‡ä»¶ä¸ä¸ºç©º
                            with open(file_path, 'r', encoding='utf-8') as f:
                                test_data = json.load(f)
                                # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                                test_data['file_name'] = file
                                test_data['file_path'] = file_path
                                history_tests.append(test_data)
                    except Exception as e:
                        logger.warning(f"è¯»å–æµ‹è¯•å†å²æ–‡ä»¶å¤±è´¥ {file}: {e}")
                        continue
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        history_tests.sort(
            key=lambda x: x.get('start_time', ''), 
            reverse=True
        )
        
        return {
            "status": "success",
            "tests": history_tests,
            "count": len(history_tests)
        }
        
    except Exception as e:
        logger.error(f"è·å–æµ‹è¯•å†å²å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æµ‹è¯•å†å²å¤±è´¥: {str(e)}")


@app.delete("/load-test/history/{test_id}")
async def delete_test_history(test_id: str):
    """åˆ é™¤æŒ‡å®šçš„æµ‹è¯•å†å²è®°å½•"""
    try:
        import os
        import glob
        
        deleted_files = []
        results_dir = "test_results"
        
        if os.path.exists(results_dir):
            # æŸ¥æ‰¾ä¸è¯¥æµ‹è¯•ç›¸å…³çš„æ‰€æœ‰æ–‡ä»¶
            patterns = [
                f"test_{test_id}*.json",
                f"test_{test_id}*.csv",
                f"test_{test_id}*.html"
            ]
            
            for pattern in patterns:
                for file_path in glob.glob(os.path.join(results_dir, pattern)):
                    try:
                        os.remove(file_path)
                        deleted_files.append(file_path)
                    except Exception as e:
                        logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return {
            "status": "success",
            "message": f"å·²åˆ é™¤æµ‹è¯• {test_id} çš„ç›¸å…³æ–‡ä»¶",
            "deleted_files": deleted_files,
            "count": len(deleted_files)
        }
        
    except Exception as e:
        logger.error(f"åˆ é™¤æµ‹è¯•å†å²å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æµ‹è¯•å†å²å¤±è´¥: {str(e)}")

@app.post("/visualization")
async def get_visualization_data(request: Request):
    """
    è·å–å¯è§†åŒ–æ•°æ®
    """
    try:
        data = await request.json()
        collection_name = data.get("collection_name", "")
        
        if not collection_name:
            raise HTTPException(status_code=400, detail="é›†åˆåç§°ä¸èƒ½ä¸ºç©º")
        
        logger.info(f"æ”¶åˆ°å¯è§†åŒ–è¯·æ±‚: {collection_name}")
        
        # å°è¯•è·å–å¯è§†åŒ–æ•°æ®
        try:
            from ColBuilder.visualization import get_all_embeddings_and_texts
            import hdbscan
            from umap import UMAP
            import pandas as pd
            import numpy as np
            
            # è·å–æ•°æ®
            from Search.milvusSer import search_vectors
            search_results = search_vectors(collection_name, query_vector=None, limit=1000)
            embeddings = [result['embedding'] for result in search_results]
            texts = [result['content'] for result in search_results]
            ids = [result['id'] for result in search_results]
            distances = [result['distance'] for result in search_results]
            urls = [result.get('metadata', {}).get('url') for result in search_results]
            
            if not embeddings:
                return []
            
            # UMAPé™ç»´
            umap_model = UMAP(n_components=2, random_state=42)
            embeddings_2d = umap_model.fit_transform(np.array(embeddings))
            
            # ä½¿ç”¨ClusteringServiceè¿›è¡Œèšç±»
            from Search.clustering import ClusteringService,SearchResult
            service = ClusteringService()
            
            search_results = [
                SearchResult(
                    id = str(ids[i]),
                    content = texts[i],
                    url = urls[i],
                    distance = distances[i],
                    embedding = embeddings[i],
                    metadata = None
                ) for i in range(len(embeddings))
            ]
            
            # æ‰§è¡Œèšç±»
            clusters = service.cluster_search_results(search_results)
            
            # æ„å»ºå¯è§†åŒ–æ•°æ®ç»“æ„
            result = []
            for i, (x, y) in enumerate(embeddings_2d):
                doc_id = ids[i] if i < len(ids) else i
                cluster_info = next(
                    (c for c in clusters if any(doc.id == doc_id for doc in c.documents)),
                    None
                )
                
                result.append({
                    "x": float(x),
                    "y": float(y),
                    "cluster": cluster_info.cluster_id if cluster_info else -1,
                    "text": texts[i][:100] if i < len(texts) else "",
                    "id": doc_id,
                    "keywords": cluster_info.keywords if cluster_info else [],
                    "representative": cluster_info.representative_doc.id if cluster_info else None
                })
            
            return result
            
        except ImportError as e:
            logger.warning(f"å¯è§†åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            return []
        except Exception as e:
            logger.error(f"å¯è§†åŒ–æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return []
        
    except Exception as e:
        logger.error(f"å¯è§†åŒ–è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"å¯è§†åŒ–è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"
        )

@app.get("/llm/configs")
async def get_llm_configs():
    """è·å–LLMé…ç½®åˆ—è¡¨"""
    try:
        # è¿™é‡Œåº”è¯¥ä»é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“ä¸­è¯»å–LLMé…ç½®
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
        configs = {}
        active_config = None
        
        # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
        try:
            with open("config.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
                llm_configs = config.get("llm_configs", {})
                active_config_id = config.get("active_llm_config")
                
                for config_id, config_data in llm_configs.items():
                    configs[config_id] = {
                        "provider": config_data.get("provider"),
                        "model_name": config_data.get("model_name"),
                        "api_endpoint": config_data.get("api_endpoint")
                    }
                
                if active_config_id and active_config_id in configs:
                    active_config = {
                        "id": active_config_id,
                        **configs[active_config_id]
                    }
        except:
            pass
        
        return {
            "configs": configs,
            "summary": {
                "total_configs": len(configs),
                "active_config": active_config
            },
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"è·å–LLMé…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–LLMé…ç½®å¤±è´¥: {str(e)}"
        )

@app.get("/chunking/strategies")
async def get_chunking_strategies():
    """è·å–åˆ†å—ç­–ç•¥"""
    try:
        # å°è¯•ä»åˆ†å—æ¨¡å—è·å–ç­–ç•¥åˆ—è¡¨
        try:
            from dataBuilder.chunking.chunk_strategies import get_available_strategies
            strategies = get_available_strategies()

            # æ£€æŸ¥GLMé…ç½®çŠ¶æ€ï¼Œå½±å“é«˜çº§ç­–ç•¥çš„å¯ç”¨æ€§
            # glm_configured = False
            # try:
            #     active_llm = config.get('active_llm_config', 'glm_default')
            #     llm_configs = config.get('llm_configs', {})
            #     if active_llm in llm_configs:
            #         api_key = llm_configs[active_llm].get('api_key', '')
            #         glm_configured = bool(api_key)
            # except Exception:
            #     pass
            
            # ä¸ºæ¯ä¸ªç­–ç•¥æ·»åŠ å¯ç”¨æ€§ä¿¡æ¯
            # for strategy in strategies:
            #     strategy_name = strategy.get("name", "")
            #     if strategy_name in ["meta_ppl", "margin_sampling", "msp"]:
            #         strategy["requires_glm"] = True
            #         strategy["available"] = glm_configured
            #         if not glm_configured:
            #             strategy["unavailable_reason"] = "éœ€è¦é…ç½®GLM-4.5-flashæ¨¡å‹"
            #     else:
            #         strategy["requires_glm"] = False
            #         strategy["available"] = True
            # æ‰€æœ‰ç­–ç•¥éƒ½å¯ç”¨ï¼Œä¸å†ä¾èµ–GLM
            for strategy in strategies:
                strategy["requires_glm"] = False
                strategy["available"] = True
            return {
                "strategies": strategies,
                # "glm_configured": glm_configured,
                "total_strategies": len(strategies),
                "available_strategies": len([s for s in strategies if s.get("available", True)])
            }
            
        except ImportError:
            # å¦‚æœåˆ†å—æ¨¡å—ä¸å¯ç”¨ï¼Œè¿”å›åŸºç¡€ç­–ç•¥
            logger.warning("åˆ†å—ç­–ç•¥æ¨¡å—ä¸å¯ç”¨ï¼Œè¿”å›åŸºç¡€ç­–ç•¥åˆ—è¡¨")
            strategies = [
                {
                    "name": "traditional",
                    "display_name": "ä¼ ç»Ÿå›ºå®šåˆ‡åˆ†",
                    "description": "åŸºäºå›ºå®šé•¿åº¦å’Œé‡å çš„ä¼ ç»Ÿåˆ‡åˆ†æ–¹æ³•",
                    "requires_glm": False,
                    "available": True,
                    "default_params": {
                        "chunk_size": 512,
                        "overlap": 50
                    }
                }
            ]
            
            return {
                "strategies": strategies,
                # "glm_configured": False,
                "total_strategies": len(strategies),
                "available_strategies": len(strategies),
                "warning": "é«˜çº§åˆ†å—ç­–ç•¥æ¨¡å—ä¸å¯ç”¨"
            }
            
    except Exception as e:
        logger.error(f"è·å–åˆ†å—ç­–ç•¥å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–åˆ†å—ç­–ç•¥å¤±è´¥: {str(e)}"
        )
import re

def normalize_params(strategy: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ ‡å‡†åŒ–å‚æ•°åç§°ï¼Œç¡®ä¿å‰ç«¯å‚æ•°ä¸åç«¯æœŸæœ›çš„å‚æ•°åç§°ä¸€è‡´
    
    å‚æ•°:
        strategy: åˆ†å—ç­–ç•¥åç§°
        params: åŸå§‹å‚æ•°å­—å…¸
    
    è¿”å›:
        æ ‡å‡†åŒ–åçš„å‚æ•°å­—å…¸
    """
    normalized = params.copy()
    normalized.pop('strategy', None)  # ç¡®ä¿ä¸ä¼šåŒ…å« strategy
    
    # ç»Ÿä¸€ chunk_size å’Œ chunk_length å‚æ•°
    if 'chunk_size' in normalized and 'chunk_length' not in normalized:
        normalized['chunk_length'] = normalized['chunk_size']
    
    # PPL ç­–ç•¥å‚æ•°æ˜ å°„: threshold -> ppl_threshold
    if strategy == "meta_ppl":
        if 'threshold' in normalized and 'ppl_threshold' not in normalized:
            normalized['ppl_threshold'] = normalized['threshold']
    
    # ä¸ºé«˜çº§ç­–ç•¥æ·»åŠ é»˜è®¤çš„ chunk_length å‚æ•°
    if strategy in ["margin_sampling", "msp"] and 'chunk_length' not in normalized:
        normalized['chunk_length'] = normalized.get('chunk_size', 512)
    
    # ç¡®ä¿è¯­è¨€å‚æ•°å­˜åœ¨
    if 'language' not in normalized:
        normalized['language'] = 'zh'
    
    logger.debug(f"å‚æ•°æ ‡å‡†åŒ–: {strategy} - åŸå§‹: {params} -> æ ‡å‡†åŒ–: {normalized}")
    return normalized

# @app.post("/chunking/process")
# async def process_chunking(request: Request):
#     """
#     æ–‡æœ¬åˆ‡åˆ†å¤„ç†
#     """
#     try:
#         data = await request.json()
#         text = data.get("text", "")
#         strategy = data.get("strategy", "traditional")
#         params = data.get("params", {})
        
#         if not text:
#             raise HTTPException(status_code=400, detail="æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
#         logger.info(f"æ”¶åˆ°æ–‡æœ¬åˆ‡åˆ†è¯·æ±‚: ç­–ç•¥={strategy}, æ–‡æœ¬é•¿åº¦={len(text)}, åŸå§‹å‚æ•°={params}")
        
#         # æ ‡å‡†åŒ–å‚æ•°åç§°
#         normalized_params = normalize_params(strategy, params)
        
#         # å¯¼å…¥æ–‡æœ¬åˆ‡åˆ†æ¨¡å—
#         try:
#             from dataBuilder.chunking.chunk_strategies import ChunkingManager
            
#             # ä¼ é€’å…¨å±€é…ç½®ä¿¡æ¯ç»™ChunkingManager
#             chunking_manager = ChunkingManager(config=config)
#             chunks = chunking_manager.chunk_text(text, strategy, **normalized_params)
            
#             logger.info(f"åˆ†å—å¤„ç†æˆåŠŸ: ç­–ç•¥={strategy}, ç”Ÿæˆå—æ•°={len(chunks)}")
            
#             return {
#                 "chunks": chunks,
#                 "chunk_count": len(chunks),
#                 "strategy": strategy,
#                 "params_used": normalized_params,
#                 "status": "success"
#             }
            
#         except ImportError as import_error:
#             logger.warning(f"é«˜çº§åˆ†å—æ¨¡å—ä¸å¯ç”¨: {import_error}, ä½¿ç”¨ç®€å•åˆ†å—")
#             # å¦‚æœåˆ‡åˆ†æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ‡åˆ†
#             chunk_length = normalized_params.get("chunk_length", 512)
#             overlap = normalized_params.get("overlap", 50)
            
#             chunks = []
#             start = 0
#             while start < len(text):
#                 end = min(start + chunk_length, len(text))
#                 chunk = text[start:end]
#                 chunks.append(chunk)
#                 start = end - overlap if end < len(text) else end
            
#             logger.info(f"ç®€å•åˆ†å—å®Œæˆ: ç”Ÿæˆå—æ•°={len(chunks)}")
            
#             return {
#                 "chunks": chunks,
#                 "chunk_count": len(chunks),
#                 "strategy": "simple_fallback",
#                 "params_used": normalized_params,
#                 "status": "success",
#                 "warning": "é«˜çº§åˆ†å—æ¨¡å—ä¸å¯ç”¨ï¼Œå·²é™çº§åˆ°ç®€å•åˆ†å—"
#             }
            
#         except Exception as processing_error:
#             logger.error(f"åˆ†å—å¤„ç†å¤±è´¥: {processing_error}")
            
#             # å°è¯•é™çº§åˆ°ç®€å•åˆ†å—
#             try:
#                 logger.info(f"å°è¯•é™çº§åˆ°ç®€å•åˆ†å—: ç­–ç•¥={strategy}")
#                 chunk_length = normalized_params.get("chunk_length", 512)
#                 overlap = normalized_params.get("overlap", 50)
                
#                 chunks = []
#                 start = 0
#                 while start < len(text):
#                     end = min(start + chunk_length, len(text))
#                     chunk = text[start:end]
#                     chunks.append(chunk)
#                     start = end - overlap if end < len(text) else end
                
#                 logger.info(f"é™çº§åˆ†å—å®Œæˆ: ç”Ÿæˆå—æ•°={len(chunks)}")
                
#                 return {
#                     "chunks": chunks,
#                     "chunk_count": len(chunks),
#                     "strategy": f"{strategy}_fallback",
#                     "params_used": normalized_params,
#                     "status": "success",
#                     "warning": f"ç­–ç•¥ {strategy} å¤„ç†å¤±è´¥ï¼Œå·²é™çº§åˆ°ç®€å•åˆ†å—",
#                     "error_details": str(processing_error)
#                 }
                
#             except Exception as fallback_error:
#                 logger.error(f"é™çº§åˆ†å—ä¹Ÿå¤±è´¥: {fallback_error}")
#                 raise HTTPException(
#                     status_code=500,
#                     detail=f"åˆ†å—å¤„ç†å¤±è´¥ï¼Œé™çº§ä¹Ÿå¤±è´¥: åŸå§‹é”™è¯¯={str(processing_error)}, é™çº§é”™è¯¯={str(fallback_error)}"
#                 )
        
#     except HTTPException:
#         # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
#         raise
#     except Exception as e:
#         logger.error(f"æ–‡æœ¬åˆ‡åˆ†è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
#         import traceback
#         logger.error(f"è¯¦ç»†é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
#         raise HTTPException(
#             status_code=500,
#             detail=f"æ–‡æœ¬åˆ‡åˆ†å¤±è´¥: {str(e)}"
#         )

@app.post("/chunking/process")
async def process_chunking(request: Request):
    """æ–‡æœ¬åˆ‡åˆ†å¤„ç†"""
    try:
        # åˆå§‹åŒ–ä¾èµ–
        if not CHUNKING_AVAILABLE:
            raise HTTPException(status_code=503, detail="åˆ†å—æœåŠ¡ä¸å¯ç”¨")
            
        data = await request.json()
        text = data.get("text", "")
        strategy = data.get("strategy", "traditional")
        params = data.get("params", {})
        
        # å‚æ•°æ ‡å‡†åŒ–
        normalized_params = normalize_params(strategy, params)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # ä½¿ç”¨ChunkingManagerè¿›è¡Œåˆ‡åˆ†
            if chunking_manager:
                chunks = chunking_manager.chunk_text(text=text,strategy=strategy, **normalized_params)
                processing_time = time.time() - start_time
                
                return {
                    "success": True,
                    "chunks": chunks,
                    "chunk_count": len(chunks),
                    "strategy": strategy,
                    "actual_strategy": strategy,
                    "params": normalized_params,
                    "processing_time": processing_time,
                    "status": "success",
                    "timestamp": datetime.now().isoformat()
                }
            else:
                # é™çº§å¤„ç†
                return handle_fallback_chunking(text, normalized_params)
                
        except Exception as processing_error:
            logger.error(f"åˆ†å—å¤„ç†å¤±è´¥: {processing_error}")
            # å°è¯•é™çº§ç­–ç•¥
            if strategy == "meta_ppl":
                logger.warning("PPLåˆ†å—å¤±è´¥ï¼Œé™çº§åˆ°è¯­ä¹‰åˆ†å—")
                fallback_chunks = chunking_manager.chunk_text(text=text, strategy="semantic", **normalized_params) if chunking_manager else None
                if fallback_chunks:
                    processing_time = time.time() - start_time
                    return {
                        "success": True,
                        "chunks": fallback_chunks,
                        "chunk_count": len(fallback_chunks),
                        "strategy": strategy,
                        "actual_strategy": "semantic",
                        "params": normalized_params,
                        "processing_time": processing_time,
                        "status": "success",
                        "warning": "PPLåˆ†å—å¤±è´¥ï¼Œå·²é™çº§åˆ°è¯­ä¹‰åˆ†å—",
                        "error_details": str(processing_error),
                        "timestamp": datetime.now().isoformat()
                    }
            
            return handle_fallback_chunking(text, normalized_params, str(processing_error))
            
    except Exception as e:
        logger.error(f"åˆ†å—å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))



def handle_fallback_chunking(text: str, params: dict, error_msg: str = None):
    """é™çº§å¤„ç†å‡½æ•°"""
    chunk_length = params.get("chunk_length", 512)
    overlap = params.get("overlap", 50)
    
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_length, len(text))
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap if end < len(text) else end
        
    return {
        "chunks": chunks,
        "chunk_count": len(chunks),
        "strategy": "traditional_fallback",
        "params": params,
        "status": "success",
        "warning": "å·²é™çº§åˆ°ä¼ ç»Ÿåˆ†å—æ–¹æ³•",
        "error_details": error_msg
    }
@app.get("/system/status")
async def system_status():
    """ç³»ç»ŸçŠ¶æ€æ£€æŸ¥"""
    # æ£€æŸ¥Milvusé…ç½®æ˜¯å¦å­˜åœ¨
    milvus_configured = bool(config.get("milvus", {}).get("host"))
    
    # æ£€æŸ¥GLMé…ç½®çŠ¶æ€
    # glm_configured = False
    # glm_model_name = None
    # try:
    #     active_llm = config.get('active_llm_config', 'glm_default')
    #     llm_configs = config.get('llm_configs', {})
    #     if active_llm in llm_configs:
    #         api_key = llm_configs[active_llm].get('api_key', '')
    #         glm_configured = bool(api_key)
    #         glm_model_name = llm_configs[active_llm].get('model_name', 'glm-4.5-flash')
    # except Exception:
    #     pass
    
    # æ£€æŸ¥èšç±»æœåŠ¡çŠ¶æ€
    clustering_status = {
        "available": False,
        "model_loaded": False
    }
    try:
        from Search.clustering import ClusteringService
        service = ClusteringService()
        clustering_status["available"] = service.hdbscan_available or service.sklearn_available
        clustering_status["model_loaded"] = True
    except ImportError:
        logger.warning("èšç±»æœåŠ¡ä¾èµ–æœªå®‰è£…")
    except Exception as e:
        logger.error(f"èšç±»æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")

    # æ£€æŸ¥åˆ†å—ç³»ç»ŸçŠ¶æ€
    chunking_system_status = {
        "available": True,
        "basic_chunking": True,
        "advanced_chunking": False,
        "strategies_available": []
    }
    
    try:
        from dataBuilder.chunking.chunk_strategies import get_available_strategies
        strategies = get_available_strategies()
        chunking_system_status["advanced_chunking"] = True
        chunking_system_status["strategies_available"] = [s.get("name", "") for s in strategies]
    except ImportError:
        chunking_system_status["strategies_available"] = ["traditional"]
    except Exception as e:
        logger.warning(f"æ£€æŸ¥åˆ†å—ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
    
    # è®¡ç®—æ•´ä½“å¥åº·çŠ¶æ€
    health_score = 0
    health_issues = []
    
    if config:
        health_score += 25
    else:
        health_issues.append("é…ç½®æ–‡ä»¶æœªåŠ è½½")
    
    if milvus_configured:
        health_score += 25
    else:
        health_issues.append("Milvusæœªé…ç½®")
    
    if chunking_system_status["available"]:
        health_score += 25
    else:
        health_issues.append("åˆ†å—ç³»ç»Ÿä¸å¯ç”¨")
    
    if chunking_system_status["advanced_chunking"]:
        health_score += 15
    else:
        health_issues.append("é«˜çº§åˆ†å—åŠŸèƒ½ä¸å¯ç”¨")
    
    # if glm_configured:
    #     health_score += 10
    # else:
    #     health_issues.append("GLMæœªé…ç½®")
    
    # æ›´æ–°å¥åº·è¯„åˆ†
    if clustering_status.get("available"):
        health_score += 10
    else:
        health_issues.append("èšç±»æœåŠ¡ä¸å¯ç”¨")

    # ç¡®å®šæ•´ä½“çŠ¶æ€
    if health_score >= 80:
        overall_status = "healthy"
    elif health_score >= 60:
        overall_status = "degraded"
    else:
        overall_status = "unhealthy"
    
    return {
        "system_status": "running",
        "timestamp": datetime.now().isoformat(),
        "config_loaded": bool(config),
        "upload_dir_exists": os.path.exists("data/upload"),
        "config_keys": list(config.keys()) if config else [],
        "health": {
            "overall_status": overall_status,
            "health_score": health_score,
            "issues": health_issues
        },
        "status": {
            "milvus": {
                "connected": milvus_configured,
                "host": config.get("milvus", {}).get("host", "æœªé…ç½®"),
                "collection": config.get("milvus", {}).get("collection_name", "æœªé…ç½®")
            },
            "embedding_model": {
                "available": milvus_configured
            },
            "chunking_system": chunking_system_status,
            "clustering_service": {
                "available": clustering_status.get("available"),
                "model": clustering_status.get("model_name", "æœªåŠ è½½"),
                "version": clustering_status.get("model_version", "æœªçŸ¥")
            }
        }
    }

@app.get("/files")
async def list_files():
    """åˆ—å‡ºä¸Šä¼ çš„æ–‡ä»¶"""
    try:
        upload_dir = "data/upload"
        if not os.path.exists(upload_dir):
            return {"files": []}
        
        files = []
        for filename in os.listdir(upload_dir):
            file_path = os.path.join(upload_dir, filename)
            if os.path.isfile(file_path):
                stat = os.stat(file_path)
                files.append({
                    "name": filename,
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
                })
        
        return {"files": files}
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# @app.get("/glm/config")
# async def get_glm_config():
#     """è·å–GLMé…ç½®çŠ¶æ€"""
#     try:
#         # è·å–å½“å‰æ´»è·ƒé…ç½®
#         active_llm = config.get('active_llm_config', 'glm_default')
#         llm_configs = config.get('llm_configs', {})
        
#         if active_llm in llm_configs:
#             llm_config = llm_configs[active_llm]
#             api_key = llm_config.get('api_key', '')
            
#             return {
#                 "configured": bool(api_key),
#                 "model_name": llm_config.get('model_name', 'glm-4.5-flash'),
#                 "api_key_configured": bool(api_key),
#                 "api_key_preview": f"***{api_key[-4:]}" if api_key and len(api_key) > 4 else None,
#                 "last_validated": llm_config.get('last_validated'),
#                 "is_active": True,
#                 "config_id": active_llm
#             }
#         else:
#             return {
#                 "configured": False,
#                 "model_name": None,
#                 "api_key_configured": False,
#                 "api_key_preview": None,
#                 "last_validated": None,
#                 "is_active": False,
#                 "config_id": active_llm
#             }
            
#     except Exception as e:
#         logger.error(f"è·å–GLMé…ç½®å¤±è´¥: {e}")
#         return {
#             "configured": False,
#             "error": str(e)
#         }

# @app.post("/glm/config")
# async def save_glm_config(request: dict):
#     """ä¿å­˜GLMé…ç½®åˆ°YAMLæ–‡ä»¶"""
#     try:
#         model_name = request.get("model_name", "glm-4.5-flash")
#         api_key = request.get("api_key", "")
        
#         if not api_key:
#             raise HTTPException(status_code=400, detail="APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        
#         # ç¡®ä¿llm_configsç»“æ„å­˜åœ¨
#         if 'llm_configs' not in config:
#             config['llm_configs'] = {}
        
#         # è·å–å½“å‰æ´»è·ƒé…ç½®å
#         active_llm = config.get('active_llm_config', 'glm_default')
        
#         # æ›´æ–°é…ç½®
#         from datetime import datetime
#         current_time = datetime.now().isoformat()
        
#         config['llm_configs'][active_llm] = {
#             'model_name': model_name,
#             'api_key': api_key,
#             'api_endpoint': 'https://open.bigmodel.cn/api/paas/v4/chat/completions',
#             'provider': 'zhipu',
#             'created_at': current_time,
#             'last_validated': current_time
#         }
        
#         # è®¾ç½®æ´»è·ƒé…ç½®
#         config['active_llm_config'] = active_llm
        
#         if save_config():
#             logger.info(f"GLMé…ç½®ä¿å­˜æˆåŠŸ: {active_llm}")
#             return {
#                 "success": True, 
#                 "message": "GLMé…ç½®ä¿å­˜æˆåŠŸ",
#                 "config_id": active_llm,
#                 "model_name": model_name
#             }
#         else:
#             raise HTTPException(status_code=500, detail="é…ç½®ä¿å­˜å¤±è´¥")
            
#     except Exception as e:
#         logger.error(f"ä¿å­˜GLMé…ç½®å¤±è´¥: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

# @app.delete("/glm/config")
# async def clear_glm_config():
#     """æ¸…é™¤GLMé…ç½®"""
#     try:
#         # è·å–å½“å‰æ´»è·ƒé…ç½®
#         active_llm = config.get('active_llm_config', 'glm_default')
        
#         if 'llm_configs' in config and active_llm in config['llm_configs']:
#             # æ¸…é™¤APIå¯†é’¥å’ŒéªŒè¯æ—¶é—´
#             config['llm_configs'][active_llm]['api_key'] = None
#             config['llm_configs'][active_llm]['last_validated'] = None
            
#             if save_config():
#                 logger.info(f"GLMé…ç½®å·²æ¸…é™¤: {active_llm}")
#                 return {"success": True, "message": "GLMé…ç½®å·²æ¸…é™¤"}
#             else:
#                 raise HTTPException(status_code=500, detail="é…ç½®ä¿å­˜å¤±è´¥")
#         else:
#             return {"success": True, "message": "GLMé…ç½®å·²ç»ä¸ºç©º"}
            
#     except Exception as e:
#         logger.error(f"æ¸…é™¤GLMé…ç½®å¤±è´¥: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

# @app.post("/glm/test-connection")
# async def test_glm_connection():
#     """æµ‹è¯•GLMè¿æ¥"""
#     try:
#         # è·å–å½“å‰æ´»è·ƒé…ç½®
#         active_llm = config.get('active_llm_config', 'glm_default')
#         llm_configs = config.get('llm_configs', {})
        
#         if active_llm not in llm_configs:
#             return {"success": False, "message": "GLMé…ç½®ä¸å­˜åœ¨"}
        
#         llm_config = llm_configs[active_llm]
#         api_key = llm_config.get('api_key')
        
#         if not api_key:
#             return {"success": False, "message": "GLM APIå¯†é’¥æœªé…ç½®"}
        
#         # å°è¯•å¯¼å…¥å¹¶æµ‹è¯•GLMé…ç½®æœåŠ¡
#         try:
#             import sys
#             import os
#             sys.path.append(os.path.join(os.path.dirname(__file__), 'dataBuilder', 'chunking'))
#             from dataBuilder.chunking.glm_config import get_glm_config_service
            
#             service = get_glm_config_service()
#             if service:
#                 is_valid, message = service.validate_api_key(api_key)
#                 return {"success": is_valid, "message": message}
#             else:
#                 return {"success": False, "message": "GLMé…ç½®æœåŠ¡ä¸å¯ç”¨"}
                
#         except Exception as e:
#             logger.error(f"GLMè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
#             # ç®€åŒ–çš„æ ¼å¼éªŒè¯
#             if len(api_key.strip()) < 20:
#                 return {"success": False, "message": "APIå¯†é’¥æ ¼å¼ä¸æ­£ç¡®ï¼Œé•¿åº¦è¿‡çŸ­"}
#             else:
#                 return {"success": True, "message": "APIå¯†é’¥æ ¼å¼éªŒè¯é€šè¿‡ï¼ˆæ— æ³•è¿›è¡Œå®é™…è¿æ¥æµ‹è¯•ï¼‰"}
                
#     except Exception as e:
#         logger.error(f"æµ‹è¯•GLMè¿æ¥å¤±è´¥: {e}")
#         return {"success": False, "message": f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"}

# @app.post("/glm/validate-key")
# async def validate_glm_key(request: dict):
#     """éªŒè¯GLM APIå¯†é’¥"""
#     api_key = request.get("api_key")
#     if not api_key:
#         return {"valid": False, "message": "APIå¯†é’¥ä¸èƒ½ä¸ºç©º"}
    
#     return {"valid": True, "message": "APIå¯†é’¥æ ¼å¼æœ‰æ•ˆ"}

@app.get("/chunking/config")
async def get_chunking_config():
    """è·å–åˆ†å—é…ç½®ä¿¡æ¯"""
    try:
        chunking_config = config.get("chunking", {})
        
        # è·å–GLMé…ç½®çŠ¶æ€
        glm_configured = False
        try:
            active_llm = config.get('active_llm_config', 'glm_default')
            llm_configs = config.get('llm_configs', {})
            if active_llm in llm_configs:
                api_key = llm_configs[active_llm].get('api_key', '')
                glm_configured = bool(api_key)
        except Exception:
            pass
        
        # æ£€æŸ¥é«˜çº§åˆ†å—æ¨¡å—å¯ç”¨æ€§
        advanced_chunking_available = False
        try:
            from dataBuilder.chunking.chunk_strategies import ChunkingManager
            advanced_chunking_available = True
        except ImportError:
            pass
        
        return {
            "current_config": chunking_config,
            "glm_configured": glm_configured,
            "advanced_chunking_available": advanced_chunking_available,
            "supported_strategies": {
                "traditional": {
                    "available": True,
                    "requires_glm": False,
                    "default_params": {
                        "chunk_size": 512,
                        "overlap": 50
                    }
                },
                "meta_ppl": {
                    "available": glm_configured and advanced_chunking_available,
                    "requires_glm": True,
                    "default_params": {
                        "threshold": 0.3,
                        "language": "zh"
                    }
                },
                "margin_sampling": {
                    "available": glm_configured and advanced_chunking_available,
                    "requires_glm": True,
                    "default_params": {
                        "chunk_length": 512,
                        "language": "zh"
                    }
                },
                "msp": {
                    "available": glm_configured and advanced_chunking_available,
                    "requires_glm": True,
                    "default_params": {
                        "chunk_length": 512,
                        "confidence_threshold": 0.7,
                        "language": "zh"
                    }
                },
                "semantic": {
                    "available": advanced_chunking_available,
                    "requires_glm": False,
                    "default_params": {
                        "similarity_threshold": 0.8,
                        "min_chunk_size": 100,
                        "max_chunk_size": 1000
                    }
                }
            },
            "parameter_mapping": {
                "chunk_size": "chunk_length",
                "threshold": "ppl_threshold"
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–åˆ†å—é…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–åˆ†å—é…ç½®å¤±è´¥: {str(e)}"
        )

@app.post("/update_config")
async def update_config_legacy(request: dict):
    """æ›´æ–°é…ç½®ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
    try:
        config.update(request)
        
        if save_config():
            logger.info("é…ç½®æ›´æ–°æˆåŠŸ")
            return {"success": True, "message": "é…ç½®æ›´æ–°æˆåŠŸ"}
        else:
            raise HTTPException(status_code=500, detail="é…ç½®ä¿å­˜å¤±è´¥")
            
    except Exception as e:
        logger.error(f"æ›´æ–°é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/progress/{tracking_id}")
async def get_progress(tracking_id: str):
    """è·å–è¿›åº¦ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    return {
        "tracking_id": tracking_id,
        "status": "completed",
        "progress": 100,
        "message": "å¤„ç†å®Œæˆ"
    }

@app.post("/system/integration_test")
async def integration_test():
    """ç³»ç»Ÿé›†æˆæµ‹è¯•"""
    return {
        "success": True,
        "message": "é›†æˆæµ‹è¯•é€šè¿‡",
        "tests": [
            {"name": "é…ç½®åŠ è½½", "status": "passed"},
            {"name": "æ–‡ä»¶ä¸Šä¼ ", "status": "passed"},
            {"name": "æœç´¢åŠŸèƒ½", "status": "passed"}
        ]
    }

if __name__ == "__main__":
    import uvicorn
    logger.info("å¯åŠ¨ç®€åŒ–ç‰ˆAPIæœåŠ¡...")
    uvicorn.run(app, host="0.0.0.0", port=12089)