from enum import Enum
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

try:
    from .meta_chunking import MetaChunking
except ImportError:
    from meta_chunking import MetaChunking


class ChunkingStrategy(Enum):
    TRADITIONAL = "traditional"
    META_PPL = "meta_ppl"
    MARGIN_SAMPLING = "margin_sampling"
    MSP = "msp"
    SEMANTIC = "semantic"


def get_available_strategies() -> List[Dict[str, str]]:
    """
    è·å–å¯ç”¨çš„åˆ‡åˆ†ç­–ç•¥åˆ—è¡¨
    
    è¿”å›:
        ç­–ç•¥åˆ—è¡¨ï¼ŒåŒ…å«ç­–ç•¥åç§°å’Œæè¿°
    """
    return [
        {
            "name": ChunkingStrategy.TRADITIONAL.value,
            "display_name": "ä¼ ç»Ÿå›ºå®šåˆ‡åˆ†",
            "description": "åŸºäºå›ºå®šé•¿åº¦å’Œé‡å çš„ä¼ ç»Ÿåˆ‡åˆ†æ–¹æ³•ï¼Œç®€å•é«˜æ•ˆ"
        },
        {
            "name": ChunkingStrategy.META_PPL.value,
            "display_name": "PPLå›°æƒ‘åº¦åˆ‡åˆ†",
            "description": "åŸºäºè¯­è¨€æ¨¡å‹å›°æƒ‘åº¦çš„æ™ºèƒ½åˆ‡åˆ†æ–¹æ³•ï¼Œåœ¨è¯­ä¹‰è¾¹ç•Œå¤„åˆ‡åˆ†"
        },
        {
            "name": ChunkingStrategy.MARGIN_SAMPLING.value,
            "display_name": "è¾¹é™…é‡‡æ ·åˆ‡åˆ†(MSP)",
            "description": "åŸºäºæ¦‚ç‡å†³ç­–çš„è¾¹é™…é‡‡æ ·åˆ‡åˆ†æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´åˆ‡åˆ†ç‚¹"
        },
        {
            "name": ChunkingStrategy.MSP.value,
            "display_name": "MSPåˆ‡åˆ†ç­–ç•¥",
            "description": "Margin Sampling Partitioningï¼ŒåŸºäºè¾¹é™…æ¦‚ç‡çš„é«˜çº§åˆ‡åˆ†ç­–ç•¥"
        },
        {
            "name": ChunkingStrategy.SEMANTIC.value,
            "display_name": "è¯­ä¹‰åˆ‡åˆ†",
            "description": "åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ™ºèƒ½åˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§"
        }
    ]


class ChunkingManager:
    """
    åˆ‡åˆ†ç®¡ç†å™¨ï¼Œæä¾›ç»Ÿä¸€çš„åˆ‡åˆ†æ¥å£
    """
    
    def __init__(self, model=None, tokenizer=None, config=None):
        """
        åˆå§‹åŒ–åˆ‡åˆ†ç®¡ç†å™¨
        
        å‚æ•°:
            model: è¯­è¨€æ¨¡å‹ï¼ˆç”¨äºæ™ºèƒ½åˆ‡åˆ†ç­–ç•¥ï¼‰
            tokenizer: åˆ†è¯å™¨
            config: é…ç½®å­—å…¸ï¼Œç”¨äºè‡ªåŠ¨åŠ è½½æ¨¡å‹æˆ–GLMé…ç½®
        """
        # å»¶è¿ŸåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._model = model
        self._tokenizer = tokenizer
        self._model_loaded = False
        self.config = config or {}
        
        # è·å–LLMé…ç½®ç®¡ç†å™¨
        self.llm_config_manager = self._get_llm_config_manager()
        
        # å»¶è¿Ÿåˆå§‹åŒ–MetaChunking
        self._meta_chunking = None
    
    def _get_llm_config_manager(self):
        """è·å–LLMé…ç½®ç®¡ç†å™¨"""
        try:
            from .llm_config import create_llm_config_manager
            return create_llm_config_manager()
        except Exception as e:
            logger.warning(f"è·å–LLMé…ç½®ç®¡ç†å™¨å¤±è´¥: {e}")
            return None
    
    def _ensure_model_loaded(self):
        """ç¡®ä¿æ¨¡å‹å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._model_loaded:
            return
        
        try:
            # å¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹ï¼Œå°è¯•ä»é…ç½®åŠ è½½
            if self._model is None and self._tokenizer is None:
                logger.info("ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹...")
                self._model, self._tokenizer = self._load_model_from_config(self.config)
                logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # åˆå§‹åŒ–MetaChunking
            if self._meta_chunking is None:
                self._meta_chunking = MetaChunking(self._model, self._tokenizer, None)
                logger.info("âœ… MetaChunkingåˆå§‹åŒ–å®Œæˆ")
            
            self._model_loaded = True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å³ä½¿åŠ è½½å¤±è´¥ï¼Œä¹Ÿæ ‡è®°ä¸ºå·²å°è¯•ï¼Œé¿å…é‡å¤å°è¯•
            self._model_loaded = True
    
    @property
    def meta_chunking(self):
        """è·å–MetaChunkingå®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        self._ensure_model_loaded()
        return self._meta_chunking
    
    def _create_llm_client(self):
        """é€šè¿‡LLMé…ç½®ç®¡ç†å™¨åˆ›å»ºLLMå®¢æˆ·ç«¯"""
        if not self.llm_config_manager:
            logger.warning("LLMé…ç½®ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºLLMå®¢æˆ·ç«¯")
            return None
            
        try:
            # ä½¿ç”¨é…ç½®ç®¡ç†å™¨åˆ›å»ºAPIå®¢æˆ·ç«¯
            client = self.llm_config_manager.create_api_client()
            if client:
                logger.info("æˆåŠŸåˆ›å»ºLLM APIå®¢æˆ·ç«¯")
                return client
            else:
                logger.debug("æœªæ‰¾åˆ°æœ‰æ•ˆçš„LLMé…ç½®æˆ–åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥")
                return None
        except Exception as e:
            logger.error(f"åˆ›å»ºLLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None
    
    def _load_model_from_config(self, config):
        """
        ä»é…ç½®åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        
        å‚æ•°:
            config: é…ç½®å­—å…¸
        
        è¿”å›:
            (model, tokenizer) å…ƒç»„ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å› (None, None)
        """
        try:
            chunking_config = config.get("chunking", {})
            model_config = chunking_config.get("model", {})
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨é«˜çº§åˆ†å—
            if not model_config.get("enable_advanced_chunking", False):
                return None, None
            
            model_name = model_config.get("model_name")
            tokenizer_name = model_config.get("tokenizer_name", model_name)
            device = model_config.get("device", "auto")
            
            if not model_name:
                return None, None
            
            # å°è¯•åŠ è½½æ¨¡å‹ - ModelScope
            try:
                import torch
                import torch
                
                tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
                
                # è®¾å¤‡é€‰æ‹©
                if device == "auto":
                    device = "cuda" if torch.cuda.is_available() else "cpu"
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                    device_map=device if device != "cpu" else None
                )
                
                if device == "cpu":
                    model = model.to(device)
                
                return model, tokenizer
                
            except Exception as e:
                print(f"Warning: Failed to load model {model_name}: {e}")
                return None, None
                
        except Exception as e:
            print(f"Warning: Failed to load model from config: {e}")
            return None, None
    
    def chunk_text(self, text: str, strategy: str, **kwargs) -> List[str]:
        """
        æ‰§è¡Œæ–‡æœ¬åˆ‡åˆ†
        
        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬
            strategy: åˆ‡åˆ†ç­–ç•¥
            **kwargs: ç­–ç•¥ç‰¹å®šå‚æ•°
        
        è¿”å›:
            åˆ‡åˆ†åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if strategy == ChunkingStrategy.TRADITIONAL.value:
            return self._traditional_chunking(text, **kwargs)
        elif strategy == ChunkingStrategy.META_PPL.value:
            return self._ppl_chunking(text, **kwargs)
        elif strategy == ChunkingStrategy.MARGIN_SAMPLING.value:
            return self._margin_sampling_chunking(text, **kwargs)
        elif strategy == ChunkingStrategy.MSP.value:
            return self._msp_chunking(text, **kwargs)
        elif strategy == ChunkingStrategy.SEMANTIC.value:
            return self._semantic_chunking(text, **kwargs)
        else:
            raise ValueError(f"Unknown chunking strategy: {strategy}")
    
    def _traditional_chunking(self, text: str, **kwargs) -> List[str]:
        """ä¼ ç»Ÿåˆ‡åˆ†"""
        chunk_size = kwargs.get('chunk_size', 512)
        overlap = kwargs.get('overlap', 50)
        return self.meta_chunking.traditional_chunking(text, chunk_size, overlap)
    
    def _ppl_chunking(self, text: str, **kwargs) -> List[str]:
        """PPLå›°æƒ‘åº¦åˆ‡åˆ†"""
        threshold = kwargs.get('threshold', 0.3)
        language = kwargs.get('language', 'zh')
        
        try:
            # ç¡®ä¿MetaChunkingæœ‰å¯ç”¨çš„APIå®¢æˆ·ç«¯
            if not self.meta_chunking.api_client:
                self.meta_chunking.api_client = self._create_llm_client()
            
            return self.meta_chunking.ppl_chunking(text, threshold, language)
        except Exception as e:
            logger.error(f"PPLåˆ†å—å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»Ÿåˆ†å—
            chunk_size = kwargs.get('chunk_size', 512)
            overlap = kwargs.get('overlap', 50)
            return self.meta_chunking.traditional_chunking(text, chunk_size, overlap)
    
    def _margin_sampling_chunking(self, text: str, **kwargs) -> List[str]:
        """è¾¹é™…é‡‡æ ·åˆ‡åˆ†"""
        language = kwargs.get('language', 'zh')
        chunk_length = kwargs.get('chunk_length', 512)
        
        try:
            # ç¡®ä¿MetaChunkingæœ‰å¯ç”¨çš„APIå®¢æˆ·ç«¯
            if not self.meta_chunking.api_client:
                self.meta_chunking.api_client = self._create_llm_client()
            
            return self.meta_chunking.margin_sampling_chunking(text, language, chunk_length)
        except Exception as e:
            logger.error(f"è¾¹é™…é‡‡æ ·åˆ†å—å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»Ÿåˆ†å—
            chunk_size = kwargs.get('chunk_size', 512)
            overlap = kwargs.get('overlap', 50)
            return self.meta_chunking.traditional_chunking(text, chunk_size, overlap)
    
    def _msp_chunking(self, text: str, **kwargs) -> List[str]:
        """MSPåˆ‡åˆ†ç­–ç•¥"""
        language = kwargs.get('language', 'zh')
        chunk_length = kwargs.get('chunk_length', 512)
        confidence_threshold = kwargs.get('confidence_threshold', 0.7)
        
        try:
            # ç¡®ä¿MetaChunkingæœ‰å¯ç”¨çš„APIå®¢æˆ·ç«¯
            if not self.meta_chunking.api_client:
                self.meta_chunking.api_client = self._create_llm_client()
            
            return self.meta_chunking.msp_chunking(text, language, chunk_length, confidence_threshold)
        except Exception as e:
            logger.error(f"MSPåˆ†å—å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»Ÿåˆ†å—
            chunk_size = kwargs.get('chunk_size', 512)
            overlap = kwargs.get('overlap', 50)
            return self.meta_chunking.traditional_chunking(text, chunk_size, overlap)
    
    def _semantic_chunking(self, text: str, **kwargs) -> List[str]:
        """è¯­ä¹‰åˆ‡åˆ†"""
        similarity_threshold = kwargs.get('similarity_threshold', 0.8)
        min_chunk_size = kwargs.get('min_chunk_size', 100)
        max_chunk_size = kwargs.get('max_chunk_size', 1000)
        return self.meta_chunking.semantic_chunking(text, similarity_threshold, min_chunk_size, max_chunk_size)
    
    def get_strategy_config(self, strategy: str) -> Dict[str, Any]:
        """
        è·å–ç­–ç•¥çš„é…ç½®å‚æ•°
        
        å‚æ•°:
            strategy: ç­–ç•¥åç§°
        
        è¿”å›:
            é…ç½®å‚æ•°å­—å…¸
        """
        configs = {
            ChunkingStrategy.TRADITIONAL.value: {
                "chunk_size": {
                    "type": "int",
                    "default": 512,
                    "min": 100,
                    "max": 2048,
                    "description": "æ–‡æœ¬å—å¤§å°"
                },
                "overlap": {
                    "type": "int", 
                    "default": 50,
                    "min": 0,
                    "max": 200,
                    "description": "é‡å å¤§å°"
                }
            },
            ChunkingStrategy.META_PPL.value: {
                "threshold": {
                    "type": "float",
                    "default": 0.3,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "description": "å›°æƒ‘åº¦é˜ˆå€¼"
                },
                "language": {
                    "type": "select",
                    "default": "zh",
                    "options": ["zh", "en"],
                    "description": "è¯­è¨€ç±»å‹"
                }
            },
            ChunkingStrategy.MARGIN_SAMPLING.value: {
                "language": {
                    "type": "select",
                    "default": "zh", 
                    "options": ["zh", "en"],
                    "description": "è¯­è¨€ç±»å‹"
                },
                "chunk_length": {
                    "type": "int",
                    "default": 512,
                    "min": 100,
                    "max": 2048,
                    "description": "æœ€å¤§å—é•¿åº¦"
                }
            },
            ChunkingStrategy.MSP.value: {
                "language": {
                    "type": "select",
                    "default": "zh",
                    "options": ["zh", "en"],
                    "description": "è¯­è¨€ç±»å‹"
                },
                "chunk_length": {
                    "type": "int",
                    "default": 512,
                    "min": 100,
                    "max": 2048,
                    "description": "æœ€å¤§å—é•¿åº¦"
                },
                "confidence_threshold": {
                    "type": "float",
                    "default": 0.7,
                    "min": 0.5,
                    "max": 0.95,
                    "step": 0.05,
                    "description": "ç½®ä¿¡åº¦é˜ˆå€¼"
                }
            },
            ChunkingStrategy.SEMANTIC.value: {
                "similarity_threshold": {
                    "type": "float",
                    "default": 0.8,
                    "min": 0.6,
                    "max": 0.95,
                    "step": 0.05,
                    "description": "è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼"
                },
                "min_chunk_size": {
                    "type": "int",
                    "default": 100,
                    "min": 50,
                    "max": 500,
                    "description": "æœ€å°å—å¤§å°"
                },
                "max_chunk_size": {
                    "type": "int",
                    "default": 1000,
                    "min": 500,
                    "max": 3000,
                    "description": "æœ€å¤§å—å¤§å°"
                }
            }
        }
        
        return configs.get(strategy, {})
    
    def refresh_llm_client(self):
        """åˆ·æ–°LLMå®¢æˆ·ç«¯ï¼ˆå½“LLMé…ç½®æ›´æ–°æ—¶è°ƒç”¨ï¼‰"""
        try:
            new_client = self._create_llm_client()
            meta_chunking = self.meta_chunking  # è§¦å‘å»¶è¿ŸåŠ è½½
            if meta_chunking:
                meta_chunking.api_client = new_client
                logger.info("ChunkingManager LLMå®¢æˆ·ç«¯å·²åˆ·æ–°")
        except Exception as e:
            logger.error(f"åˆ·æ–°LLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    def get_llm_status(self) -> Dict[str, Any]:
        """è·å–LLMçŠ¶æ€ä¿¡æ¯"""
        # å»¶è¿ŸåŠ è½½æ£€æŸ¥
        meta_chunking = self.meta_chunking  # è¿™ä¼šè§¦å‘å»¶è¿ŸåŠ è½½
        
        status = {
            "llm_config_manager_available": self.llm_config_manager is not None,
            "api_client_available": meta_chunking.api_client is not None if meta_chunking else False,
            "local_model_available": (meta_chunking.model is not None and 
                                    meta_chunking.tokenizer is not None) if meta_chunking else False
        }
        
        # LLMé…ç½®çŠ¶æ€
        if self.llm_config_manager:
            try:
                config_summary = self.llm_config_manager.get_config_summary()
                status["total_configs"] = config_summary.get("total_configs", 0)
                status["available_providers"] = config_summary.get("available_providers", 0)
                
                active_config = config_summary.get("active_config")
                if active_config:
                    status["active_provider"] = active_config.get("provider")
                    status["active_model"] = active_config.get("model")
                    status["llm_configured"] = True
                else:
                    status["llm_configured"] = False
                    
            except Exception as e:
                logger.error(f"è·å–LLMé…ç½®çŠ¶æ€å¤±è´¥: {e}")
                status["llm_configured"] = False
        
        # APIå®¢æˆ·ç«¯ç±»å‹
        if meta_chunking and meta_chunking.api_client:
            try:
                client_type = type(meta_chunking.api_client).__name__
                status["api_client_type"] = client_type
            except:
                status["api_client_type"] = "unknown"
        
        return status