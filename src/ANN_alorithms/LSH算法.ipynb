{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSHç®—æ³•",
        "",
        "## 1.LSH ç®—æ³•åŸç†åˆ†æ­¥è¯¦è§£",
        "",
        "**ğŸ§© LSH ç®—æ³•åŸç†åˆ†æ­¥è¯¦è§£ï¼ˆLocality-Sensitive Hashingï¼‰**",
        "",
        "---",
        "",
        "### ä¸€ã€æ ¸å¿ƒæ€æƒ³",
        "",
        "> **ç›®çš„**ï¼šåœ¨é«˜ç»´ç©ºé—´ä¸­å¿«é€Ÿæ‰¾åˆ°â€œç›¸ä¼¼â€çš„æ•°æ®ç‚¹ã€‚  ",
        "> **å…³é”®æ€æƒ³**ï¼šè®©ç›¸ä¼¼çš„æ ·æœ¬åœ¨å“ˆå¸Œå **è½å…¥åŒä¸€ä¸ªæ¡¶ï¼ˆbucketï¼‰** çš„æ¦‚ç‡é«˜ï¼Œä¸ç›¸ä¼¼çš„æ ·æœ¬è½å…¥åŒæ¡¶çš„æ¦‚ç‡ä½ã€‚",
        "",
        "ç›¸æ¯”æš´åŠ›æœç´¢ï¼ˆO(N)ï¼‰ï¼ŒLSH é€šè¿‡ **å¤šç»„éšæœºå“ˆå¸Œå‡½æ•°** å°†æœç´¢å¤æ‚åº¦é™ä½åˆ° **äºšçº¿æ€§ï¼ˆsublinearï¼‰**ã€‚",
        "",
        "---",
        "",
        "### äºŒã€é€‚ç”¨åœºæ™¯",
        "",
        "| ç›¸ä¼¼åº¦åº¦é‡ | å¸¸ç”¨ LSH å˜ä½“ | å“ˆå¸Œæ€æƒ³ |",
        "|-------------|----------------|-----------|",
        "| ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆcosine similarityï¼‰ | Random Projection LSH | ç”¨éšæœºè¶…å¹³é¢åˆ’åˆ†ç©ºé—´ |",
        "| æ¬§å¼è·ç¦»ï¼ˆL2 distanceï¼‰ | p-stable LSH | ç”¨éšæœºæŠ•å½± + æ¨¡è¿ç®—è¿‘ä¼¼æ¬§æ°è·ç¦» |",
        "| Jaccard ç›¸ä¼¼åº¦ï¼ˆé›†åˆç›¸ä¼¼åº¦ï¼‰ | MinHash LSH | åˆ©ç”¨æœ€å°å“ˆå¸Œå€¼è¿‘ä¼¼é›†åˆäº¤å¹¶æ¯” |",
        "",
        "---",
        "",
        "### ä¸‰ã€ç®—æ³•åŸç†åˆ†æ­¥è®²è§£ï¼ˆä»¥ä½™å¼¦ç›¸ä¼¼åº¦ä¸ºä¾‹ï¼‰",
        "",
        "#### ç¬¬ 1 æ­¥ï¼šæ„å»ºéšæœºè¶…å¹³é¢ï¼ˆRandom Hyperplanesï¼‰",
        "",
        "- åœ¨ d ç»´ç©ºé—´éšæœºç”Ÿæˆ k ä¸ªå‘é‡",
        "  $$",
        "  ( r_1, r_2, \\ldots, r_k )",
        "  $$",
        "  æ¯ä¸ªå‘é‡çš„åˆ†é‡æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ",
        "  $$",
        "   N(0,1) ",
        "  $$",
        "  ",
        "- æ¯ä¸ªè¶…å¹³é¢ä»£è¡¨ä¸€ä¸ªéšæœºæ–¹å‘ã€‚",
        "",
        "---",
        "",
        "#### ç¬¬ 2 æ­¥ï¼šè®¡ç®—å“ˆå¸Œç­¾åï¼ˆHash Signatureï¼‰",
        "",
        "å¯¹äºä¸€ä¸ªå‘é‡ \\( x \\)ï¼Œé€šè¿‡è®¡ç®—ï¼š",
        "",
        "$$",
        "h_i(x) =",
        "\\begin{cases}",
        "1, & \\text{if } r_i \\cdot x \\ge 0 \\\\",
        "0, & \\text{otherwise}",
        "\\end{cases}",
        "$$",
        "å°†æ¯ä¸ªè¶…å¹³é¢çš„ç»“æœæ‹¼æ¥æˆä¸€ä¸ªäºŒè¿›åˆ¶ä¸²ï¼ˆå¦‚ `10100110`ï¼‰ï¼Œ",
        "è¿™å°±æ˜¯è¯¥å‘é‡çš„ **å“ˆå¸Œç­¾å**ã€‚",
        "",
        "> âœ… ç›´è§‰ï¼šä¸¤ä¸ªè§’åº¦ç›¸ä¼¼çš„å‘é‡ï¼Œæ›´å¯èƒ½è¢«è¶…å¹³é¢åˆ’åˆ†åˆ°ç›¸åŒä¾§ï¼Œå› æ­¤å“ˆå¸Œç­¾åç›¸ä¼¼ã€‚",
        "",
        "---",
        "",
        "#### ç¬¬ 3 æ­¥ï¼šæ„å»ºå¤šä¸ªå“ˆå¸Œè¡¨ï¼ˆMulti-Table Strategyï¼‰",
        "",
        "- ä¸ºäº†å‡å°‘ç¢°æ’é”™è¯¯ï¼ˆä¸åŒå‘é‡å“ˆå¸Œç›¸åŒï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šç»„ç‹¬ç«‹å“ˆå¸Œå‡½æ•°ã€‚",
        "- å‡è®¾æœ‰ï¼š",
        "  - æ¯ç»„ k ä¸ªå“ˆå¸Œå‡½æ•°ç»„æˆä¸€ä¸ª **å“ˆå¸Œè¡¨ï¼ˆhash tableï¼‰**",
        "  - å…± L ä¸ªè¿™æ ·çš„è¡¨ã€‚",
        "",
        "æ¯ä¸ªæ ·æœ¬è¢«æ’å…¥åˆ° L ä¸ªä¸åŒè¡¨ä¸­ï¼Œä»è€Œæå‡å¬å›ç‡ã€‚",
        "",
        "---",
        "",
        "#### ç¬¬ 4 æ­¥ï¼šæŸ¥è¯¢ï¼ˆQueryï¼‰",
        "",
        "ç»™å®šæŸ¥è¯¢å‘é‡ \\( q \\)ï¼š",
        "",
        "1. è®¡ç®—å…¶åœ¨æ¯ä¸ªå“ˆå¸Œè¡¨ä¸­çš„ç­¾åï¼›",
        "2. æ‰¾å‡ºæ‰€æœ‰æ¡¶ä¸­ä¸å…¶å“ˆå¸Œç›¸åŒçš„å€™é€‰æ ·æœ¬ï¼›",
        "3. å¯¹å€™é€‰æ ·æœ¬è®¡ç®—çœŸå®ç›¸ä¼¼åº¦ï¼ˆå¦‚ä½™å¼¦æˆ–æ¬§å¼è·ç¦»ï¼‰ï¼›",
        "4. è¿”å›æœ€ç›¸ä¼¼çš„ Top-Kã€‚",
        "",
        "---",
        "",
        "### å››ã€å‚æ•°ä¸æ€§èƒ½æƒè¡¡",
        "",
        "| å‚æ•° | å«ä¹‰ | å½±å“ |",
        "|------|------|------|",
        "| k | æ¯ç»„å“ˆå¸Œå‡½æ•°æ•°é‡ | è¶Šå¤§ â†’ æ¡¶æ›´å°ï¼Œå¬å›ç‡ä¸‹é™ä½†ç²¾åº¦æé«˜ |",
        "| L | å“ˆå¸Œè¡¨æ•°é‡ | è¶Šå¤š â†’ å¬å›ç‡ä¸Šå‡ä½†å†…å­˜æ¶ˆè€—å¤§ |",
        "| n | æ ·æœ¬æ•°é‡ | å½±å“æŸ¥è¯¢é€Ÿåº¦ï¼Œè¶Šå¤šæ”¶ç›Šè¶Šæ˜æ˜¾ |",
        "",
        "ä¸€èˆ¬ç»éªŒå€¼ï¼š",
        "- **k** = 10ï½20  ",
        "- **L** = 20ï½100  ",
        "",
        "---",
        "",
        "### äº”ã€ç®—æ³•å¤æ‚åº¦åˆ†æ",
        "",
        "| é˜¶æ®µ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ |",
        "|------|-------------|-------------|",
        "| å»ºè¡¨ | \\( O(nLk) \\) | \\( O(nL) \\) |",
        "| æŸ¥è¯¢ | \\( O(L(k + c)) \\)ï¼Œå…¶ä¸­ c ä¸ºå€™é€‰æ•°é‡ | - |",
        "",
        "ç›¸æ¯”æš´åŠ›æœç´¢ \\( O(n) \\)ï¼ŒLSH æŸ¥è¯¢å¤æ‚åº¦å¯è¾¾ **äºšçº¿æ€§çº§ï¼ˆå¦‚ O(n^0.5)ï¼‰**ã€‚",
        "",
        "---",
        "",
        "",
        "",
        "## 2.LSHç®—æ³•å®ç°",
        "",
        "ğŸ§  LSHç®—æ³•Pythonå®ç°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np",
        "from typing import List, Union, Dict",
        "import numpy as np",
        "",
        "",
        "class CosineLSH:",
        "    \"\"\"",
        "    åŸºäºéšæœºè¶…å¹³é¢æŠ•å½±çš„å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰ï¼Œé€‚ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦æµ‹é‡ã€‚",
        "    ",
        "    å‚æ•°:",
        "        hash_size (int): å•ä¸ªå“ˆå¸Œè¡¨çš„å“ˆå¸Œå‡½æ•°æ•°é‡ï¼ˆå³å“ˆå¸Œç çš„ä½æ•°ï¼‰",
        "        num_tables (int): ä½¿ç”¨çš„å“ˆå¸Œè¡¨æ•°é‡",
        "    \"\"\"",
        "    ",
        "    def __init__(self, hash_size: int = 6, num_tables: int = 5):",
        "        self.hash_size = hash_size  # æ¯ä¸ªå“ˆå¸Œè¡¨çš„ä½æ•°",
        "        self.num_tables = num_tables  # å“ˆå¸Œè¡¨æ•°é‡",
        "        self.hash_tables = [dict() for _ in range(num_tables)]  # åˆå§‹åŒ–å“ˆå¸Œè¡¨",
        "        self.random_planes_list = []  # å­˜å‚¨æ¯ä¸ªå“ˆå¸Œè¡¨çš„éšæœºè¶…å¹³é¢",
        "        self.dimension = None  # æ•°æ®ç»´åº¦ï¼ˆåœ¨æ’å…¥æ•°æ®æ—¶ç¡®å®šï¼‰",
        "    ",
        "    def _generate_random_planes(self, dimension: int) -> np.ndarray:",
        "        \"\"\"ä¸ºå•ä¸ªå“ˆå¸Œè¡¨ç”Ÿæˆéšæœºè¶…å¹³é¢ï¼ˆæ¯ä¸ªè¶…å¹³é¢å¯¹åº”ä¸€ä¸ªå“ˆå¸Œå‡½æ•°ï¼‰\"\"\"",
        "        return np.random.randn(self.hash_size, dimension)",
        "    ",
        "    def _hash(self, vector: np.ndarray, random_planes: np.ndarray) -> str:",
        "        \"\"\"è®¡ç®—å•ä¸ªå‘é‡çš„å“ˆå¸Œé”®ï¼ˆäºŒè¿›åˆ¶å­—ç¬¦ä¸²ï¼‰\"\"\"",
        "        # è®¡ç®—å‘é‡ä¸æ¯ä¸ªéšæœºè¶…å¹³é¢çš„ç‚¹ç§¯ï¼Œæ ¹æ®ç¬¦å·ç”ŸæˆäºŒè¿›åˆ¶ä½",
        "        projections = np.dot(vector, random_planes.T)",
        "        hash_bits = (projections > 0).astype(int)  # å¤§äº0ä¸º1ï¼Œå¦åˆ™ä¸º0",
        "        return ''.join(hash_bits.astype(str))  # è½¬æ¢ä¸ºäºŒè¿›åˆ¶å­—ç¬¦ä¸²ä½œä¸ºå“ˆå¸Œé”®",
        "    ",
        "    def index(self, data: Union[List[List[float]], np.ndarray]) -> None:",
        "        \"\"\"",
        "        å°†æ•°æ®å‘é‡æ’å…¥LSHç´¢å¼•ä¸­",
        "        ",
        "        å‚æ•°:",
        "            data: å¾…ç´¢å¼•çš„å‘é‡åˆ—è¡¨æˆ–æ•°ç»„",
        "        \"\"\"",
        "        data_array = np.array(data)",
        "        if len(data_array.shape) == 1:",
        "            data_array = data_array.reshape(1, -1)",
        "        ",
        "        self.dimension = data_array.shape[1]  # è®¾ç½®æ•°æ®ç»´åº¦",
        "        ",
        "        # ä¸ºæ¯ä¸ªå“ˆå¸Œè¡¨ç”Ÿæˆéšæœºè¶…å¹³é¢",
        "        self.random_planes_list = [",
        "            self._generate_random_planes(self.dimension) ",
        "            for _ in range(self.num_tables)",
        "        ]",
        "        ",
        "        # å°†æ¯ä¸ªå‘é‡æ’å…¥æ‰€æœ‰å“ˆå¸Œè¡¨",
        "        for i, vector in enumerate(data_array):",
        "            for table_idx in range(self.num_tables):",
        "                hash_key = self._hash(vector, self.random_planes_list[table_idx])",
        "                ",
        "                # å°†å‘é‡ç´¢å¼•å­˜å…¥å¯¹åº”å“ˆå¸Œæ¡¶",
        "                if hash_key in self.hash_tables[table_idx]:",
        "                    self.hash_tables[table_idx][hash_key].append(i)",
        "                else:",
        "                    self.hash_tables[table_idx][hash_key] = [i]",
        "    ",
        "    def query(self, query_vector: Union[List[float], np.ndarray], ",
        "              max_results: int = 10) -> List[int]:",
        "        \"\"\"",
        "        æŸ¥è¯¢ä¸ç»™å®šå‘é‡ç›¸ä¼¼çš„å‘é‡",
        "        ",
        "        å‚æ•°:",
        "            query_vector: æŸ¥è¯¢å‘é‡",
        "            max_results: è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡",
        "            ",
        "        è¿”å›:",
        "            ç›¸ä¼¼å‘é‡çš„ç´¢å¼•åˆ—è¡¨",
        "        \"\"\"",
        "        if self.dimension is None:",
        "            raise ValueError(\"è¯·å…ˆä½¿ç”¨indexæ–¹æ³•æ’å…¥æ•°æ®\")",
        "        ",
        "        query_vec = np.array(query_vector)",
        "        candidates = set()",
        "        ",
        "        # åœ¨æ‰€æœ‰å“ˆå¸Œè¡¨ä¸­æŸ¥æ‰¾å€™é€‰å‘é‡",
        "        for table_idx in range(self.num_tables):",
        "            hash_key = self._hash(query_vec, self.random_planes_list[table_idx])",
        "            if hash_key in self.hash_tables[table_idx]:",
        "                candidates.update(self.hash_tables[table_idx][hash_key])",
        "        ",
        "        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å€™é€‰å‘é‡ï¼Œå°è¯•æŸ¥æ‰¾é‚»è¿‘æ¡¶",
        "        if not candidates:",
        "            print(\"æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„å€™é€‰å‘é‡ï¼Œæ­£åœ¨æœç´¢é‚»è¿‘æ¡¶...\")",
        "            for table_idx in range(self.num_tables):",
        "                original_key = self._hash(query_vec, self.random_planes_list[table_idx])",
        "                # æŸ¥æ‰¾å“ˆå¸Œç åªæœ‰1ä½ä¸åŒçš„æ¡¶",
        "                for i in range(self.hash_size):",
        "                    neighbor_key = list(original_key)",
        "                    neighbor_key[i] = '1' if neighbor_key[i] == '0' else '0'",
        "                    neighbor_key = ''.join(neighbor_key)",
        "                    if neighbor_key in self.hash_tables[table_idx]:",
        "                        candidates.update(self.hash_tables[table_idx][neighbor_key])",
        "        ",
        "        return list(candidates)[:max_results]",
        "    ",
        "    def get_hash_tables_info(self) -> Dict:",
        "        \"\"\"è¿”å›å“ˆå¸Œè¡¨çš„ç»Ÿè®¡ä¿¡æ¯\"\"\"",
        "        info = {",
        "            'num_tables': self.num_tables,",
        "            'hash_size': self.hash_size,",
        "            'total_buckets': 0,",
        "            'average_bucket_size': 0,",
        "            'table_details': []",
        "        }",
        "        ",
        "        total_vectors = 0",
        "        for i, table in enumerate(self.hash_tables):",
        "            num_buckets = len(table)",
        "            vectors_in_table = sum(len(bucket) for bucket in table.values())",
        "            total_vectors += vectors_in_table",
        "            ",
        "            avg_size = vectors_in_table / num_buckets if num_buckets > 0 else 0",
        "            info['table_details'].append({",
        "                'table_index': i,",
        "                'num_buckets': num_buckets,",
        "                'total_vectors': vectors_in_table,",
        "                'average_bucket_size': avg_size",
        "            })",
        "        ",
        "        info['total_buckets'] = sum(detail['num_buckets'] ",
        "                                  for detail in info['table_details'])",
        "        if info['total_buckets'] > 0:",
        "            info['average_bucket_size'] = (total_vectors / ",
        "                                         info['total_buckets'])",
        "        ",
        "        return info",
        "",
        "",
        "# ç¤ºä¾‹ä½¿ç”¨å’Œæµ‹è¯•",
        "if __name__ == \"__main__\":",
        "    # ç”Ÿæˆç¤ºä¾‹æ•°æ®",
        "    np.random.seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°",
        "    data_vectors = np.random.randn(100, 10)  # 100ä¸ª10ç»´å‘é‡",
        "    ",
        "    # åˆ›å»ºLSHç´¢å¼•",
        "    print(\"æ­£åœ¨æ„å»ºLSHç´¢å¼•...\")",
        "    lsh = CosineLSH(hash_size=8, num_tables=3)",
        "    lsh.index(data_vectors)",
        "    ",
        "    # æ˜¾ç¤ºå“ˆå¸Œè¡¨ç»Ÿè®¡ä¿¡æ¯",
        "    info = lsh.get_hash_tables_info()",
        "    print(f\"\\nå“ˆå¸Œè¡¨ç»Ÿè®¡ä¿¡æ¯:\")",
        "    print(f\"å“ˆå¸Œè¡¨æ•°é‡: {info['num_tables']}\")",
        "    print(f\"æ€»æ¡¶æ•°: {info['total_buckets']}\")",
        "    print(f\"å¹³å‡æ¯ä¸ªæ¡¶çš„å‘é‡æ•°: {info['average_bucket_size']:.2f}\")",
        "    ",
        "    # æŸ¥è¯¢ç¤ºä¾‹",
        "    query_vec = data_vectors[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå‘é‡ä½œä¸ºæŸ¥è¯¢",
        "    print(f\"\\næŸ¥è¯¢å‘é‡ç´¢å¼•: 0\")",
        "    ",
        "    similar_indices = lsh.query(query_vec, max_results=5)",
        "    print(f\"æ‰¾åˆ°çš„ç›¸ä¼¼å‘é‡ç´¢å¼•: {similar_indices}\")",
        "    ",
        "    # éªŒè¯ç»“æœï¼šè®¡ç®—å®é™…ä½™å¼¦ç›¸ä¼¼åº¦",
        "    from sklearn.metrics.pairwise import cosine_similarity",
        "    ",
        "    print(\"\\nç›¸ä¼¼åº¦éªŒè¯:\")",
        "    for idx in similar_indices:",
        "        similarity = cosine_similarity([query_vec], [data_vectors[idx]])[0][0]",
        "        print(f\"å‘é‡ {idx} ä¸æŸ¥è¯¢å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}\")",
        "    ",
        "    # å¯¹æ¯”çº¿æ€§æœç´¢ç»“æœ",
        "    print(\"\\n=== ä¸çº¿æ€§æœç´¢å¯¹æ¯” ===\")",
        "    all_similarities = cosine_similarity([query_vec], data_vectors)[0]",
        "    top_linear = np.argsort(all_similarities)[::-1][1:6]  # æ’é™¤è‡ªèº«ï¼Œå–å‰5ä¸ª",
        "    print(f\"çº¿æ€§æœç´¢Top-5ç»“æœ: {top_linear}\")",
        "    ",
        "    # è®¡ç®—å¬å›ç‡",
        "    lsh_recall = len(set(similar_indices) & set(top_linear)) / len(top_linear)",
        "    print(f\"LSHå¬å›ç‡ï¼ˆä¸çœŸå®Top-5ç›¸æ¯”ï¼‰: {lsh_recall:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "è¿è¡Œæ¼”ç¤ºï¼š",
        "æ­£åœ¨æ„å»ºLSHç´¢å¼•...",
        "",
        "å“ˆå¸Œè¡¨ç»Ÿè®¡ä¿¡æ¯:",
        "å“ˆå¸Œè¡¨æ•°é‡: 3",
        "æ€»æ¡¶æ•°: 189",
        "å¹³å‡æ¯ä¸ªæ¡¶çš„å‘é‡æ•°: 1.59",
        "",
        "æŸ¥è¯¢å‘é‡ç´¢å¼•: 0",
        "æ‰¾åˆ°çš„ç›¸ä¼¼å‘é‡ç´¢å¼•: [0, 48, 20, 54, 86]",
        "",
        "ç›¸ä¼¼åº¦éªŒè¯:",
        "å‘é‡ 0 ä¸æŸ¥è¯¢å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦: 1.0000",
        "å‘é‡ 48 ä¸æŸ¥è¯¢å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦: -0.2653",
        "å‘é‡ 20 ä¸æŸ¥è¯¢å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦: 0.5041",
        "å‘é‡ 54 ä¸æŸ¥è¯¢å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦: 0.4609",
        "å‘é‡ 86 ä¸æŸ¥è¯¢å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦: 0.6143",
        "",
        "=== ä¸çº¿æ€§æœç´¢å¯¹æ¯” ===",
        "çº¿æ€§æœç´¢Top-5ç»“æœ: [91 32 15 86 20]",
        "LSHå¬å›ç‡ï¼ˆä¸çœŸå®Top-5ç›¸æ¯”ï¼‰: 40.00%",
        "",
        "LSHç®—æ³•å¯è§†åŒ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "from sklearn.metrics.pairwise import cosine_similarity",
        "from sklearn.decomposition import PCA",
        "plt.rcParams['font.sans-serif'] = ['Hiragino Sans GB', 'STHeiti', 'PingFang SC', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']",
        "plt.rcParams['axes.unicode_minus'] = False",
        "class VisualizableLSH:",
        "    \"\"\"",
        "    å¯å¯è§†åŒ–çš„å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰å®ç°ï¼ŒåŸºäºéšæœºè¶…å¹³é¢æŠ•å½±",
        "    é€‚ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦æœç´¢",
        "    \"\"\"",
        "    ",
        "    def __init__(self, hash_size=4, num_tables=3, dimension=2):",
        "        \"\"\"",
        "        åˆå§‹åŒ–LSHå‚æ•°",
        "        ",
        "        å‚æ•°:",
        "        - hash_size: æ¯ä¸ªå“ˆå¸Œè¡¨çš„ä½æ•°ï¼ˆè¶…å¹³é¢æ•°é‡ï¼‰",
        "        - num_tables: å“ˆå¸Œè¡¨æ•°é‡",
        "        - dimension: æ•°æ®ç»´åº¦",
        "        \"\"\"",
        "        self.hash_size = hash_size",
        "        self.num_tables = num_tables",
        "        self.dimension = dimension",
        "        self.hash_tables = [{} for _ in range(num_tables)]",
        "        self.random_planes_list = []",
        "        self.data_points = []",
        "        self.is_trained = False",
        "        ",
        "        # ç”Ÿæˆéšæœºè¶…å¹³é¢ï¼ˆæ³•å‘é‡ï¼‰",
        "        self._generate_random_planes()",
        "        ",
        "    def _generate_random_planes(self):",
        "        \"\"\"ç”Ÿæˆéšæœºè¶…å¹³é¢æ³•å‘é‡\"\"\"",
        "        for i in range(self.num_tables):",
        "            # ç”Ÿæˆéšæœºè¶…å¹³é¢æ³•å‘é‡ï¼Œä»¥åŸç‚¹ä¸ºä¸­å¿ƒ",
        "            planes = np.random.randn(self.hash_size, self.dimension) - 0.5",
        "            self.random_planes_list.append(planes)",
        "        self.is_trained = True",
        "    ",
        "    def _hash_vector(self, vector, plane_norms):",
        "        \"\"\"è®¡ç®—å•ä¸ªå‘é‡çš„å“ˆå¸Œå€¼ï¼ˆäºŒè¿›åˆ¶ç¼–ç ï¼‰\"\"\"",
        "        # è®¡ç®—å‘é‡ä¸æ¯ä¸ªè¶…å¹³é¢æ³•å‘é‡çš„ç‚¹ç§¯",
        "        projections = np.dot(vector, plane_norms.T)",
        "        # æ ¹æ®ç‚¹ç§¯ç¬¦å·ç”ŸæˆäºŒè¿›åˆ¶ç¼–ç ",
        "        hash_bits = (projections > 0).astype(int)",
        "        # è½¬æ¢ä¸ºäºŒè¿›åˆ¶å­—ç¬¦ä¸²ä½œä¸ºå“ˆå¸Œé”®",
        "        return ''.join(hash_bits.astype(str))",
        "    ",
        "    def add_vector(self, vector, vector_id=None):",
        "        \"\"\"å‘LSHç´¢å¼•ä¸­æ·»åŠ å‘é‡\"\"\"",
        "        if vector_id is None:",
        "            vector_id = len(self.data_points)",
        "        ",
        "        self.data_points.append(vector)",
        "        ",
        "        # å°†å‘é‡æ·»åŠ åˆ°æ‰€æœ‰å“ˆå¸Œè¡¨",
        "        for table_idx in range(self.num_tables):",
        "            hash_key = self._hash_vector(vector, self.random_planes_list[table_idx])",
        "            ",
        "            if hash_key in self.hash_tables[table_idx]:",
        "                self.hash_tables[table_idx][hash_key].append(vector_id)",
        "            else:",
        "                self.hash_tables[table_idx][hash_key] = [vector_id]",
        "        ",
        "        return vector_id",
        "    ",
        "    def query(self, query_vector, max_results=5):",
        "        \"\"\"æŸ¥è¯¢ç›¸ä¼¼å‘é‡\"\"\"",
        "        candidates = set()",
        "        ",
        "        # åœ¨æ‰€æœ‰å“ˆå¸Œè¡¨ä¸­æŸ¥æ‰¾å€™é€‰å‘é‡",
        "        for table_idx in range(self.num_tables):",
        "            hash_key = self._hash_vector(query_vector, self.random_planes_list[table_idx])",
        "            if hash_key in self.hash_tables[table_idx]:",
        "                candidates.update(self.hash_tables[table_idx][hash_key])",
        "        ",
        "        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼ŒæŸ¥æ‰¾æ±‰æ˜è·ç¦»æœ€è¿‘çš„æ¡¶",
        "        if not candidates:",
        "            print(\"æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œæ­£åœ¨æœç´¢é‚»è¿‘æ¡¶...\")",
        "            for table_idx in range(self.num_tables):",
        "                original_key = self._hash_vector(query_vector, self.random_planes_list[table_idx])",
        "                # æŸ¥æ‰¾æ±‰æ˜è·ç¦»ä¸º1çš„é‚»è¿‘æ¡¶",
        "                for i in range(len(original_key)):",
        "                    neighbor_key = list(original_key)",
        "                    neighbor_key[i] = '1' if neighbor_key[i] == '0' else '0'",
        "                    neighbor_key = ''.join(neighbor_key)",
        "                    if neighbor_key in self.hash_tables[table_idx]:",
        "                        candidates.update(self.hash_tables[table_idx][neighbor_key])",
        "        ",
        "        candidate_ids = list(candidates)[:max_results]",
        "        candidate_vectors = [self.data_points[i] for i in candidate_ids]",
        "        ",
        "        return candidate_ids, candidate_vectors",
        "    ",
        "    def get_hash_stats(self):",
        "        \"\"\"è·å–å“ˆå¸Œè¡¨ç»Ÿè®¡ä¿¡æ¯\"\"\"",
        "        stats = {",
        "            'total_vectors': len(self.data_points),",
        "            'total_buckets': 0,",
        "            'table_details': []",
        "        }",
        "        ",
        "        for i, table in enumerate(self.hash_tables):",
        "            num_buckets = len(table)",
        "            vectors_in_table = sum(len(bucket) for bucket in table.values())",
        "            avg_size = vectors_in_table / num_buckets if num_buckets > 0 else 0",
        "            ",
        "            stats['table_details'].append({",
        "                'table_index': i,",
        "                'num_buckets': num_buckets,",
        "                'total_vectors': vectors_in_table,",
        "                'average_bucket_size': avg_size",
        "            })",
        "        ",
        "        stats['total_buckets'] = sum(detail['num_buckets'] for detail in stats['table_details'])",
        "        return stats",
        "",
        "def visualize_lsh_process(lsh, query_vector=None, highlight_vector=None):",
        "    \"\"\"å¯è§†åŒ–LSHå“ˆå¸Œè¿‡ç¨‹\"\"\"",
        "    ",
        "    if len(lsh.data_points) == 0:",
        "        print(\"æ²¡æœ‰æ•°æ®å¯å¯è§†åŒ–\")",
        "        return",
        "    ",
        "    # å¦‚æœæ•°æ®ç»´åº¦å¤§äº2ï¼Œä½¿ç”¨PCAé™ç»´",
        "    if lsh.dimension > 2:",
        "        pca = PCA(n_components=2)",
        "        all_vectors = np.array(lsh.data_points + ([query_vector] if query_vector is not None else []))",
        "        vectors_2d = pca.fit_transform(all_vectors)",
        "        ",
        "        data_2d = vectors_2d[:len(lsh.data_points)]",
        "        if query_vector is not None:",
        "            query_2d = vectors_2d[-1]",
        "        else:",
        "            query_2d = None",
        "    else:",
        "        data_2d = np.array(lsh.data_points)",
        "        query_2d = query_vector",
        "    ",
        "    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))",
        "    fig.suptitle('LSHç®—æ³•å¯è§†åŒ–', fontsize=16, fontweight='bold')",
        "    ",
        "    # å­å›¾1: æ˜¾ç¤ºæ•°æ®ç‚¹å’Œè¶…å¹³é¢",
        "    ax1 = axes[0, 0]",
        "    ",
        "    # ç»˜åˆ¶æ•°æ®ç‚¹",
        "    colors = plt.cm.Set1(np.linspace(0, 1, len(data_2d)))",
        "    for i, point in enumerate(data_2d):",
        "        ax1.scatter(point[0], point[1], c=[colors[i]], s=100, alpha=0.7, label=f'å‘é‡ {i}')",
        "    ",
        "    # ç»˜åˆ¶è¶…å¹³é¢ï¼ˆåªæ˜¾ç¤ºç¬¬ä¸€ä¸ªå“ˆå¸Œè¡¨çš„å‰ä¸¤ä¸ªè¶…å¹³é¢ï¼‰",
        "    if len(lsh.random_planes_list) > 0:",
        "        planes = lsh.random_planes_list[0]",
        "        for j, plane in enumerate(planes[:2]):  # åªæ˜¾ç¤ºå‰ä¸¤ä¸ªè¶…å¹³é¢",
        "            # åœ¨2Dç©ºé—´ä¸­ï¼Œè¶…å¹³é¢æ˜¯ç›´çº¿",
        "            if lsh.dimension > 2:",
        "                # å¯¹äºé«˜ç»´æ•°æ®ï¼Œæ˜¾ç¤ºæŠ•å½±åçš„è¶…å¹³é¢æ–¹å‘",
        "                plane_2d = pca.transform([plane])[0]",
        "            else:",
        "                plane_2d = plane",
        "            ",
        "            # è®¡ç®—è¶…å¹³é¢çš„æ³•çº¿æ–¹å‘",
        "            norm = np.linalg.norm(plane_2d)",
        "            if norm > 0:",
        "                # ç»˜åˆ¶è¶…å¹³é¢æ³•çº¿",
        "                ax1.quiver(0, 0, plane_2d[0], plane_2d[1], ",
        "                          angles='xy', scale_units='xy', scale=1, ",
        "                          color='red', alpha=0.7, width=0.01)",
        "                ",
        "                # ç»˜åˆ¶è¶…å¹³é¢ï¼ˆå‚ç›´äºæ³•çº¿ï¼‰",
        "                if abs(plane_2d[0]) > 1e-10:  # é¿å…é™¤é›¶",
        "                    slope = -plane_2d[0] / plane_2d[1] if abs(plane_2d[1]) > 1e-10 else 1e10",
        "                    x_vals = np.array(ax1.get_xlim())",
        "                    y_vals = slope * (x_vals - 0) + 0",
        "                    ax1.plot(x_vals, y_vals, 'r--', alpha=0.5, label=f'è¶…å¹³é¢ {j+1}')",
        "    ",
        "    # æ ‡è®°æŸ¥è¯¢ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰",
        "    if query_2d is not None:",
        "        ax1.scatter(query_2d[0], query_2d[1], c='yellow', marker='*', ",
        "                   s=300, edgecolors='black', linewidth=2, label='æŸ¥è¯¢ç‚¹')",
        "    ",
        "    ax1.set_title('æ•°æ®ç‚¹å’Œè¶…å¹³é¢åˆ’åˆ†')",
        "    ax1.set_xlabel('ç‰¹å¾ 1')",
        "    ax1.set_ylabel('ç‰¹å¾ 2')",
        "    ax1.grid(True, alpha=0.3)",
        "    ax1.legend()",
        "    ",
        "    # å­å›¾2: æ˜¾ç¤ºå“ˆå¸Œæ¡¶åˆ†å¸ƒ",
        "    ax2 = axes[0, 1]",
        "    ",
        "    # ç»Ÿè®¡æ¯ä¸ªæ¡¶çš„å‘é‡æ•°é‡",
        "    bucket_sizes = []",
        "    bucket_labels = []",
        "    for table_idx, table in enumerate(lsh.hash_tables):",
        "        for hash_key, vectors in table.items():",
        "            bucket_sizes.append(len(vectors))",
        "            bucket_labels.append(f'T{table_idx}_{hash_key[:4]}...')",
        "    ",
        "    if bucket_sizes:",
        "        ax2.bar(range(len(bucket_sizes)), bucket_sizes, alpha=0.7)",
        "        ax2.set_title('å„å“ˆå¸Œæ¡¶ä¸­çš„å‘é‡æ•°é‡åˆ†å¸ƒ')",
        "        ax2.set_xlabel('å“ˆå¸Œæ¡¶')",
        "        ax2.set_ylabel('å‘é‡æ•°é‡')",
        "        ax2.tick_params(axis='x', rotation=45)",
        "    ",
        "    # å­å›¾3: æ˜¾ç¤ºå“ˆå¸Œç¼–ç ",
        "    ax3 = axes[1, 0]",
        "    ",
        "    # æ˜¾ç¤ºå‰å‡ ä¸ªå‘é‡çš„å“ˆå¸Œç¼–ç ",
        "    display_count = min(5, len(lsh.data_points))",
        "    hash_codes = []",
        "    vector_labels = []",
        "    ",
        "    for i in range(display_count):",
        "        hash_code = lsh._hash_vector(lsh.data_points[i], lsh.random_planes_list[0])",
        "        hash_codes.append([int(bit) for bit in hash_code])",
        "        vector_labels.append(f'å‘é‡{i}')",
        "    ",
        "    if hash_codes:",
        "        im = ax3.imshow(hash_codes, cmap='Blues', aspect='auto')",
        "        ax3.set_xticks(range(len(hash_code)))",
        "        ax3.set_xticklabels([f'ä½{i+1}' for i in range(len(hash_code))])",
        "        ax3.set_yticks(range(display_count))",
        "        ax3.set_yticklabels(vector_labels)",
        "        ",
        "        # æ·»åŠ æ•°å€¼æ ‡æ³¨",
        "        for i in range(len(hash_codes)):",
        "            for j in range(len(hash_codes[0])):",
        "                ax3.text(j, i, hash_codes[i][j], ",
        "                        ha='center', va='center', fontweight='bold')",
        "        ",
        "        ax3.set_title('å‘é‡å“ˆå¸Œç¼–ç ï¼ˆç¬¬ä¸€ä¸ªå“ˆå¸Œè¡¨ï¼‰')",
        "        plt.colorbar(im, ax=ax3, label='æ¯”ç‰¹å€¼')",
        "    ",
        "    # å­å›¾4: æ˜¾ç¤ºæŸ¥è¯¢ç»“æœï¼ˆå¦‚æœæœ‰æŸ¥è¯¢ï¼‰",
        "    ax4 = axes[1, 1]",
        "    ",
        "    if query_2d is not None:",
        "        # æ‰§è¡ŒæŸ¥è¯¢",
        "        candidate_ids, candidate_vectors = lsh.query(query_vector)",
        "        ",
        "        # ç»˜åˆ¶æ‰€æœ‰æ•°æ®ç‚¹",
        "        for i, point in enumerate(data_2d):",
        "            if i in candidate_ids:",
        "                # é«˜äº®å€™é€‰å‘é‡",
        "                ax4.scatter(point[0], point[1], c='red', s=150, ",
        "                           alpha=0.8, label='å€™é€‰å‘é‡' if i == candidate_ids[0] else \"\")",
        "            else:",
        "                ax4.scatter(point[0], point[1], c='blue', s=80, alpha=0.3)",
        "        ",
        "        # æ ‡è®°æŸ¥è¯¢ç‚¹",
        "        ax4.scatter(query_2d[0], query_2d[1], c='yellow', marker='*', ",
        "                   s=300, edgecolors='black', linewidth=2, label='æŸ¥è¯¢ç‚¹')",
        "        ",
        "        ax4.set_title('LSHæŸ¥è¯¢ç»“æœ')",
        "        ax4.set_xlabel('ç‰¹å¾ 1')",
        "        ax4.set_ylabel('ç‰¹å¾ 2')",
        "        ax4.grid(True, alpha=0.3)",
        "        ax4.legend()",
        "    else:",
        "        # å¦‚æœæ²¡æœ‰æŸ¥è¯¢ï¼Œæ˜¾ç¤ºå“ˆå¸Œè¡¨ç»Ÿè®¡ä¿¡æ¯",
        "        stats = lsh.get_hash_stats()",
        "        ax4.text(0.1, 0.9, f\"LSHç´¢å¼•ç»Ÿè®¡:\", fontsize=12, fontweight='bold')",
        "        ax4.text(0.1, 0.8, f\"å‘é‡æ€»æ•°: {stats['total_vectors']}\", fontsize=10)",
        "        ax4.text(0.1, 0.7, f\"å“ˆå¸Œè¡¨æ•°é‡: {lsh.num_tables}\", fontsize=10)",
        "        ax4.text(0.1, 0.6, f\"æ€»æ¡¶æ•°: {stats['total_buckets']}\", fontsize=10)",
        "        ",
        "        for i, detail in enumerate(stats['table_details']):",
        "            ax4.text(0.1, 0.5 - i*0.1, ",
        "                    f\"è¡¨{detail['table_index']}: {detail['num_buckets']}ä¸ªæ¡¶\", ",
        "                    fontsize=9)",
        "        ",
        "        ax4.set_xlim(0, 1)",
        "        ax4.set_ylim(0, 1)",
        "        ax4.set_title('LSHç´¢å¼•ç»Ÿè®¡ä¿¡æ¯')",
        "        ax4.axis('off')",
        "    ",
        "    plt.tight_layout()",
        "    plt.show()",
        "",
        "def demonstrate_lsh():",
        "    \"\"\"æ¼”ç¤ºLSHç®—æ³•çš„å®Œæ•´æµç¨‹\"\"\"",
        "    print(\"=\" * 60)",
        "    print(\"LSHç®—æ³•å®Œæ•´æ¼”ç¤º\")",
        "    print(\"=\" * 60)",
        "    ",
        "    # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆ2ç»´ä¾¿äºå¯è§†åŒ–ï¼‰",
        "    np.random.seed(42)",
        "    n_samples = 20",
        "    dim = 2",
        "    ",
        "    # ç”Ÿæˆå…·æœ‰èšç±»ç»“æ„çš„æ•°æ®",
        "    cluster1 = np.random.normal(loc=[2, 3], scale=0.5, size=(n_samples//2, dim))",
        "    cluster2 = np.random.normal(loc=[-1, -1], scale=0.3, size=(n_samples//2, dim))",
        "    data = np.vstack([cluster1, cluster2])",
        "    ",
        "    print(f\"ç”Ÿæˆ{len(data)}ä¸ª{dim}ç»´æ•°æ®ç‚¹\")",
        "    ",
        "    # 2. åˆ›å»ºå¹¶åˆå§‹åŒ–LSHç´¢å¼•",
        "    lsh = VisualizableLSH(hash_size=4, num_tables=2, dimension=dim)",
        "    ",
        "    # 3. å‘LSHç´¢å¼•ä¸­æ·»åŠ æ•°æ®",
        "    for i, vector in enumerate(data):",
        "        lsh.add_vector(vector, i)",
        "    ",
        "    print(\"LSHç´¢å¼•æ„å»ºå®Œæˆ!\")",
        "    ",
        "    # 4. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯",
        "    stats = lsh.get_hash_stats()",
        "    print(f\"\\nLSHç´¢å¼•ç»Ÿè®¡:\")",
        "    print(f\"å‘é‡æ€»æ•°: {stats['total_vectors']}\")",
        "    print(f\"å“ˆå¸Œè¡¨æ•°é‡: {lsh.num_tables}\")",
        "    print(f\"æ€»æ¡¶æ•°: {stats['total_buckets']}\")",
        "    ",
        "    for detail in stats['table_details']:",
        "        print(f\"è¡¨{detail['table_index']}: {detail['num_buckets']}ä¸ªæ¡¶, \"",
        "              f\"å¹³å‡æ¯ä¸ªæ¡¶{detail['average_bucket_size']:.2f}ä¸ªå‘é‡\")",
        "    ",
        "    # 5. åˆ›å»ºæŸ¥è¯¢ç‚¹",
        "    query_point = np.array([1.5, 2.0])",
        "    print(f\"\\næŸ¥è¯¢ç‚¹: {query_point}\")",
        "    ",
        "    # 6. æ‰§è¡ŒæŸ¥è¯¢",
        "    candidate_ids, candidate_vectors = lsh.query(query_point, max_results=3)",
        "    ",
        "    print(f\"æ‰¾åˆ°{len(candidate_ids)}ä¸ªå€™é€‰å‘é‡:\")",
        "    for i, vec_id in enumerate(candidate_ids):",
        "        similarity = cosine_similarity([query_point], [data[vec_id]])[0][0]",
        "        print(f\"å€™é€‰å‘é‡{vec_id}: ç›¸ä¼¼åº¦={similarity:.4f}\")",
        "    ",
        "    # 7. å¯è§†åŒ–æ•´ä¸ªè¿‡ç¨‹",
        "    print(\"\\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...\")",
        "    visualize_lsh_process(lsh, query_point)",
        "    ",
        "    return lsh, data, query_point, candidate_ids",
        "",
        "# è¿è¡Œæ¼”ç¤º",
        "lsh, data, query, candidates = demonstrate_lsh()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "è¾“å‡ºç»“æœï¼š",
        "![alt text](/images/LSHç®—æ³•ç»“æœ.png)",
        "",
        "è¿™ä¸ªå®ç°é€šè¿‡å››ä¸ªå­å›¾å±•ç¤ºäº†LSHç®—æ³•çš„å…³é”®è¿‡ç¨‹ï¼š",
        "1.æ•°æ®ç‚¹å’Œè¶…å¹³é¢åˆ’åˆ†ï¼šæ˜¾ç¤ºåŸå§‹æ•°æ®åˆ†å¸ƒå’Œéšæœºè¶…å¹³é¢å¦‚ä½•åˆ’åˆ†ç©ºé—´",
        "2.å“ˆå¸Œæ¡¶åˆ†å¸ƒï¼šå±•ç¤ºå‘é‡åœ¨ä¸åŒå“ˆå¸Œæ¡¶ä¸­çš„åˆ†å¸ƒæƒ…å†µ",
        "3.å‘é‡å“ˆå¸Œç¼–ç ï¼šç”¨çƒ­å›¾å½¢å¼æ˜¾ç¤ºæ¯ä¸ªå‘é‡çš„äºŒè¿›åˆ¶å“ˆå¸Œç¼–ç ",
        "4.æŸ¥è¯¢ç»“æœï¼šé«˜äº®æ˜¾ç¤ºLSHæ‰¾åˆ°çš„ç›¸ä¼¼å‘é‡å€™é€‰é›†",
        "",
        "",
        "ç†è§£LSHç®—æ³•çš„å‚æ•°å¯¹æŒæ¡å…¶å·¥ä½œåŸç†è‡³å…³é‡è¦ï¼š",
        "- hash_sizeï¼ˆå“ˆå¸Œå¤§å°ï¼‰ï¼š",
        "> ä½œç”¨ï¼šå†³å®šæ¯ä¸ªå“ˆå¸Œè¡¨çš„å“ˆå¸Œç é•¿åº¦ï¼ˆä½æ•°ï¼‰",
        "> å½±å“ï¼šå€¼è¶Šå¤§ï¼Œå“ˆå¸Œæ¡¶åˆ’åˆ†è¶Šç²¾ç»†ï¼Œç›¸ä¼¼åº¦åˆ¤æ–­è¶Šå‡†ç¡®ï¼Œä½†æ¯ä¸ªæ¡¶å†…çš„å‘é‡å¯èƒ½è¶Šå°‘",
        "",
        "- num_tablesï¼ˆå“ˆå¸Œè¡¨æ•°é‡ï¼‰ï¼š",
        "> ä½œç”¨ï¼šæ§åˆ¶ä½¿ç”¨çš„ç‹¬ç«‹å“ˆå¸Œè¡¨æ•°é‡",
        ">å½±å“ï¼šå€¼è¶Šå¤§ï¼Œæ‰¾åˆ°çœŸæ­£è¿‘é‚»çš„æ¦‚ç‡è¶Šé«˜ï¼Œä½†å†…å­˜æ¶ˆè€—ä¹Ÿä¼šå¢åŠ "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}